{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473b0e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from: /Users/seyonghw/Desktop/simulation/engression-python/engression_local/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# repo root: one level up from this notebook\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from engression_local import engression, Engressor\n",
    "from engression_local.data.data_generator import preanm_generator, postanm_generator, generate_mats\n",
    "\n",
    "# sanity check\n",
    "import engression_local as pkg\n",
    "print(\"Loaded from:\", pkg.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b81adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5109df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef84432",
   "metadata": {},
   "source": [
    "# 1-dimensional case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89681d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A0, M0 = generate_mats(dx=1, dy=1, k=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba221e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1642]]), tensor([[-0.4398]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0, M0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ae629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = preanm_generator(n=10000, dx=1, dy=1, k=1, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37b08517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7882],\n",
       "        [1.3966],\n",
       "        [2.0153],\n",
       "        ...,\n",
       "        [2.1214],\n",
       "        [0.4053],\n",
       "        [3.9219]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84011faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2219],\n",
       "        [-0.2723],\n",
       "        [-0.2259],\n",
       "        ...,\n",
       "        [-0.2334],\n",
       "        [-0.2303],\n",
       "        [-0.1902]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fd056a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = torch.linspace(0, 5, 300)\n",
    "y_eval = M0* F.softplus(A0 * x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d6fdf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5128,  E(|Y-Yhat|): 0.9642,  E(|Yhat-Yhat'|): 0.9028\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3240,  E(|Y-Yhat|): 0.6389,  E(|Yhat-Yhat'|): 0.6298\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3222,  E(|Y-Yhat|): 0.6593,  E(|Yhat-Yhat'|): 0.6742\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3172,  E(|Y-Yhat|): 0.6543,  E(|Yhat-Yhat'|): 0.6742\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3214,  E(|Y-Yhat|): 0.6491,  E(|Yhat-Yhat'|): 0.6554\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3266,  E(|Y-Yhat|): 0.6524,  E(|Yhat-Yhat'|): 0.6516\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0008,  E(|Y-Yhat|): 0.0017,  E(|Yhat-Yhat'|): 0.0018\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "# Fit an engression model\n",
    "engressor = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=500, batch_size=1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7f27039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = engressor.predict(x_eval, target=\"mean\", sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f497b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x115961880>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.scatter(x.cpu(), y.cpu(), label=\"training data\", s=1, color=\"silver\")\n",
    "plt.plot(x_eval, y_eval[0], label=\"true function\", color=\"C3\")\n",
    "plt.scatter(x_eval.cpu(), y_pred.cpu(), label=\"predictions\", s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc41df",
   "metadata": {},
   "source": [
    "# multi dimensional (X,Y in R^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce52995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A0, M0 = generate_mats(dx=2, dy=2, k=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cecbdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.2155, 0.3199]]),\n",
       " tensor([[0.0973],\n",
       "         [1.3078]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0, M0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26350c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = preanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7343ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7882, 1.3966],\n",
       "        [2.0153, 3.6734],\n",
       "        [0.1464, 3.9993],\n",
       "        ...,\n",
       "        [1.2031, 0.1459],\n",
       "        [3.2959, 2.3947],\n",
       "        [1.3760, 2.7974]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e68d0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2437, 3.2750],\n",
       "        [0.5675, 7.6280],\n",
       "        [0.1130, 1.5183],\n",
       "        ...,\n",
       "        [0.1511, 2.0306],\n",
       "        [0.6579, 8.8423],\n",
       "        [0.1171, 1.5740]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad54add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.linspace(0, 5, 100)\n",
    "x2 = torch.linspace(0, 5, 100)\n",
    "X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2495710e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0505, 0.1010, 0.1515, 0.2020, 0.2525, 0.3030, 0.3535, 0.4040,\n",
       "        0.4545, 0.5051, 0.5556, 0.6061, 0.6566, 0.7071, 0.7576, 0.8081, 0.8586,\n",
       "        0.9091, 0.9596, 1.0101, 1.0606, 1.1111, 1.1616, 1.2121, 1.2626, 1.3131,\n",
       "        1.3636, 1.4141, 1.4646, 1.5152, 1.5657, 1.6162, 1.6667, 1.7172, 1.7677,\n",
       "        1.8182, 1.8687, 1.9192, 1.9697, 2.0202, 2.0707, 2.1212, 2.1717, 2.2222,\n",
       "        2.2727, 2.3232, 2.3737, 2.4242, 2.4747, 2.5253, 2.5758, 2.6263, 2.6768,\n",
       "        2.7273, 2.7778, 2.8283, 2.8788, 2.9293, 2.9798, 3.0303, 3.0808, 3.1313,\n",
       "        3.1818, 3.2323, 3.2828, 3.3333, 3.3838, 3.4343, 3.4848, 3.5354, 3.5859,\n",
       "        3.6364, 3.6869, 3.7374, 3.7879, 3.8384, 3.8889, 3.9394, 3.9899, 4.0404,\n",
       "        4.0909, 4.1414, 4.1919, 4.2424, 4.2929, 4.3434, 4.3939, 4.4444, 4.4949,\n",
       "        4.5455, 4.5960, 4.6465, 4.6970, 4.7475, 4.7980, 4.8485, 4.8990, 4.9495,\n",
       "        5.0000])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_eval[0:100,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1962e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = x_eval @ A0.T            # (N*N, 1)\n",
    "U = F.softplus(Z)        # (N*N, 1)\n",
    "y_eval = U @ M0.T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1c9d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7660,  E(|Y-Yhat|): 1.3377,  E(|Yhat-Yhat'|): 1.1433\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.4504,  E(|Y-Yhat|): 0.9360,  E(|Yhat-Yhat'|): 0.9710\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.4493,  E(|Y-Yhat|): 0.9091,  E(|Yhat-Yhat'|): 0.9196\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.4413,  E(|Y-Yhat|): 0.8999,  E(|Yhat-Yhat'|): 0.9172\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.4522,  E(|Y-Yhat|): 0.9085,  E(|Yhat-Yhat'|): 0.9127\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.4460,  E(|Y-Yhat|): 0.9080,  E(|Yhat-Yhat'|): 0.9240\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.3483,  E(|Y-Yhat|): 4.7436,  E(|Yhat-Yhat'|): 4.7906\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "# Fit an engression model\n",
    "engressor = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=500, batch_size=1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84fffeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = engressor.predict(x_eval, target=\"mean\", sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fda29f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0846, 1.1333],\n",
       "        [0.0856, 1.1456],\n",
       "        [0.0875, 1.1709],\n",
       "        ...,\n",
       "        [0.7326, 9.8468],\n",
       "        [0.7310, 9.8243],\n",
       "        [0.7375, 9.9125]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb889a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0059)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y_pred - y_eval)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff87324",
   "metadata": {},
   "source": [
    "# pre ANM, comparing 4 loss functions under different true fucntions ($X, Y \\in R^1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e151e40",
   "metadata": {},
   "source": [
    "## True function: softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81465518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5403,  E(|Y-Yhat|): 1.0034,  E(|Yhat-Yhat'|): 0.9263\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3141,  E(|Y-Yhat|): 0.6437,  E(|Yhat-Yhat'|): 0.6593\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3165,  E(|Y-Yhat|): 0.6451,  E(|Yhat-Yhat'|): 0.6572\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3154,  E(|Y-Yhat|): 0.6344,  E(|Yhat-Yhat'|): 0.6380\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3179,  E(|Y-Yhat|): 0.6400,  E(|Yhat-Yhat'|): 0.6440\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3172,  E(|Y-Yhat|): 0.6365,  E(|Yhat-Yhat'|): 0.6387\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1657,  E(|Y-Yhat|): 0.3315,  E(|Yhat-Yhat'|): 0.3316\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8320,  E(|Y-Yhat|): 1.0202,  E(|Yhat-Yhat'|): 0.3763\n",
      "[Epoch 100 (20%), batch 9] energy-loss: -0.2987,  E(|Y-Yhat|): 19.3738,  E(|Yhat-Yhat'|): 39.3451\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2141,  E(|Y-Yhat|): 21.9075,  E(|Yhat-Yhat'|): 43.3868\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.4929,  E(|Y-Yhat|): 70.4984,  E(|Yhat-Yhat'|): 140.0110\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 1.3911,  E(|Y-Yhat|): 75.4848,  E(|Yhat-Yhat'|): 148.1874\n",
      "[Epoch 500 (100%), batch 9] energy-loss: -3.4556,  E(|Y-Yhat|): 178.7583,  E(|Yhat-Yhat'|): 364.4277\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -1.5283,  E(|Y-Yhat|): 99.0690,  E(|Yhat-Yhat'|): 201.1946\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2073,  E(|Y-Yhat|): 0.3712,  E(|Yhat-Yhat'|): 0.3278\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1106,  E(|Y-Yhat|): 0.2235,  E(|Yhat-Yhat'|): 0.2259\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1064,  E(|Y-Yhat|): 0.2153,  E(|Yhat-Yhat'|): 0.2179\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1099,  E(|Y-Yhat|): 0.2178,  E(|Yhat-Yhat'|): 0.2158\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1068,  E(|Y-Yhat|): 0.2156,  E(|Yhat-Yhat'|): 0.2176\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1072,  E(|Y-Yhat|): 0.2218,  E(|Yhat-Yhat'|): 0.2293\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0655,  E(|Y-Yhat|): 0.1353,  E(|Yhat-Yhat'|): 0.1396\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4266,  E(|Y-Yhat|): 0.7078,  E(|Yhat-Yhat'|): 0.5625\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.2001,  E(|Y-Yhat|): 0.4124,  E(|Yhat-Yhat'|): 0.4246\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2014,  E(|Y-Yhat|): 0.3985,  E(|Yhat-Yhat'|): 0.3942\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1940,  E(|Y-Yhat|): 0.3949,  E(|Yhat-Yhat'|): 0.4019\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1948,  E(|Y-Yhat|): 0.3997,  E(|Yhat-Yhat'|): 0.4098\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1945,  E(|Y-Yhat|): 0.3948,  E(|Yhat-Yhat'|): 0.4007\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1223,  E(|Y-Yhat|): 0.2477,  E(|Yhat-Yhat'|): 0.2508\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2395,  E(|Y-Yhat|): 0.4423,  E(|Yhat-Yhat'|): 0.4056\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1353,  E(|Y-Yhat|): 0.2738,  E(|Yhat-Yhat'|): 0.2769\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1334,  E(|Y-Yhat|): 0.2744,  E(|Yhat-Yhat'|): 0.2821\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1376,  E(|Y-Yhat|): 0.2732,  E(|Yhat-Yhat'|): 0.2711\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1388,  E(|Y-Yhat|): 0.2748,  E(|Yhat-Yhat'|): 0.2720\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1369,  E(|Y-Yhat|): 0.2751,  E(|Yhat-Yhat'|): 0.2765\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0942,  E(|Y-Yhat|): 0.1901,  E(|Yhat-Yhat'|): 0.1917\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5104,  E(|Y-Yhat|): 0.9749,  E(|Yhat-Yhat'|): 0.9290\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3133,  E(|Y-Yhat|): 0.6448,  E(|Yhat-Yhat'|): 0.6629\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3198,  E(|Y-Yhat|): 0.6554,  E(|Yhat-Yhat'|): 0.6712\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3166,  E(|Y-Yhat|): 0.6437,  E(|Yhat-Yhat'|): 0.6541\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3184,  E(|Y-Yhat|): 0.6358,  E(|Yhat-Yhat'|): 0.6349\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3149,  E(|Y-Yhat|): 0.6352,  E(|Yhat-Yhat'|): 0.6405\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0201,  E(|Y-Yhat|): 0.0415,  E(|Yhat-Yhat'|): 0.0428\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9012,  E(|Y-Yhat|): 1.0850,  E(|Yhat-Yhat'|): 0.3676\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3149,  E(|Y-Yhat|): 7.5312,  E(|Yhat-Yhat'|): 14.4326\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.5149,  E(|Y-Yhat|): 13.5423,  E(|Yhat-Yhat'|): 26.0547\n",
      "[Epoch 300 (60%), batch 9] energy-loss: -1.6034,  E(|Y-Yhat|): 347.7390,  E(|Yhat-Yhat'|): 698.6847\n",
      "[Epoch 400 (80%), batch 9] energy-loss: -1.9119,  E(|Y-Yhat|): 191.8914,  E(|Yhat-Yhat'|): 387.6067\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 3.9463,  E(|Y-Yhat|): 502.1986,  E(|Yhat-Yhat'|): 996.5047\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.3173,  E(|Y-Yhat|): 30.4511,  E(|Yhat-Yhat'|): 61.5368\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1956,  E(|Y-Yhat|): 0.3678,  E(|Yhat-Yhat'|): 0.3443\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1083,  E(|Y-Yhat|): 0.2181,  E(|Yhat-Yhat'|): 0.2195\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1069,  E(|Y-Yhat|): 0.2193,  E(|Yhat-Yhat'|): 0.2247\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1066,  E(|Y-Yhat|): 0.2194,  E(|Yhat-Yhat'|): 0.2255\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1075,  E(|Y-Yhat|): 0.2160,  E(|Yhat-Yhat'|): 0.2171\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1090,  E(|Y-Yhat|): 0.2171,  E(|Yhat-Yhat'|): 0.2162\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0095,  E(|Y-Yhat|): 0.0192,  E(|Yhat-Yhat'|): 0.0193\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3766,  E(|Y-Yhat|): 0.6673,  E(|Yhat-Yhat'|): 0.5814\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.3985,  E(|Yhat-Yhat'|): 0.4053\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2007,  E(|Y-Yhat|): 0.4104,  E(|Yhat-Yhat'|): 0.4194\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1976,  E(|Y-Yhat|): 0.4052,  E(|Yhat-Yhat'|): 0.4152\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1982,  E(|Y-Yhat|): 0.3905,  E(|Yhat-Yhat'|): 0.3846\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1951,  E(|Y-Yhat|): 0.3968,  E(|Yhat-Yhat'|): 0.4033\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0190,  E(|Y-Yhat|): 0.0387,  E(|Yhat-Yhat'|): 0.0394\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2348,  E(|Y-Yhat|): 0.4413,  E(|Yhat-Yhat'|): 0.4130\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1361,  E(|Y-Yhat|): 0.2777,  E(|Yhat-Yhat'|): 0.2832\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1359,  E(|Y-Yhat|): 0.2728,  E(|Yhat-Yhat'|): 0.2738\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1377,  E(|Y-Yhat|): 0.2803,  E(|Yhat-Yhat'|): 0.2852\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1402,  E(|Y-Yhat|): 0.2733,  E(|Yhat-Yhat'|): 0.2663\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1350,  E(|Y-Yhat|): 0.2696,  E(|Yhat-Yhat'|): 0.2690\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0180,  E(|Y-Yhat|): 0.0363,  E(|Yhat-Yhat'|): 0.0364\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5474,  E(|Y-Yhat|): 0.9475,  E(|Yhat-Yhat'|): 0.8000\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3218,  E(|Y-Yhat|): 0.6495,  E(|Yhat-Yhat'|): 0.6553\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3190,  E(|Y-Yhat|): 0.6423,  E(|Yhat-Yhat'|): 0.6467\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3227,  E(|Y-Yhat|): 0.6382,  E(|Yhat-Yhat'|): 0.6311\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3252,  E(|Y-Yhat|): 0.6443,  E(|Yhat-Yhat'|): 0.6382\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3186,  E(|Y-Yhat|): 0.6465,  E(|Yhat-Yhat'|): 0.6559\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0041,  E(|Y-Yhat|): 0.0085,  E(|Yhat-Yhat'|): 0.0087\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7743,  E(|Y-Yhat|): 0.9842,  E(|Yhat-Yhat'|): 0.4199\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.0809,  E(|Y-Yhat|): 33.2596,  E(|Yhat-Yhat'|): 66.3575\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 4.8801,  E(|Y-Yhat|): 196.5299,  E(|Yhat-Yhat'|): 383.2996\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.9935,  E(|Y-Yhat|): 62.2771,  E(|Yhat-Yhat'|): 122.5672\n",
      "[Epoch 400 (80%), batch 9] energy-loss: -0.2227,  E(|Y-Yhat|): 144.3634,  E(|Yhat-Yhat'|): 289.1722\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.4748,  E(|Y-Yhat|): 51.1366,  E(|Yhat-Yhat'|): 101.3236\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0066,  E(|Y-Yhat|): 0.6260,  E(|Yhat-Yhat'|): 1.2387\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2233,  E(|Y-Yhat|): 0.3867,  E(|Yhat-Yhat'|): 0.3268\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1083,  E(|Y-Yhat|): 0.2272,  E(|Yhat-Yhat'|): 0.2377\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1081,  E(|Y-Yhat|): 0.2214,  E(|Yhat-Yhat'|): 0.2266\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1110,  E(|Y-Yhat|): 0.2239,  E(|Yhat-Yhat'|): 0.2258\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1122,  E(|Y-Yhat|): 0.2254,  E(|Yhat-Yhat'|): 0.2263\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1083,  E(|Y-Yhat|): 0.2208,  E(|Yhat-Yhat'|): 0.2250\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0020,  E(|Y-Yhat|): 0.0041,  E(|Yhat-Yhat'|): 0.0042\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3698,  E(|Y-Yhat|): 0.6739,  E(|Yhat-Yhat'|): 0.6081\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.2002,  E(|Y-Yhat|): 0.4157,  E(|Yhat-Yhat'|): 0.4310\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1975,  E(|Y-Yhat|): 0.4012,  E(|Yhat-Yhat'|): 0.4074\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1989,  E(|Y-Yhat|): 0.3950,  E(|Yhat-Yhat'|): 0.3922\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1946,  E(|Y-Yhat|): 0.4014,  E(|Yhat-Yhat'|): 0.4135\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1979,  E(|Y-Yhat|): 0.4044,  E(|Yhat-Yhat'|): 0.4130\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0040,  E(|Y-Yhat|): 0.0081,  E(|Yhat-Yhat'|): 0.0083\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2311,  E(|Y-Yhat|): 0.4275,  E(|Yhat-Yhat'|): 0.3929\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1369,  E(|Y-Yhat|): 0.2763,  E(|Yhat-Yhat'|): 0.2788\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1362,  E(|Y-Yhat|): 0.2761,  E(|Yhat-Yhat'|): 0.2797\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1337,  E(|Y-Yhat|): 0.2714,  E(|Yhat-Yhat'|): 0.2753\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1378,  E(|Y-Yhat|): 0.2778,  E(|Yhat-Yhat'|): 0.2799\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1376,  E(|Y-Yhat|): 0.2738,  E(|Yhat-Yhat'|): 0.2723\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0040,  E(|Y-Yhat|): 0.0080,  E(|Yhat-Yhat'|): 0.0081\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5388,  E(|Y-Yhat|): 0.9689,  E(|Yhat-Yhat'|): 0.8601\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3156,  E(|Y-Yhat|): 0.6495,  E(|Yhat-Yhat'|): 0.6677\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3165,  E(|Y-Yhat|): 0.6432,  E(|Yhat-Yhat'|): 0.6534\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3147,  E(|Y-Yhat|): 0.6374,  E(|Yhat-Yhat'|): 0.6455\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3186,  E(|Y-Yhat|): 0.6398,  E(|Yhat-Yhat'|): 0.6425\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3159,  E(|Y-Yhat|): 0.6388,  E(|Yhat-Yhat'|): 0.6457\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0134,  E(|Y-Yhat|): 0.0280,  E(|Yhat-Yhat'|): 0.0292\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8470,  E(|Y-Yhat|): 1.0304,  E(|Yhat-Yhat'|): 0.3669\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.4409,  E(|Y-Yhat|): 5.3379,  E(|Yhat-Yhat'|): 9.7939\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 1.3116,  E(|Y-Yhat|): 94.6398,  E(|Yhat-Yhat'|): 186.6564\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 1.7460,  E(|Y-Yhat|): 85.8771,  E(|Yhat-Yhat'|): 168.2621\n",
      "[Epoch 400 (80%), batch 9] energy-loss: -0.4577,  E(|Y-Yhat|): 71.0832,  E(|Yhat-Yhat'|): 143.0819\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.6823,  E(|Y-Yhat|): 25.3165,  E(|Yhat-Yhat'|): 49.2685\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0062,  E(|Y-Yhat|): 1.0205,  E(|Yhat-Yhat'|): 2.0287\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2046,  E(|Y-Yhat|): 0.3688,  E(|Yhat-Yhat'|): 0.3283\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1119,  E(|Y-Yhat|): 0.2250,  E(|Yhat-Yhat'|): 0.2261\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1086,  E(|Y-Yhat|): 0.2161,  E(|Yhat-Yhat'|): 0.2150\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1078,  E(|Y-Yhat|): 0.2182,  E(|Yhat-Yhat'|): 0.2209\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1113,  E(|Y-Yhat|): 0.2201,  E(|Yhat-Yhat'|): 0.2175\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1074,  E(|Y-Yhat|): 0.2197,  E(|Yhat-Yhat'|): 0.2247\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0069,  E(|Y-Yhat|): 0.0139,  E(|Yhat-Yhat'|): 0.0142\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3992,  E(|Y-Yhat|): 0.6666,  E(|Yhat-Yhat'|): 0.5349\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1983,  E(|Y-Yhat|): 0.4102,  E(|Yhat-Yhat'|): 0.4237\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1915,  E(|Y-Yhat|): 0.3942,  E(|Yhat-Yhat'|): 0.4053\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.2030,  E(|Y-Yhat|): 0.4008,  E(|Yhat-Yhat'|): 0.3956\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.3921,  E(|Yhat-Yhat'|): 0.3924\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1957,  E(|Y-Yhat|): 0.3893,  E(|Yhat-Yhat'|): 0.3873\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0136,  E(|Y-Yhat|): 0.0272,  E(|Yhat-Yhat'|): 0.0271\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2265,  E(|Y-Yhat|): 0.4189,  E(|Yhat-Yhat'|): 0.3847\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1342,  E(|Y-Yhat|): 0.2755,  E(|Yhat-Yhat'|): 0.2827\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1371,  E(|Y-Yhat|): 0.2721,  E(|Yhat-Yhat'|): 0.2701\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1364,  E(|Y-Yhat|): 0.2756,  E(|Yhat-Yhat'|): 0.2786\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1362,  E(|Y-Yhat|): 0.2764,  E(|Yhat-Yhat'|): 0.2802\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1364,  E(|Y-Yhat|): 0.2746,  E(|Yhat-Yhat'|): 0.2764\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0130,  E(|Y-Yhat|): 0.0260,  E(|Yhat-Yhat'|): 0.0260\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4307,  E(|Y-Yhat|): 0.7670,  E(|Yhat-Yhat'|): 0.6726\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.2416,  E(|Y-Yhat|): 0.4918,  E(|Yhat-Yhat'|): 0.5004\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2374,  E(|Y-Yhat|): 0.4814,  E(|Yhat-Yhat'|): 0.4880\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.2472,  E(|Y-Yhat|): 0.4863,  E(|Yhat-Yhat'|): 0.4781\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.2443,  E(|Y-Yhat|): 0.4735,  E(|Yhat-Yhat'|): 0.4585\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.2420,  E(|Y-Yhat|): 0.4878,  E(|Yhat-Yhat'|): 0.4917\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0064,  E(|Y-Yhat|): 0.0130,  E(|Yhat-Yhat'|): 0.0133\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8440,  E(|Y-Yhat|): 1.1678,  E(|Yhat-Yhat'|): 0.6478\n",
      "[Epoch 100 (20%), batch 9] energy-loss: -8.6169,  E(|Y-Yhat|): 466.9802,  E(|Yhat-Yhat'|): 951.1942\n",
      "[Epoch 200 (40%), batch 9] energy-loss: -9.1783,  E(|Y-Yhat|): 1450.3800,  E(|Yhat-Yhat'|): 2919.1166\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 3.9672,  E(|Y-Yhat|): 861.5723,  E(|Yhat-Yhat'|): 1715.2103\n",
      "[Epoch 400 (80%), batch 9] energy-loss: -42.0029,  E(|Y-Yhat|): 2559.0476,  E(|Yhat-Yhat'|): 5202.1010\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 20.2275,  E(|Y-Yhat|): 2414.6753,  E(|Yhat-Yhat'|): 4788.8956\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2635,  E(|Y-Yhat|): 22.6703,  E(|Yhat-Yhat'|): 44.8135\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1392,  E(|Y-Yhat|): 0.2450,  E(|Yhat-Yhat'|): 0.2115\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.0705,  E(|Y-Yhat|): 0.1469,  E(|Yhat-Yhat'|): 0.1528\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.0680,  E(|Y-Yhat|): 0.1462,  E(|Yhat-Yhat'|): 0.1564\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.0725,  E(|Y-Yhat|): 0.1488,  E(|Yhat-Yhat'|): 0.1525\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.0701,  E(|Y-Yhat|): 0.1416,  E(|Yhat-Yhat'|): 0.1429\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.0691,  E(|Y-Yhat|): 0.1428,  E(|Yhat-Yhat'|): 0.1473\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0029,  E(|Y-Yhat|): 0.0056,  E(|Yhat-Yhat'|): 0.0054\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2130,  E(|Y-Yhat|): 0.3669,  E(|Yhat-Yhat'|): 0.3079\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1579,  E(|Y-Yhat|): 0.3205,  E(|Yhat-Yhat'|): 0.3251\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1534,  E(|Y-Yhat|): 0.3183,  E(|Yhat-Yhat'|): 0.3298\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1558,  E(|Y-Yhat|): 0.3116,  E(|Yhat-Yhat'|): 0.3116\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1507,  E(|Y-Yhat|): 0.3165,  E(|Yhat-Yhat'|): 0.3316\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1571,  E(|Y-Yhat|): 0.3121,  E(|Yhat-Yhat'|): 0.3101\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0056,  E(|Y-Yhat|): 0.0108,  E(|Yhat-Yhat'|): 0.0104\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1433,  E(|Y-Yhat|): 0.2597,  E(|Yhat-Yhat'|): 0.2329\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.0756,  E(|Y-Yhat|): 0.1578,  E(|Yhat-Yhat'|): 0.1643\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.0755,  E(|Y-Yhat|): 0.1529,  E(|Yhat-Yhat'|): 0.1548\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.0780,  E(|Y-Yhat|): 0.1573,  E(|Yhat-Yhat'|): 0.1587\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.0798,  E(|Y-Yhat|): 0.1555,  E(|Yhat-Yhat'|): 0.1515\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.0764,  E(|Y-Yhat|): 0.1547,  E(|Yhat-Yhat'|): 0.1566\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0053,  E(|Y-Yhat|): 0.0096,  E(|Yhat-Yhat'|): 0.0085\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4996,  E(|Y-Yhat|): 0.8924,  E(|Yhat-Yhat'|): 0.7855\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3100,  E(|Y-Yhat|): 0.6331,  E(|Yhat-Yhat'|): 0.6461\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3046,  E(|Y-Yhat|): 0.6190,  E(|Yhat-Yhat'|): 0.6288\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3111,  E(|Y-Yhat|): 0.6259,  E(|Yhat-Yhat'|): 0.6295\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3144,  E(|Y-Yhat|): 0.6190,  E(|Yhat-Yhat'|): 0.6090\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3106,  E(|Y-Yhat|): 0.6158,  E(|Yhat-Yhat'|): 0.6103\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0077,  E(|Y-Yhat|): 0.0147,  E(|Yhat-Yhat'|): 0.0140\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7713,  E(|Y-Yhat|): 1.0222,  E(|Yhat-Yhat'|): 0.5018\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.5307,  E(|Y-Yhat|): 10.4057,  E(|Yhat-Yhat'|): 19.7501\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3369,  E(|Y-Yhat|): 28.0974,  E(|Yhat-Yhat'|): 55.5209\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.4362,  E(|Y-Yhat|): 11.0168,  E(|Yhat-Yhat'|): 21.1613\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.4193,  E(|Y-Yhat|): 17.9514,  E(|Yhat-Yhat'|): 35.0641\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.2014,  E(|Y-Yhat|): 109.0213,  E(|Yhat-Yhat'|): 217.6398\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0316,  E(|Y-Yhat|): 2.0930,  E(|Yhat-Yhat'|): 4.1227\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1859,  E(|Y-Yhat|): 0.3431,  E(|Yhat-Yhat'|): 0.3144\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1044,  E(|Y-Yhat|): 0.2080,  E(|Yhat-Yhat'|): 0.2072\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1039,  E(|Y-Yhat|): 0.2108,  E(|Yhat-Yhat'|): 0.2139\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1026,  E(|Y-Yhat|): 0.2066,  E(|Yhat-Yhat'|): 0.2081\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1026,  E(|Y-Yhat|): 0.2101,  E(|Yhat-Yhat'|): 0.2151\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1065,  E(|Y-Yhat|): 0.2089,  E(|Yhat-Yhat'|): 0.2049\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0038,  E(|Y-Yhat|): 0.0072,  E(|Yhat-Yhat'|): 0.0068\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4060,  E(|Y-Yhat|): 0.6800,  E(|Yhat-Yhat'|): 0.5481\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1919,  E(|Y-Yhat|): 0.3912,  E(|Yhat-Yhat'|): 0.3987\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1987,  E(|Y-Yhat|): 0.3947,  E(|Yhat-Yhat'|): 0.3922\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1939,  E(|Y-Yhat|): 0.3867,  E(|Yhat-Yhat'|): 0.3856\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1924,  E(|Y-Yhat|): 0.3864,  E(|Yhat-Yhat'|): 0.3880\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1945,  E(|Y-Yhat|): 0.3910,  E(|Yhat-Yhat'|): 0.3930\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0073,  E(|Y-Yhat|): 0.0144,  E(|Yhat-Yhat'|): 0.0143\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2070,  E(|Y-Yhat|): 0.3850,  E(|Yhat-Yhat'|): 0.3560\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1250,  E(|Y-Yhat|): 0.2536,  E(|Yhat-Yhat'|): 0.2571\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1242,  E(|Y-Yhat|): 0.2522,  E(|Yhat-Yhat'|): 0.2560\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1229,  E(|Y-Yhat|): 0.2475,  E(|Yhat-Yhat'|): 0.2493\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1232,  E(|Y-Yhat|): 0.2485,  E(|Yhat-Yhat'|): 0.2507\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1263,  E(|Y-Yhat|): 0.2479,  E(|Yhat-Yhat'|): 0.2433\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0071,  E(|Y-Yhat|): 0.0137,  E(|Yhat-Yhat'|): 0.0132\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3582,  E(|Y-Yhat|): 0.5902,  E(|Yhat-Yhat'|): 0.4641\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.2322,  E(|Y-Yhat|): 0.4791,  E(|Yhat-Yhat'|): 0.4938\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2365,  E(|Y-Yhat|): 0.4778,  E(|Yhat-Yhat'|): 0.4826\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.2413,  E(|Y-Yhat|): 0.4610,  E(|Yhat-Yhat'|): 0.4395\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.2308,  E(|Y-Yhat|): 0.4564,  E(|Yhat-Yhat'|): 0.4513\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.2389,  E(|Y-Yhat|): 0.4687,  E(|Yhat-Yhat'|): 0.4595\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1580,  E(|Y-Yhat|): 0.3072,  E(|Yhat-Yhat'|): 0.2985\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9328,  E(|Y-Yhat|): 1.1880,  E(|Yhat-Yhat'|): 0.5104\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.6640,  E(|Y-Yhat|): 14.8541,  E(|Yhat-Yhat'|): 28.3802\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.7030,  E(|Y-Yhat|): 3.0047,  E(|Yhat-Yhat'|): 4.6032\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.7186,  E(|Y-Yhat|): 1.5793,  E(|Yhat-Yhat'|): 1.7215\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.7030,  E(|Y-Yhat|): 3.3416,  E(|Yhat-Yhat'|): 5.2774\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.7111,  E(|Y-Yhat|): 2.7586,  E(|Yhat-Yhat'|): 4.0950\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1650,  E(|Y-Yhat|): 0.5882,  E(|Yhat-Yhat'|): 0.8465\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.0910,  E(|Y-Yhat|): 0.1754,  E(|Yhat-Yhat'|): 0.1689\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.0693,  E(|Y-Yhat|): 0.1380,  E(|Yhat-Yhat'|): 0.1375\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.0668,  E(|Y-Yhat|): 0.1401,  E(|Yhat-Yhat'|): 0.1466\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.0693,  E(|Y-Yhat|): 0.1338,  E(|Yhat-Yhat'|): 0.1289\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.0682,  E(|Y-Yhat|): 0.1325,  E(|Yhat-Yhat'|): 0.1287\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.0693,  E(|Y-Yhat|): 0.1407,  E(|Yhat-Yhat'|): 0.1429\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0339,  E(|Y-Yhat|): 0.0691,  E(|Yhat-Yhat'|): 0.0703\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2398,  E(|Y-Yhat|): 0.4097,  E(|Yhat-Yhat'|): 0.3399\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1527,  E(|Y-Yhat|): 0.3119,  E(|Yhat-Yhat'|): 0.3184\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1495,  E(|Y-Yhat|): 0.3089,  E(|Yhat-Yhat'|): 0.3189\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1496,  E(|Y-Yhat|): 0.3105,  E(|Yhat-Yhat'|): 0.3218\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1498,  E(|Y-Yhat|): 0.3034,  E(|Yhat-Yhat'|): 0.3073\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1485,  E(|Y-Yhat|): 0.2983,  E(|Yhat-Yhat'|): 0.2995\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0677,  E(|Y-Yhat|): 0.1351,  E(|Yhat-Yhat'|): 0.1348\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1549,  E(|Y-Yhat|): 0.2903,  E(|Yhat-Yhat'|): 0.2707\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.0722,  E(|Y-Yhat|): 0.1483,  E(|Yhat-Yhat'|): 0.1522\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.0715,  E(|Y-Yhat|): 0.1487,  E(|Yhat-Yhat'|): 0.1545\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.0729,  E(|Y-Yhat|): 0.1477,  E(|Yhat-Yhat'|): 0.1497\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.0723,  E(|Y-Yhat|): 0.1478,  E(|Yhat-Yhat'|): 0.1509\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.0729,  E(|Y-Yhat|): 0.1473,  E(|Yhat-Yhat'|): 0.1488\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0414,  E(|Y-Yhat|): 0.0809,  E(|Yhat-Yhat'|): 0.0790\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5397,  E(|Y-Yhat|): 0.9841,  E(|Yhat-Yhat'|): 0.8888\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3164,  E(|Y-Yhat|): 0.6499,  E(|Yhat-Yhat'|): 0.6671\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3207,  E(|Y-Yhat|): 0.6440,  E(|Yhat-Yhat'|): 0.6467\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3202,  E(|Y-Yhat|): 0.6407,  E(|Yhat-Yhat'|): 0.6410\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3205,  E(|Y-Yhat|): 0.6517,  E(|Yhat-Yhat'|): 0.6623\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3228,  E(|Y-Yhat|): 0.6547,  E(|Yhat-Yhat'|): 0.6637\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0022,  E(|Y-Yhat|): 0.0047,  E(|Yhat-Yhat'|): 0.0050\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8265,  E(|Y-Yhat|): 1.0617,  E(|Yhat-Yhat'|): 0.4703\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3889,  E(|Y-Yhat|): 20.3073,  E(|Yhat-Yhat'|): 39.8367\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 3.1506,  E(|Y-Yhat|): 107.2600,  E(|Yhat-Yhat'|): 208.2187\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 11.1564,  E(|Y-Yhat|): 817.0510,  E(|Yhat-Yhat'|): 1611.7891\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 3.6290,  E(|Y-Yhat|): 295.3892,  E(|Yhat-Yhat'|): 583.5205\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 3.8169,  E(|Y-Yhat|): 243.4675,  E(|Yhat-Yhat'|): 479.3011\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0188,  E(|Y-Yhat|): 1.6705,  E(|Yhat-Yhat'|): 3.3035\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2021,  E(|Y-Yhat|): 0.3717,  E(|Yhat-Yhat'|): 0.3392\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1091,  E(|Y-Yhat|): 0.2232,  E(|Yhat-Yhat'|): 0.2282\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1065,  E(|Y-Yhat|): 0.2226,  E(|Yhat-Yhat'|): 0.2322\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1103,  E(|Y-Yhat|): 0.2202,  E(|Yhat-Yhat'|): 0.2198\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1075,  E(|Y-Yhat|): 0.2226,  E(|Yhat-Yhat'|): 0.2303\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1126,  E(|Y-Yhat|): 0.2207,  E(|Yhat-Yhat'|): 0.2162\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0011,  E(|Y-Yhat|): 0.0023,  E(|Yhat-Yhat'|): 0.0023\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3961,  E(|Y-Yhat|): 0.6702,  E(|Yhat-Yhat'|): 0.5481\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.4032,  E(|Yhat-Yhat'|): 0.4148\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2023,  E(|Y-Yhat|): 0.4067,  E(|Yhat-Yhat'|): 0.4090\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1998,  E(|Y-Yhat|): 0.4015,  E(|Yhat-Yhat'|): 0.4035\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1962,  E(|Y-Yhat|): 0.3889,  E(|Yhat-Yhat'|): 0.3853\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.2004,  E(|Y-Yhat|): 0.3982,  E(|Yhat-Yhat'|): 0.3956\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0023,  E(|Y-Yhat|): 0.0046,  E(|Yhat-Yhat'|): 0.0046\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2280,  E(|Y-Yhat|): 0.4253,  E(|Yhat-Yhat'|): 0.3946\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1381,  E(|Y-Yhat|): 0.2798,  E(|Yhat-Yhat'|): 0.2833\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1394,  E(|Y-Yhat|): 0.2768,  E(|Yhat-Yhat'|): 0.2747\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1413,  E(|Y-Yhat|): 0.2754,  E(|Yhat-Yhat'|): 0.2683\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1375,  E(|Y-Yhat|): 0.2808,  E(|Yhat-Yhat'|): 0.2866\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1384,  E(|Y-Yhat|): 0.2726,  E(|Yhat-Yhat'|): 0.2684\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0023,  E(|Y-Yhat|): 0.0045,  E(|Yhat-Yhat'|): 0.0044\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5340,  E(|Y-Yhat|): 0.9413,  E(|Yhat-Yhat'|): 0.8146\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3217,  E(|Y-Yhat|): 0.6578,  E(|Yhat-Yhat'|): 0.6721\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3241,  E(|Y-Yhat|): 0.6468,  E(|Yhat-Yhat'|): 0.6453\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3184,  E(|Y-Yhat|): 0.6449,  E(|Yhat-Yhat'|): 0.6530\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3198,  E(|Y-Yhat|): 0.6514,  E(|Yhat-Yhat'|): 0.6632\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3177,  E(|Y-Yhat|): 0.6396,  E(|Yhat-Yhat'|): 0.6438\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0508,  E(|Y-Yhat|): 0.1034,  E(|Yhat-Yhat'|): 0.1051\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8649,  E(|Y-Yhat|): 1.0673,  E(|Yhat-Yhat'|): 0.4047\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.2628,  E(|Y-Yhat|): 34.8863,  E(|Yhat-Yhat'|): 69.2471\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.4149,  E(|Y-Yhat|): 32.3335,  E(|Yhat-Yhat'|): 63.8374\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.7502,  E(|Y-Yhat|): 84.6449,  E(|Yhat-Yhat'|): 167.7893\n",
      "[Epoch 400 (80%), batch 9] energy-loss: -11.8845,  E(|Y-Yhat|): 1464.0187,  E(|Yhat-Yhat'|): 2951.8065\n",
      "[Epoch 500 (100%), batch 9] energy-loss: -8.0801,  E(|Y-Yhat|): 686.0776,  E(|Yhat-Yhat'|): 1388.3153\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9214,  E(|Y-Yhat|): 108.4950,  E(|Yhat-Yhat'|): 215.1472\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2104,  E(|Y-Yhat|): 0.3652,  E(|Yhat-Yhat'|): 0.3097\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1035,  E(|Y-Yhat|): 0.2214,  E(|Yhat-Yhat'|): 0.2357\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1082,  E(|Y-Yhat|): 0.2248,  E(|Yhat-Yhat'|): 0.2332\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1107,  E(|Y-Yhat|): 0.2212,  E(|Yhat-Yhat'|): 0.2211\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1120,  E(|Y-Yhat|): 0.2219,  E(|Yhat-Yhat'|): 0.2196\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1082,  E(|Y-Yhat|): 0.2215,  E(|Yhat-Yhat'|): 0.2266\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0237,  E(|Y-Yhat|): 0.0477,  E(|Yhat-Yhat'|): 0.0479\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3826,  E(|Y-Yhat|): 0.6752,  E(|Yhat-Yhat'|): 0.5853\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.2009,  E(|Y-Yhat|): 0.4066,  E(|Yhat-Yhat'|): 0.4115\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1952,  E(|Y-Yhat|): 0.3963,  E(|Yhat-Yhat'|): 0.4021\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.2049,  E(|Y-Yhat|): 0.4056,  E(|Yhat-Yhat'|): 0.4014\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1969,  E(|Y-Yhat|): 0.4047,  E(|Yhat-Yhat'|): 0.4157\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1934,  E(|Y-Yhat|): 0.3926,  E(|Yhat-Yhat'|): 0.3984\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0434,  E(|Y-Yhat|): 0.0899,  E(|Yhat-Yhat'|): 0.0929\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2284,  E(|Y-Yhat|): 0.4196,  E(|Yhat-Yhat'|): 0.3825\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1371,  E(|Y-Yhat|): 0.2774,  E(|Yhat-Yhat'|): 0.2804\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1374,  E(|Y-Yhat|): 0.2831,  E(|Yhat-Yhat'|): 0.2913\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1401,  E(|Y-Yhat|): 0.2769,  E(|Yhat-Yhat'|): 0.2736\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1363,  E(|Y-Yhat|): 0.2778,  E(|Yhat-Yhat'|): 0.2830\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1360,  E(|Y-Yhat|): 0.2754,  E(|Yhat-Yhat'|): 0.2788\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0398,  E(|Y-Yhat|): 0.0812,  E(|Yhat-Yhat'|): 0.0827\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5301,  E(|Y-Yhat|): 0.9800,  E(|Yhat-Yhat'|): 0.8997\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.3248,  E(|Y-Yhat|): 0.6463,  E(|Yhat-Yhat'|): 0.6431\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.3204,  E(|Y-Yhat|): 0.6553,  E(|Yhat-Yhat'|): 0.6698\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.3217,  E(|Y-Yhat|): 0.6575,  E(|Yhat-Yhat'|): 0.6716\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.3240,  E(|Y-Yhat|): 0.6493,  E(|Yhat-Yhat'|): 0.6506\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.3173,  E(|Y-Yhat|): 0.6460,  E(|Yhat-Yhat'|): 0.6573\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0021,  E(|Y-Yhat|): 0.0041,  E(|Yhat-Yhat'|): 0.0042\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7718,  E(|Y-Yhat|): 1.0080,  E(|Yhat-Yhat'|): 0.4724\n",
      "[Epoch 100 (20%), batch 9] energy-loss: -0.1139,  E(|Y-Yhat|): 29.6402,  E(|Yhat-Yhat'|): 59.5081\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2708,  E(|Y-Yhat|): 7.0078,  E(|Yhat-Yhat'|): 13.4740\n",
      "[Epoch 300 (60%), batch 9] energy-loss: -0.7621,  E(|Y-Yhat|): 76.0783,  E(|Yhat-Yhat'|): 153.6808\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 1.3377,  E(|Y-Yhat|): 200.1197,  E(|Yhat-Yhat'|): 397.5639\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.5068,  E(|Y-Yhat|): 89.9660,  E(|Yhat-Yhat'|): 178.9184\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0130,  E(|Y-Yhat|): 0.5609,  E(|Yhat-Yhat'|): 1.1477\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2136,  E(|Y-Yhat|): 0.3764,  E(|Yhat-Yhat'|): 0.3257\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1130,  E(|Y-Yhat|): 0.2258,  E(|Yhat-Yhat'|): 0.2256\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1101,  E(|Y-Yhat|): 0.2231,  E(|Yhat-Yhat'|): 0.2260\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1111,  E(|Y-Yhat|): 0.2221,  E(|Yhat-Yhat'|): 0.2218\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1108,  E(|Y-Yhat|): 0.2208,  E(|Yhat-Yhat'|): 0.2200\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1108,  E(|Y-Yhat|): 0.2173,  E(|Yhat-Yhat'|): 0.2130\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0010,  E(|Y-Yhat|): 0.0020,  E(|Yhat-Yhat'|): 0.0021\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3729,  E(|Y-Yhat|): 0.6889,  E(|Yhat-Yhat'|): 0.6320\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1962,  E(|Y-Yhat|): 0.4091,  E(|Yhat-Yhat'|): 0.4260\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.2014,  E(|Y-Yhat|): 0.4086,  E(|Yhat-Yhat'|): 0.4144\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1970,  E(|Y-Yhat|): 0.3949,  E(|Yhat-Yhat'|): 0.3959\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.2053,  E(|Y-Yhat|): 0.4097,  E(|Yhat-Yhat'|): 0.4087\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.2014,  E(|Y-Yhat|): 0.4090,  E(|Yhat-Yhat'|): 0.4152\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0020,  E(|Y-Yhat|): 0.0042,  E(|Yhat-Yhat'|): 0.0044\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2278,  E(|Y-Yhat|): 0.4088,  E(|Yhat-Yhat'|): 0.3620\n",
      "[Epoch 100 (20%), batch 9] energy-loss: 0.1392,  E(|Y-Yhat|): 0.2796,  E(|Yhat-Yhat'|): 0.2807\n",
      "[Epoch 200 (40%), batch 9] energy-loss: 0.1408,  E(|Y-Yhat|): 0.2811,  E(|Yhat-Yhat'|): 0.2807\n",
      "[Epoch 300 (60%), batch 9] energy-loss: 0.1419,  E(|Y-Yhat|): 0.2828,  E(|Yhat-Yhat'|): 0.2819\n",
      "[Epoch 400 (80%), batch 9] energy-loss: 0.1385,  E(|Y-Yhat|): 0.2831,  E(|Yhat-Yhat'|): 0.2893\n",
      "[Epoch 500 (100%), batch 9] energy-loss: 0.1417,  E(|Y-Yhat|): 0.2804,  E(|Yhat-Yhat'|): 0.2774\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0019,  E(|Y-Yhat|): 0.0040,  E(|Yhat-Yhat'|): 0.0040\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=1, dy=1, k=1, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(0, 5, 100)\n",
    "    y_eval = M0* F.softplus(A0 * x_eval)\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=500, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=500, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=500, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=500, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=500, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfee26c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0001), tensor(0.0699))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4364b95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0002), tensor(0.0002), tensor(0.0001))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb53dc",
   "metadata": {},
   "source": [
    "## True function: cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f16ac9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4030,  E(|Y-Yhat|): 0.6842,  E(|Yhat-Yhat'|): 0.5625\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3277,  E(|Y-Yhat|): 0.6421,  E(|Yhat-Yhat'|): 0.6289\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3227,  E(|Y-Yhat|): 0.6392,  E(|Yhat-Yhat'|): 0.6331\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3212,  E(|Y-Yhat|): 0.6420,  E(|Yhat-Yhat'|): 0.6417\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 11.3009,  E(|Y-Yhat|): 21.4782,  E(|Yhat-Yhat'|): 20.3546\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8393,  E(|Y-Yhat|): 1.0340,  E(|Yhat-Yhat'|): 0.3894\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8976,  E(|Y-Yhat|): 20.0288,  E(|Yhat-Yhat'|): 38.2624\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.0963,  E(|Y-Yhat|): 72.2296,  E(|Yhat-Yhat'|): 144.6518\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7843,  E(|Y-Yhat|): 28.7266,  E(|Yhat-Yhat'|): 55.8846\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.3912,  E(|Y-Yhat|): 463.9224,  E(|Yhat-Yhat'|): 919.0623\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1354,  E(|Y-Yhat|): 0.2483,  E(|Yhat-Yhat'|): 0.2258\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1010,  E(|Y-Yhat|): 0.2088,  E(|Yhat-Yhat'|): 0.2157\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0988,  E(|Y-Yhat|): 0.2002,  E(|Yhat-Yhat'|): 0.2028\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0985,  E(|Y-Yhat|): 0.2016,  E(|Yhat-Yhat'|): 0.2061\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2524,  E(|Y-Yhat|): 0.5377,  E(|Yhat-Yhat'|): 0.5707\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2686,  E(|Y-Yhat|): 0.4502,  E(|Yhat-Yhat'|): 0.3634\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2075,  E(|Y-Yhat|): 0.4035,  E(|Yhat-Yhat'|): 0.3920\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2074,  E(|Y-Yhat|): 0.4165,  E(|Yhat-Yhat'|): 0.4183\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2013,  E(|Y-Yhat|): 0.4154,  E(|Yhat-Yhat'|): 0.4281\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7300,  E(|Y-Yhat|): 1.4959,  E(|Yhat-Yhat'|): 1.5319\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1630,  E(|Y-Yhat|): 0.2946,  E(|Yhat-Yhat'|): 0.2633\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1184,  E(|Y-Yhat|): 0.2298,  E(|Yhat-Yhat'|): 0.2228\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1163,  E(|Y-Yhat|): 0.2303,  E(|Yhat-Yhat'|): 0.2280\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1176,  E(|Y-Yhat|): 0.2272,  E(|Yhat-Yhat'|): 0.2191\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2568,  E(|Y-Yhat|): 0.5141,  E(|Yhat-Yhat'|): 0.5144\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3744,  E(|Y-Yhat|): 0.6200,  E(|Yhat-Yhat'|): 0.4912\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3109,  E(|Y-Yhat|): 0.5959,  E(|Yhat-Yhat'|): 0.5701\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3158,  E(|Y-Yhat|): 0.6154,  E(|Yhat-Yhat'|): 0.5993\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3114,  E(|Y-Yhat|): 0.6092,  E(|Yhat-Yhat'|): 0.5955\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0636,  E(|Y-Yhat|): 0.1102,  E(|Yhat-Yhat'|): 0.0934\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8963,  E(|Y-Yhat|): 1.1557,  E(|Yhat-Yhat'|): 0.5187\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6434,  E(|Y-Yhat|): 7.0632,  E(|Yhat-Yhat'|): 12.8395\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6643,  E(|Y-Yhat|): 41.0534,  E(|Yhat-Yhat'|): 80.7782\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8957,  E(|Y-Yhat|): 74.0186,  E(|Yhat-Yhat'|): 146.2458\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1455,  E(|Y-Yhat|): 6.3995,  E(|Yhat-Yhat'|): 12.5081\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1126,  E(|Y-Yhat|): 0.2188,  E(|Yhat-Yhat'|): 0.2125\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0972,  E(|Y-Yhat|): 0.1925,  E(|Yhat-Yhat'|): 0.1906\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0956,  E(|Y-Yhat|): 0.1868,  E(|Yhat-Yhat'|): 0.1824\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0969,  E(|Y-Yhat|): 0.1929,  E(|Yhat-Yhat'|): 0.1919\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0221,  E(|Y-Yhat|): 0.0393,  E(|Yhat-Yhat'|): 0.0345\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2562,  E(|Y-Yhat|): 0.4539,  E(|Yhat-Yhat'|): 0.3953\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1973,  E(|Y-Yhat|): 0.3932,  E(|Yhat-Yhat'|): 0.3918\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1987,  E(|Y-Yhat|): 0.3931,  E(|Yhat-Yhat'|): 0.3888\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1964,  E(|Y-Yhat|): 0.3938,  E(|Yhat-Yhat'|): 0.3948\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0417,  E(|Y-Yhat|): 0.0807,  E(|Yhat-Yhat'|): 0.0780\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1318,  E(|Y-Yhat|): 0.2523,  E(|Yhat-Yhat'|): 0.2410\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1126,  E(|Y-Yhat|): 0.2226,  E(|Yhat-Yhat'|): 0.2200\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1097,  E(|Y-Yhat|): 0.2200,  E(|Yhat-Yhat'|): 0.2205\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1110,  E(|Y-Yhat|): 0.2243,  E(|Yhat-Yhat'|): 0.2267\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0317,  E(|Y-Yhat|): 0.0616,  E(|Yhat-Yhat'|): 0.0597\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4289,  E(|Y-Yhat|): 0.7129,  E(|Yhat-Yhat'|): 0.5679\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3193,  E(|Y-Yhat|): 0.6197,  E(|Yhat-Yhat'|): 0.6007\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3118,  E(|Y-Yhat|): 0.6136,  E(|Yhat-Yhat'|): 0.6035\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3172,  E(|Y-Yhat|): 0.6184,  E(|Yhat-Yhat'|): 0.6023\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0021,  E(|Y-Yhat|): 0.0038,  E(|Yhat-Yhat'|): 0.0033\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9523,  E(|Y-Yhat|): 1.1454,  E(|Yhat-Yhat'|): 0.3863\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6250,  E(|Y-Yhat|): 11.2350,  E(|Yhat-Yhat'|): 21.2200\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9889,  E(|Y-Yhat|): 11.9646,  E(|Yhat-Yhat'|): 21.9513\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3585,  E(|Y-Yhat|): 28.0519,  E(|Yhat-Yhat'|): 55.3867\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0019,  E(|Y-Yhat|): 0.0942,  E(|Yhat-Yhat'|): 0.1846\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1184,  E(|Y-Yhat|): 0.2237,  E(|Yhat-Yhat'|): 0.2106\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0957,  E(|Y-Yhat|): 0.1870,  E(|Yhat-Yhat'|): 0.1825\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0955,  E(|Y-Yhat|): 0.1866,  E(|Yhat-Yhat'|): 0.1823\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0979,  E(|Y-Yhat|): 0.1936,  E(|Yhat-Yhat'|): 0.1914\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0010,  E(|Y-Yhat|): 0.0017,  E(|Yhat-Yhat'|): 0.0014\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2339,  E(|Y-Yhat|): 0.4082,  E(|Yhat-Yhat'|): 0.3487\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1998,  E(|Y-Yhat|): 0.3841,  E(|Yhat-Yhat'|): 0.3685\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1982,  E(|Y-Yhat|): 0.3938,  E(|Yhat-Yhat'|): 0.3912\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2004,  E(|Y-Yhat|): 0.3994,  E(|Yhat-Yhat'|): 0.3980\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0020,  E(|Y-Yhat|): 0.0036,  E(|Yhat-Yhat'|): 0.0031\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1316,  E(|Y-Yhat|): 0.2482,  E(|Yhat-Yhat'|): 0.2333\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1103,  E(|Y-Yhat|): 0.2172,  E(|Yhat-Yhat'|): 0.2137\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1069,  E(|Y-Yhat|): 0.2178,  E(|Yhat-Yhat'|): 0.2217\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1086,  E(|Y-Yhat|): 0.2160,  E(|Yhat-Yhat'|): 0.2150\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0020,  E(|Y-Yhat|): 0.0032,  E(|Yhat-Yhat'|): 0.0024\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3965,  E(|Y-Yhat|): 0.6575,  E(|Yhat-Yhat'|): 0.5219\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3265,  E(|Y-Yhat|): 0.6156,  E(|Yhat-Yhat'|): 0.5782\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3209,  E(|Y-Yhat|): 0.6292,  E(|Yhat-Yhat'|): 0.6167\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3183,  E(|Y-Yhat|): 0.6219,  E(|Yhat-Yhat'|): 0.6071\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0853,  E(|Y-Yhat|): 0.1557,  E(|Yhat-Yhat'|): 0.1408\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8883,  E(|Y-Yhat|): 1.1265,  E(|Yhat-Yhat'|): 0.4764\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2908,  E(|Y-Yhat|): 79.9622,  E(|Yhat-Yhat'|): 157.3427\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6013,  E(|Y-Yhat|): 39.1787,  E(|Yhat-Yhat'|): 77.1548\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5568,  E(|Y-Yhat|): 163.6713,  E(|Yhat-Yhat'|): 326.2290\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3300,  E(|Y-Yhat|): 20.5243,  E(|Yhat-Yhat'|): 40.3887\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1193,  E(|Y-Yhat|): 0.2196,  E(|Yhat-Yhat'|): 0.2006\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0986,  E(|Y-Yhat|): 0.1931,  E(|Yhat-Yhat'|): 0.1889\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0953,  E(|Y-Yhat|): 0.1944,  E(|Yhat-Yhat'|): 0.1982\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0983,  E(|Y-Yhat|): 0.1946,  E(|Yhat-Yhat'|): 0.1927\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0285,  E(|Y-Yhat|): 0.0519,  E(|Yhat-Yhat'|): 0.0467\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2435,  E(|Y-Yhat|): 0.4229,  E(|Yhat-Yhat'|): 0.3589\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2068,  E(|Y-Yhat|): 0.4073,  E(|Yhat-Yhat'|): 0.4011\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2034,  E(|Y-Yhat|): 0.4132,  E(|Yhat-Yhat'|): 0.4196\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1943,  E(|Y-Yhat|): 0.4038,  E(|Yhat-Yhat'|): 0.4190\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0526,  E(|Y-Yhat|): 0.1045,  E(|Yhat-Yhat'|): 0.1037\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1564,  E(|Y-Yhat|): 0.2867,  E(|Yhat-Yhat'|): 0.2606\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1116,  E(|Y-Yhat|): 0.2257,  E(|Yhat-Yhat'|): 0.2282\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1105,  E(|Y-Yhat|): 0.2230,  E(|Yhat-Yhat'|): 0.2248\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1122,  E(|Y-Yhat|): 0.2216,  E(|Yhat-Yhat'|): 0.2186\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0388,  E(|Y-Yhat|): 0.0731,  E(|Yhat-Yhat'|): 0.0685\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4151,  E(|Y-Yhat|): 0.7210,  E(|Yhat-Yhat'|): 0.6118\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3204,  E(|Y-Yhat|): 0.6347,  E(|Yhat-Yhat'|): 0.6285\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3137,  E(|Y-Yhat|): 0.6221,  E(|Yhat-Yhat'|): 0.6167\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3209,  E(|Y-Yhat|): 0.6403,  E(|Yhat-Yhat'|): 0.6388\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 8.8287,  E(|Y-Yhat|): 16.2083,  E(|Yhat-Yhat'|): 14.7592\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8123,  E(|Y-Yhat|): 1.1239,  E(|Yhat-Yhat'|): 0.6233\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6706,  E(|Y-Yhat|): 2.5611,  E(|Yhat-Yhat'|): 3.7811\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6349,  E(|Y-Yhat|): 8.8403,  E(|Yhat-Yhat'|): 16.4108\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6797,  E(|Y-Yhat|): 5.5598,  E(|Yhat-Yhat'|): 9.7602\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 7.9850,  E(|Y-Yhat|): 73.2836,  E(|Yhat-Yhat'|): 130.5971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1402,  E(|Y-Yhat|): 0.2594,  E(|Yhat-Yhat'|): 0.2383\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1021,  E(|Y-Yhat|): 0.1960,  E(|Yhat-Yhat'|): 0.1879\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1019,  E(|Y-Yhat|): 0.1989,  E(|Yhat-Yhat'|): 0.1940\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0997,  E(|Y-Yhat|): 0.2027,  E(|Yhat-Yhat'|): 0.2060\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2320,  E(|Y-Yhat|): 0.4796,  E(|Yhat-Yhat'|): 0.4951\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2571,  E(|Y-Yhat|): 0.4271,  E(|Yhat-Yhat'|): 0.3398\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2035,  E(|Y-Yhat|): 0.4046,  E(|Yhat-Yhat'|): 0.4022\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2047,  E(|Y-Yhat|): 0.4151,  E(|Yhat-Yhat'|): 0.4209\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2007,  E(|Y-Yhat|): 0.4090,  E(|Yhat-Yhat'|): 0.4166\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6564,  E(|Y-Yhat|): 1.3298,  E(|Yhat-Yhat'|): 1.3467\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1350,  E(|Y-Yhat|): 0.2440,  E(|Yhat-Yhat'|): 0.2180\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1128,  E(|Y-Yhat|): 0.2209,  E(|Yhat-Yhat'|): 0.2162\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1131,  E(|Y-Yhat|): 0.2272,  E(|Yhat-Yhat'|): 0.2281\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1114,  E(|Y-Yhat|): 0.2216,  E(|Yhat-Yhat'|): 0.2205\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2400,  E(|Y-Yhat|): 0.4933,  E(|Yhat-Yhat'|): 0.5066\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4048,  E(|Y-Yhat|): 0.6611,  E(|Yhat-Yhat'|): 0.5126\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3168,  E(|Y-Yhat|): 0.6151,  E(|Yhat-Yhat'|): 0.5967\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3191,  E(|Y-Yhat|): 0.6265,  E(|Yhat-Yhat'|): 0.6148\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3220,  E(|Y-Yhat|): 0.6378,  E(|Yhat-Yhat'|): 0.6317\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0490,  E(|Y-Yhat|): 0.0877,  E(|Yhat-Yhat'|): 0.0775\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9124,  E(|Y-Yhat|): 1.1033,  E(|Yhat-Yhat'|): 0.3819\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2764,  E(|Y-Yhat|): 19.6153,  E(|Yhat-Yhat'|): 38.6778\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.2853,  E(|Y-Yhat|): 144.6680,  E(|Yhat-Yhat'|): 289.9066\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5896,  E(|Y-Yhat|): 16.6078,  E(|Yhat-Yhat'|): 32.0364\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0590,  E(|Y-Yhat|): 1.1802,  E(|Yhat-Yhat'|): 2.2424\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1326,  E(|Y-Yhat|): 0.2325,  E(|Yhat-Yhat'|): 0.1997\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0981,  E(|Y-Yhat|): 0.1936,  E(|Yhat-Yhat'|): 0.1909\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0992,  E(|Y-Yhat|): 0.1926,  E(|Yhat-Yhat'|): 0.1868\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0977,  E(|Y-Yhat|): 0.1953,  E(|Yhat-Yhat'|): 0.1953\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0187,  E(|Y-Yhat|): 0.0331,  E(|Yhat-Yhat'|): 0.0289\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2630,  E(|Y-Yhat|): 0.4443,  E(|Yhat-Yhat'|): 0.3626\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2041,  E(|Y-Yhat|): 0.3912,  E(|Yhat-Yhat'|): 0.3743\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2022,  E(|Y-Yhat|): 0.4000,  E(|Yhat-Yhat'|): 0.3957\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2026,  E(|Y-Yhat|): 0.4067,  E(|Yhat-Yhat'|): 0.4083\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0345,  E(|Y-Yhat|): 0.0670,  E(|Yhat-Yhat'|): 0.0651\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1323,  E(|Y-Yhat|): 0.2466,  E(|Yhat-Yhat'|): 0.2287\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1131,  E(|Y-Yhat|): 0.2262,  E(|Yhat-Yhat'|): 0.2261\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1119,  E(|Y-Yhat|): 0.2209,  E(|Yhat-Yhat'|): 0.2181\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1139,  E(|Y-Yhat|): 0.2253,  E(|Yhat-Yhat'|): 0.2228\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0270,  E(|Y-Yhat|): 0.0487,  E(|Yhat-Yhat'|): 0.0434\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3785,  E(|Y-Yhat|): 0.6444,  E(|Yhat-Yhat'|): 0.5318\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3242,  E(|Y-Yhat|): 0.6383,  E(|Yhat-Yhat'|): 0.6281\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3239,  E(|Y-Yhat|): 0.6375,  E(|Yhat-Yhat'|): 0.6273\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3225,  E(|Y-Yhat|): 0.6347,  E(|Yhat-Yhat'|): 0.6244\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 423.6908,  E(|Y-Yhat|): 771.8505,  E(|Yhat-Yhat'|): 696.3195\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8654,  E(|Y-Yhat|): 1.0587,  E(|Yhat-Yhat'|): 0.3865\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8670,  E(|Y-Yhat|): 13.2795,  E(|Yhat-Yhat'|): 24.8250\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4509,  E(|Y-Yhat|): 35.5944,  E(|Yhat-Yhat'|): 68.2872\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0764,  E(|Y-Yhat|): 44.6307,  E(|Yhat-Yhat'|): 89.1088\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 943.4727,  E(|Y-Yhat|): 29429.0312,  E(|Yhat-Yhat'|): 56971.1172\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1282,  E(|Y-Yhat|): 0.2346,  E(|Yhat-Yhat'|): 0.2129\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1024,  E(|Y-Yhat|): 0.2008,  E(|Yhat-Yhat'|): 0.1969\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1007,  E(|Y-Yhat|): 0.1978,  E(|Yhat-Yhat'|): 0.1942\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1009,  E(|Y-Yhat|): 0.1998,  E(|Yhat-Yhat'|): 0.1978\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4297,  E(|Y-Yhat|): 0.8715,  E(|Yhat-Yhat'|): 0.8835\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2535,  E(|Y-Yhat|): 0.4038,  E(|Yhat-Yhat'|): 0.3007\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2081,  E(|Y-Yhat|): 0.4037,  E(|Yhat-Yhat'|): 0.3912\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2016,  E(|Y-Yhat|): 0.4113,  E(|Yhat-Yhat'|): 0.4193\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2071,  E(|Y-Yhat|): 0.4122,  E(|Yhat-Yhat'|): 0.4102\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9905,  E(|Y-Yhat|): 4.0943,  E(|Yhat-Yhat'|): 4.2076\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1543,  E(|Y-Yhat|): 0.2775,  E(|Yhat-Yhat'|): 0.2464\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1147,  E(|Y-Yhat|): 0.2267,  E(|Yhat-Yhat'|): 0.2239\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1136,  E(|Y-Yhat|): 0.2257,  E(|Yhat-Yhat'|): 0.2243\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1144,  E(|Y-Yhat|): 0.2272,  E(|Yhat-Yhat'|): 0.2255\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4152,  E(|Y-Yhat|): 0.8416,  E(|Yhat-Yhat'|): 0.8528\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3972,  E(|Y-Yhat|): 0.6441,  E(|Yhat-Yhat'|): 0.4938\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3190,  E(|Y-Yhat|): 0.6210,  E(|Yhat-Yhat'|): 0.6039\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3235,  E(|Y-Yhat|): 0.6327,  E(|Yhat-Yhat'|): 0.6185\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3211,  E(|Y-Yhat|): 0.6183,  E(|Yhat-Yhat'|): 0.5945\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0001,  E(|Y-Yhat|): 0.0001,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9698,  E(|Y-Yhat|): 1.1701,  E(|Yhat-Yhat'|): 0.4006\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8288,  E(|Y-Yhat|): 23.2077,  E(|Yhat-Yhat'|): 44.7576\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6321,  E(|Y-Yhat|): 7.6845,  E(|Yhat-Yhat'|): 14.1048\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3155,  E(|Y-Yhat|): 33.7018,  E(|Yhat-Yhat'|): 66.7727\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0001,  E(|Y-Yhat|): 0.0033,  E(|Yhat-Yhat'|): 0.0065\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1275,  E(|Y-Yhat|): 0.2355,  E(|Yhat-Yhat'|): 0.2159\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1010,  E(|Y-Yhat|): 0.1997,  E(|Yhat-Yhat'|): 0.1974\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0979,  E(|Y-Yhat|): 0.2003,  E(|Yhat-Yhat'|): 0.2049\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0964,  E(|Y-Yhat|): 0.1949,  E(|Yhat-Yhat'|): 0.1969\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0001,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2415,  E(|Y-Yhat|): 0.4126,  E(|Yhat-Yhat'|): 0.3422\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2032,  E(|Y-Yhat|): 0.4066,  E(|Yhat-Yhat'|): 0.4069\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2034,  E(|Y-Yhat|): 0.4000,  E(|Yhat-Yhat'|): 0.3932\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2048,  E(|Y-Yhat|): 0.4037,  E(|Yhat-Yhat'|): 0.3978\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0001,  E(|Y-Yhat|): 0.0001,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1392,  E(|Y-Yhat|): 0.2525,  E(|Yhat-Yhat'|): 0.2268\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1119,  E(|Y-Yhat|): 0.2192,  E(|Yhat-Yhat'|): 0.2145\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1156,  E(|Y-Yhat|): 0.2297,  E(|Yhat-Yhat'|): 0.2282\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1103,  E(|Y-Yhat|): 0.2225,  E(|Yhat-Yhat'|): 0.2244\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0001,  E(|Y-Yhat|): 0.0001,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4058,  E(|Y-Yhat|): 0.6805,  E(|Yhat-Yhat'|): 0.5494\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3202,  E(|Y-Yhat|): 0.6195,  E(|Yhat-Yhat'|): 0.5986\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3159,  E(|Y-Yhat|): 0.6224,  E(|Yhat-Yhat'|): 0.6129\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3241,  E(|Y-Yhat|): 0.6389,  E(|Yhat-Yhat'|): 0.6296\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0063,  E(|Y-Yhat|): 0.0112,  E(|Yhat-Yhat'|): 0.0098\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9531,  E(|Y-Yhat|): 1.1696,  E(|Yhat-Yhat'|): 0.4331\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5992,  E(|Y-Yhat|): 9.4612,  E(|Yhat-Yhat'|): 17.7240\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7229,  E(|Y-Yhat|): 3.5779,  E(|Yhat-Yhat'|): 5.7101\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6997,  E(|Y-Yhat|): 2.3034,  E(|Yhat-Yhat'|): 3.2074\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0066,  E(|Y-Yhat|): 0.0221,  E(|Yhat-Yhat'|): 0.0310\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1221,  E(|Y-Yhat|): 0.2222,  E(|Yhat-Yhat'|): 0.2003\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0969,  E(|Y-Yhat|): 0.1951,  E(|Yhat-Yhat'|): 0.1966\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0986,  E(|Y-Yhat|): 0.1942,  E(|Yhat-Yhat'|): 0.1913\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0980,  E(|Y-Yhat|): 0.1935,  E(|Yhat-Yhat'|): 0.1910\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0030,  E(|Y-Yhat|): 0.0048,  E(|Yhat-Yhat'|): 0.0037\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2586,  E(|Y-Yhat|): 0.4302,  E(|Yhat-Yhat'|): 0.3433\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1996,  E(|Y-Yhat|): 0.3895,  E(|Yhat-Yhat'|): 0.3799\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1956,  E(|Y-Yhat|): 0.3810,  E(|Yhat-Yhat'|): 0.3708\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2044,  E(|Y-Yhat|): 0.3875,  E(|Yhat-Yhat'|): 0.3664\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0058,  E(|Y-Yhat|): 0.0103,  E(|Yhat-Yhat'|): 0.0090\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1362,  E(|Y-Yhat|): 0.2576,  E(|Yhat-Yhat'|): 0.2428\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1119,  E(|Y-Yhat|): 0.2233,  E(|Yhat-Yhat'|): 0.2228\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1130,  E(|Y-Yhat|): 0.2252,  E(|Yhat-Yhat'|): 0.2244\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1125,  E(|Y-Yhat|): 0.2207,  E(|Yhat-Yhat'|): 0.2164\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0054,  E(|Y-Yhat|): 0.0092,  E(|Yhat-Yhat'|): 0.0077\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4059,  E(|Y-Yhat|): 0.6990,  E(|Yhat-Yhat'|): 0.5863\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3196,  E(|Y-Yhat|): 0.6151,  E(|Yhat-Yhat'|): 0.5909\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3112,  E(|Y-Yhat|): 0.6149,  E(|Yhat-Yhat'|): 0.6073\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3143,  E(|Y-Yhat|): 0.6218,  E(|Yhat-Yhat'|): 0.6150\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9157,  E(|Y-Yhat|): 1.1787,  E(|Yhat-Yhat'|): 0.5260\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7553,  E(|Y-Yhat|): 47.5009,  E(|Yhat-Yhat'|): 93.4913\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0291,  E(|Y-Yhat|): 37.1799,  E(|Yhat-Yhat'|): 72.3017\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2643,  E(|Y-Yhat|): 92.2810,  E(|Yhat-Yhat'|): 182.0333\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0001\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1278,  E(|Y-Yhat|): 0.2290,  E(|Yhat-Yhat'|): 0.2023\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0967,  E(|Y-Yhat|): 0.1897,  E(|Yhat-Yhat'|): 0.1859\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0961,  E(|Y-Yhat|): 0.1925,  E(|Yhat-Yhat'|): 0.1928\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0987,  E(|Y-Yhat|): 0.1955,  E(|Yhat-Yhat'|): 0.1937\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2502,  E(|Y-Yhat|): 0.4070,  E(|Yhat-Yhat'|): 0.3136\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2067,  E(|Y-Yhat|): 0.3992,  E(|Yhat-Yhat'|): 0.3850\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1999,  E(|Y-Yhat|): 0.3959,  E(|Yhat-Yhat'|): 0.3920\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1977,  E(|Y-Yhat|): 0.3943,  E(|Yhat-Yhat'|): 0.3933\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1350,  E(|Y-Yhat|): 0.2471,  E(|Yhat-Yhat'|): 0.2241\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1117,  E(|Y-Yhat|): 0.2245,  E(|Yhat-Yhat'|): 0.2257\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1115,  E(|Y-Yhat|): 0.2182,  E(|Yhat-Yhat'|): 0.2133\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1106,  E(|Y-Yhat|): 0.2258,  E(|Yhat-Yhat'|): 0.2303\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=1, dy=1, k=1, true_function =\"cubic\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(2, -2, 100)\n",
    "    y_eval = M0* (A0 * x_eval)**3 / 3.0\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41605647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0682),\n",
       " tensor(28.4558),\n",
       " tensor(0.0986),\n",
       " tensor(0.1208),\n",
       " tensor(0.0536))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d6433",
   "metadata": {},
   "source": [
    "## True funcction: square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3650e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4489,  E(|Y-Yhat|): 0.7774,  E(|Yhat-Yhat'|): 0.6570\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2886,  E(|Y-Yhat|): 0.5680,  E(|Yhat-Yhat'|): 0.5589\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2880,  E(|Y-Yhat|): 0.5767,  E(|Yhat-Yhat'|): 0.5773\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2904,  E(|Y-Yhat|): 0.5743,  E(|Yhat-Yhat'|): 0.5677\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4690,  E(|Y-Yhat|): 0.8881,  E(|Yhat-Yhat'|): 0.8382\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8707,  E(|Y-Yhat|): 1.0767,  E(|Yhat-Yhat'|): 0.4120\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8045,  E(|Y-Yhat|): 5.6044,  E(|Yhat-Yhat'|): 9.5998\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6446,  E(|Y-Yhat|): 22.8637,  E(|Yhat-Yhat'|): 44.4381\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6609,  E(|Y-Yhat|): 10.0841,  E(|Yhat-Yhat'|): 18.8462\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4639,  E(|Y-Yhat|): 7.0746,  E(|Yhat-Yhat'|): 13.2212\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1238,  E(|Y-Yhat|): 0.2242,  E(|Yhat-Yhat'|): 0.2009\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0902,  E(|Y-Yhat|): 0.1934,  E(|Yhat-Yhat'|): 0.2064\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0926,  E(|Y-Yhat|): 0.1864,  E(|Yhat-Yhat'|): 0.1877\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0938,  E(|Y-Yhat|): 0.1862,  E(|Yhat-Yhat'|): 0.1847\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0789,  E(|Y-Yhat|): 0.1604,  E(|Yhat-Yhat'|): 0.1631\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3189,  E(|Y-Yhat|): 0.5234,  E(|Yhat-Yhat'|): 0.4089\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1957,  E(|Y-Yhat|): 0.3967,  E(|Yhat-Yhat'|): 0.4019\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1875,  E(|Y-Yhat|): 0.3903,  E(|Yhat-Yhat'|): 0.4057\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1937,  E(|Y-Yhat|): 0.3923,  E(|Yhat-Yhat'|): 0.3971\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1596,  E(|Y-Yhat|): 0.3240,  E(|Yhat-Yhat'|): 0.3288\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1972,  E(|Y-Yhat|): 0.3455,  E(|Yhat-Yhat'|): 0.2966\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1011,  E(|Y-Yhat|): 0.2029,  E(|Yhat-Yhat'|): 0.2036\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1027,  E(|Y-Yhat|): 0.2061,  E(|Yhat-Yhat'|): 0.2067\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0995,  E(|Y-Yhat|): 0.2007,  E(|Yhat-Yhat'|): 0.2024\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0885,  E(|Y-Yhat|): 0.1754,  E(|Yhat-Yhat'|): 0.1740\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4229,  E(|Y-Yhat|): 0.7173,  E(|Yhat-Yhat'|): 0.5888\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2753,  E(|Y-Yhat|): 0.5546,  E(|Yhat-Yhat'|): 0.5586\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2818,  E(|Y-Yhat|): 0.5675,  E(|Yhat-Yhat'|): 0.5714\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2805,  E(|Y-Yhat|): 0.5682,  E(|Yhat-Yhat'|): 0.5755\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0131,  E(|Y-Yhat|): 0.0257,  E(|Yhat-Yhat'|): 0.0251\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8798,  E(|Y-Yhat|): 1.1509,  E(|Yhat-Yhat'|): 0.5423\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4432,  E(|Y-Yhat|): 42.3787,  E(|Yhat-Yhat'|): 83.8710\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.3773,  E(|Y-Yhat|): 106.1199,  E(|Yhat-Yhat'|): 212.9944\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9233,  E(|Y-Yhat|): 17.4698,  E(|Yhat-Yhat'|): 33.0931\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0109,  E(|Y-Yhat|): 0.3250,  E(|Yhat-Yhat'|): 0.6282\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1283,  E(|Y-Yhat|): 0.2355,  E(|Yhat-Yhat'|): 0.2144\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0933,  E(|Y-Yhat|): 0.1841,  E(|Yhat-Yhat'|): 0.1816\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0894,  E(|Y-Yhat|): 0.1851,  E(|Yhat-Yhat'|): 0.1915\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0917,  E(|Y-Yhat|): 0.1844,  E(|Yhat-Yhat'|): 0.1855\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0060,  E(|Y-Yhat|): 0.0114,  E(|Yhat-Yhat'|): 0.0107\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2553,  E(|Y-Yhat|): 0.4477,  E(|Yhat-Yhat'|): 0.3847\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1962,  E(|Y-Yhat|): 0.3866,  E(|Yhat-Yhat'|): 0.3807\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1919,  E(|Y-Yhat|): 0.3756,  E(|Yhat-Yhat'|): 0.3674\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1877,  E(|Y-Yhat|): 0.3827,  E(|Yhat-Yhat'|): 0.3900\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0113,  E(|Y-Yhat|): 0.0230,  E(|Yhat-Yhat'|): 0.0234\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1403,  E(|Y-Yhat|): 0.2613,  E(|Yhat-Yhat'|): 0.2419\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1004,  E(|Y-Yhat|): 0.2052,  E(|Yhat-Yhat'|): 0.2096\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1002,  E(|Y-Yhat|): 0.2012,  E(|Yhat-Yhat'|): 0.2019\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0970,  E(|Y-Yhat|): 0.1954,  E(|Yhat-Yhat'|): 0.1966\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0103,  E(|Y-Yhat|): 0.0201,  E(|Yhat-Yhat'|): 0.0195\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4106,  E(|Y-Yhat|): 0.6678,  E(|Yhat-Yhat'|): 0.5144\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2893,  E(|Y-Yhat|): 0.5865,  E(|Yhat-Yhat'|): 0.5943\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2792,  E(|Y-Yhat|): 0.5559,  E(|Yhat-Yhat'|): 0.5533\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2918,  E(|Y-Yhat|): 0.5800,  E(|Yhat-Yhat'|): 0.5763\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0011,  E(|Y-Yhat|): 0.0022,  E(|Yhat-Yhat'|): 0.0022\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9738,  E(|Y-Yhat|): 1.1781,  E(|Yhat-Yhat'|): 0.4085\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6808,  E(|Y-Yhat|): 9.1558,  E(|Yhat-Yhat'|): 16.9499\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7591,  E(|Y-Yhat|): 6.3948,  E(|Yhat-Yhat'|): 11.2715\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7100,  E(|Y-Yhat|): 8.9494,  E(|Yhat-Yhat'|): 16.4789\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0011,  E(|Y-Yhat|): 0.0151,  E(|Yhat-Yhat'|): 0.0281\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1463,  E(|Y-Yhat|): 0.2653,  E(|Yhat-Yhat'|): 0.2380\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0934,  E(|Y-Yhat|): 0.1923,  E(|Yhat-Yhat'|): 0.1978\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0948,  E(|Y-Yhat|): 0.1980,  E(|Yhat-Yhat'|): 0.2064\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0913,  E(|Y-Yhat|): 0.1858,  E(|Yhat-Yhat'|): 0.1891\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0006,  E(|Y-Yhat|): 0.0011,  E(|Yhat-Yhat'|): 0.0011\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2794,  E(|Y-Yhat|): 0.4741,  E(|Yhat-Yhat'|): 0.3894\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1887,  E(|Y-Yhat|): 0.3962,  E(|Yhat-Yhat'|): 0.4151\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1966,  E(|Y-Yhat|): 0.3941,  E(|Yhat-Yhat'|): 0.3951\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1845,  E(|Y-Yhat|): 0.3856,  E(|Yhat-Yhat'|): 0.4022\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0012,  E(|Y-Yhat|): 0.0023,  E(|Yhat-Yhat'|): 0.0022\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1360,  E(|Y-Yhat|): 0.2440,  E(|Yhat-Yhat'|): 0.2159\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0976,  E(|Y-Yhat|): 0.2021,  E(|Yhat-Yhat'|): 0.2090\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1001,  E(|Y-Yhat|): 0.2046,  E(|Yhat-Yhat'|): 0.2090\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1012,  E(|Y-Yhat|): 0.2039,  E(|Yhat-Yhat'|): 0.2055\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0011,  E(|Y-Yhat|): 0.0021,  E(|Yhat-Yhat'|): 0.0019\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4434,  E(|Y-Yhat|): 0.7512,  E(|Yhat-Yhat'|): 0.6156\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2858,  E(|Y-Yhat|): 0.5670,  E(|Yhat-Yhat'|): 0.5625\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2866,  E(|Y-Yhat|): 0.5647,  E(|Yhat-Yhat'|): 0.5562\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2770,  E(|Y-Yhat|): 0.5666,  E(|Yhat-Yhat'|): 0.5792\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0123,  E(|Y-Yhat|): 0.0247,  E(|Yhat-Yhat'|): 0.0248\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8793,  E(|Y-Yhat|): 1.1650,  E(|Yhat-Yhat'|): 0.5714\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5179,  E(|Y-Yhat|): 13.4344,  E(|Yhat-Yhat'|): 25.8332\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6161,  E(|Y-Yhat|): 4.3529,  E(|Yhat-Yhat'|): 7.4734\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6931,  E(|Y-Yhat|): 8.1168,  E(|Yhat-Yhat'|): 14.8473\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0122,  E(|Y-Yhat|): 0.1479,  E(|Yhat-Yhat'|): 0.2714\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1376,  E(|Y-Yhat|): 0.2461,  E(|Yhat-Yhat'|): 0.2170\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0918,  E(|Y-Yhat|): 0.1884,  E(|Yhat-Yhat'|): 0.1933\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0962,  E(|Y-Yhat|): 0.1899,  E(|Yhat-Yhat'|): 0.1875\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0912,  E(|Y-Yhat|): 0.1864,  E(|Yhat-Yhat'|): 0.1905\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0061,  E(|Y-Yhat|): 0.0113,  E(|Yhat-Yhat'|): 0.0104\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2699,  E(|Y-Yhat|): 0.4520,  E(|Yhat-Yhat'|): 0.3641\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1895,  E(|Y-Yhat|): 0.3915,  E(|Yhat-Yhat'|): 0.4040\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1904,  E(|Y-Yhat|): 0.3812,  E(|Yhat-Yhat'|): 0.3816\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1886,  E(|Y-Yhat|): 0.3797,  E(|Yhat-Yhat'|): 0.3822\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0111,  E(|Y-Yhat|): 0.0218,  E(|Yhat-Yhat'|): 0.0213\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1892,  E(|Y-Yhat|): 0.3397,  E(|Yhat-Yhat'|): 0.3011\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1011,  E(|Y-Yhat|): 0.2056,  E(|Yhat-Yhat'|): 0.2091\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1003,  E(|Y-Yhat|): 0.2047,  E(|Yhat-Yhat'|): 0.2088\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1027,  E(|Y-Yhat|): 0.2070,  E(|Yhat-Yhat'|): 0.2085\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0100,  E(|Y-Yhat|): 0.0200,  E(|Yhat-Yhat'|): 0.0199\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4720,  E(|Y-Yhat|): 0.8293,  E(|Yhat-Yhat'|): 0.7147\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2809,  E(|Y-Yhat|): 0.5657,  E(|Yhat-Yhat'|): 0.5697\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2746,  E(|Y-Yhat|): 0.5397,  E(|Yhat-Yhat'|): 0.5303\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2782,  E(|Y-Yhat|): 0.5602,  E(|Yhat-Yhat'|): 0.5640\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3464,  E(|Y-Yhat|): 0.6835,  E(|Yhat-Yhat'|): 0.6741\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8486,  E(|Y-Yhat|): 1.0917,  E(|Yhat-Yhat'|): 0.4862\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5300,  E(|Y-Yhat|): 100.7767,  E(|Yhat-Yhat'|): 200.4934\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0241,  E(|Y-Yhat|): 41.2643,  E(|Yhat-Yhat'|): 82.4804\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5384,  E(|Y-Yhat|): 8.2515,  E(|Yhat-Yhat'|): 15.4262\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3283,  E(|Y-Yhat|): 4.3719,  E(|Yhat-Yhat'|): 8.0871\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1683,  E(|Y-Yhat|): 0.2997,  E(|Yhat-Yhat'|): 0.2627\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0880,  E(|Y-Yhat|): 0.1816,  E(|Yhat-Yhat'|): 0.1873\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0913,  E(|Y-Yhat|): 0.1835,  E(|Yhat-Yhat'|): 0.1845\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0882,  E(|Y-Yhat|): 0.1794,  E(|Yhat-Yhat'|): 0.1823\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0668,  E(|Y-Yhat|): 0.1323,  E(|Yhat-Yhat'|): 0.1310\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2638,  E(|Y-Yhat|): 0.4292,  E(|Yhat-Yhat'|): 0.3309\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1897,  E(|Y-Yhat|): 0.3829,  E(|Yhat-Yhat'|): 0.3864\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1858,  E(|Y-Yhat|): 0.3800,  E(|Yhat-Yhat'|): 0.3883\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1916,  E(|Y-Yhat|): 0.3911,  E(|Yhat-Yhat'|): 0.3989\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1319,  E(|Y-Yhat|): 0.2647,  E(|Yhat-Yhat'|): 0.2656\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1411,  E(|Y-Yhat|): 0.2506,  E(|Yhat-Yhat'|): 0.2190\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0973,  E(|Y-Yhat|): 0.1962,  E(|Yhat-Yhat'|): 0.1980\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0976,  E(|Y-Yhat|): 0.1940,  E(|Yhat-Yhat'|): 0.1926\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0970,  E(|Y-Yhat|): 0.1954,  E(|Yhat-Yhat'|): 0.1968\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0754,  E(|Y-Yhat|): 0.1526,  E(|Yhat-Yhat'|): 0.1543\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4146,  E(|Y-Yhat|): 0.6723,  E(|Yhat-Yhat'|): 0.5155\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2878,  E(|Y-Yhat|): 0.5757,  E(|Yhat-Yhat'|): 0.5759\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2888,  E(|Y-Yhat|): 0.5774,  E(|Yhat-Yhat'|): 0.5772\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2897,  E(|Y-Yhat|): 0.5774,  E(|Yhat-Yhat'|): 0.5753\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0209,  E(|Y-Yhat|): 0.0403,  E(|Yhat-Yhat'|): 0.0388\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9322,  E(|Y-Yhat|): 1.1042,  E(|Yhat-Yhat'|): 0.3440\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5105,  E(|Y-Yhat|): 6.1157,  E(|Yhat-Yhat'|): 11.2104\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9304,  E(|Y-Yhat|): 42.1715,  E(|Yhat-Yhat'|): 82.4822\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0369,  E(|Y-Yhat|): 66.8748,  E(|Yhat-Yhat'|): 131.6757\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0468,  E(|Y-Yhat|): 2.0847,  E(|Yhat-Yhat'|): 4.0758\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1611,  E(|Y-Yhat|): 0.2760,  E(|Yhat-Yhat'|): 0.2299\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0922,  E(|Y-Yhat|): 0.1890,  E(|Yhat-Yhat'|): 0.1936\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0910,  E(|Y-Yhat|): 0.1919,  E(|Yhat-Yhat'|): 0.2019\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0922,  E(|Y-Yhat|): 0.1889,  E(|Yhat-Yhat'|): 0.1935\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0090,  E(|Y-Yhat|): 0.0177,  E(|Yhat-Yhat'|): 0.0174\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3111,  E(|Y-Yhat|): 0.5158,  E(|Yhat-Yhat'|): 0.4094\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1906,  E(|Y-Yhat|): 0.4029,  E(|Yhat-Yhat'|): 0.4245\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1976,  E(|Y-Yhat|): 0.3970,  E(|Yhat-Yhat'|): 0.3988\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1933,  E(|Y-Yhat|): 0.3924,  E(|Yhat-Yhat'|): 0.3984\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0171,  E(|Y-Yhat|): 0.0334,  E(|Yhat-Yhat'|): 0.0328\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1595,  E(|Y-Yhat|): 0.2895,  E(|Yhat-Yhat'|): 0.2599\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1049,  E(|Y-Yhat|): 0.2080,  E(|Yhat-Yhat'|): 0.2061\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1016,  E(|Y-Yhat|): 0.2086,  E(|Yhat-Yhat'|): 0.2139\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1037,  E(|Y-Yhat|): 0.2059,  E(|Yhat-Yhat'|): 0.2043\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0144,  E(|Y-Yhat|): 0.0272,  E(|Yhat-Yhat'|): 0.0257\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4166,  E(|Y-Yhat|): 0.7013,  E(|Yhat-Yhat'|): 0.5694\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2752,  E(|Y-Yhat|): 0.5631,  E(|Yhat-Yhat'|): 0.5758\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2805,  E(|Y-Yhat|): 0.5651,  E(|Yhat-Yhat'|): 0.5692\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2787,  E(|Y-Yhat|): 0.5544,  E(|Yhat-Yhat'|): 0.5513\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 11.6779,  E(|Y-Yhat|): 21.8900,  E(|Yhat-Yhat'|): 20.4242\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8872,  E(|Y-Yhat|): 1.0765,  E(|Yhat-Yhat'|): 0.3786\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6359,  E(|Y-Yhat|): 3.5251,  E(|Yhat-Yhat'|): 5.7784\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7791,  E(|Y-Yhat|): 4.0379,  E(|Yhat-Yhat'|): 6.5177\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7875,  E(|Y-Yhat|): 10.5871,  E(|Yhat-Yhat'|): 19.5993\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 9.0473,  E(|Y-Yhat|): 173.9396,  E(|Yhat-Yhat'|): 329.7846\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1257,  E(|Y-Yhat|): 0.2248,  E(|Yhat-Yhat'|): 0.1983\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0922,  E(|Y-Yhat|): 0.1876,  E(|Yhat-Yhat'|): 0.1907\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0888,  E(|Y-Yhat|): 0.1796,  E(|Yhat-Yhat'|): 0.1817\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0893,  E(|Y-Yhat|): 0.1816,  E(|Yhat-Yhat'|): 0.1846\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2008,  E(|Y-Yhat|): 0.4380,  E(|Yhat-Yhat'|): 0.4745\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2734,  E(|Y-Yhat|): 0.4202,  E(|Yhat-Yhat'|): 0.2936\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1880,  E(|Y-Yhat|): 0.3850,  E(|Yhat-Yhat'|): 0.3940\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1846,  E(|Y-Yhat|): 0.3729,  E(|Yhat-Yhat'|): 0.3765\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1927,  E(|Y-Yhat|): 0.3863,  E(|Yhat-Yhat'|): 0.3873\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6154,  E(|Y-Yhat|): 1.2681,  E(|Yhat-Yhat'|): 1.3053\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1459,  E(|Y-Yhat|): 0.2531,  E(|Yhat-Yhat'|): 0.2142\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0981,  E(|Y-Yhat|): 0.1994,  E(|Yhat-Yhat'|): 0.2027\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0982,  E(|Y-Yhat|): 0.2011,  E(|Yhat-Yhat'|): 0.2058\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0985,  E(|Y-Yhat|): 0.1990,  E(|Yhat-Yhat'|): 0.2011\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1973,  E(|Y-Yhat|): 0.4001,  E(|Yhat-Yhat'|): 0.4057\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4399,  E(|Y-Yhat|): 0.7133,  E(|Yhat-Yhat'|): 0.5469\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2836,  E(|Y-Yhat|): 0.5639,  E(|Yhat-Yhat'|): 0.5606\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2934,  E(|Y-Yhat|): 0.5829,  E(|Yhat-Yhat'|): 0.5789\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2913,  E(|Y-Yhat|): 0.5749,  E(|Yhat-Yhat'|): 0.5673\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0003,  E(|Y-Yhat|): 0.0005,  E(|Yhat-Yhat'|): 0.0005\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9640,  E(|Y-Yhat|): 1.1704,  E(|Yhat-Yhat'|): 0.4127\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6387,  E(|Y-Yhat|): 3.0520,  E(|Yhat-Yhat'|): 4.8267\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6358,  E(|Y-Yhat|): 4.3993,  E(|Yhat-Yhat'|): 7.5270\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7130,  E(|Y-Yhat|): 2.6700,  E(|Yhat-Yhat'|): 3.9139\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0003,  E(|Y-Yhat|): 0.0010,  E(|Yhat-Yhat'|): 0.0016\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1603,  E(|Y-Yhat|): 0.2779,  E(|Yhat-Yhat'|): 0.2352\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0959,  E(|Y-Yhat|): 0.1955,  E(|Yhat-Yhat'|): 0.1992\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0969,  E(|Y-Yhat|): 0.1982,  E(|Yhat-Yhat'|): 0.2026\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0938,  E(|Y-Yhat|): 0.1897,  E(|Yhat-Yhat'|): 0.1918\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0001,  E(|Y-Yhat|): 0.0003,  E(|Yhat-Yhat'|): 0.0002\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2811,  E(|Y-Yhat|): 0.4618,  E(|Yhat-Yhat'|): 0.3614\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1995,  E(|Y-Yhat|): 0.3931,  E(|Yhat-Yhat'|): 0.3873\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.4051,  E(|Yhat-Yhat'|): 0.4184\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1998,  E(|Y-Yhat|): 0.3946,  E(|Yhat-Yhat'|): 0.3896\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0003,  E(|Y-Yhat|): 0.0005,  E(|Yhat-Yhat'|): 0.0005\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1538,  E(|Y-Yhat|): 0.2730,  E(|Yhat-Yhat'|): 0.2384\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1037,  E(|Y-Yhat|): 0.2050,  E(|Yhat-Yhat'|): 0.2026\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1044,  E(|Y-Yhat|): 0.2124,  E(|Yhat-Yhat'|): 0.2161\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1044,  E(|Y-Yhat|): 0.2111,  E(|Yhat-Yhat'|): 0.2133\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0003,  E(|Y-Yhat|): 0.0005,  E(|Yhat-Yhat'|): 0.0005\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4840,  E(|Y-Yhat|): 0.8103,  E(|Yhat-Yhat'|): 0.6526\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2895,  E(|Y-Yhat|): 0.5800,  E(|Yhat-Yhat'|): 0.5809\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2892,  E(|Y-Yhat|): 0.5851,  E(|Yhat-Yhat'|): 0.5918\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2897,  E(|Y-Yhat|): 0.5736,  E(|Yhat-Yhat'|): 0.5679\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0074,  E(|Y-Yhat|): 0.0141,  E(|Yhat-Yhat'|): 0.0133\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9605,  E(|Y-Yhat|): 1.1516,  E(|Yhat-Yhat'|): 0.3822\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8849,  E(|Y-Yhat|): 42.8713,  E(|Yhat-Yhat'|): 83.9728\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6943,  E(|Y-Yhat|): 23.8801,  E(|Yhat-Yhat'|): 46.3715\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -6.2544,  E(|Y-Yhat|): 289.5150,  E(|Yhat-Yhat'|): 591.5389\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0020,  E(|Y-Yhat|): 3.6991,  E(|Yhat-Yhat'|): 7.4021\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1477,  E(|Y-Yhat|): 0.2554,  E(|Yhat-Yhat'|): 0.2154\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0916,  E(|Y-Yhat|): 0.1941,  E(|Yhat-Yhat'|): 0.2050\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0937,  E(|Y-Yhat|): 0.1897,  E(|Yhat-Yhat'|): 0.1919\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0933,  E(|Y-Yhat|): 0.1883,  E(|Yhat-Yhat'|): 0.1901\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0037,  E(|Y-Yhat|): 0.0069,  E(|Yhat-Yhat'|): 0.0064\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2649,  E(|Y-Yhat|): 0.4525,  E(|Yhat-Yhat'|): 0.3750\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1899,  E(|Y-Yhat|): 0.3905,  E(|Yhat-Yhat'|): 0.4012\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1987,  E(|Y-Yhat|): 0.4014,  E(|Yhat-Yhat'|): 0.4055\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1977,  E(|Y-Yhat|): 0.3875,  E(|Yhat-Yhat'|): 0.3798\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0068,  E(|Y-Yhat|): 0.0130,  E(|Yhat-Yhat'|): 0.0123\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1384,  E(|Y-Yhat|): 0.2520,  E(|Yhat-Yhat'|): 0.2272\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1018,  E(|Y-Yhat|): 0.2080,  E(|Yhat-Yhat'|): 0.2123\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1011,  E(|Y-Yhat|): 0.2041,  E(|Yhat-Yhat'|): 0.2058\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1028,  E(|Y-Yhat|): 0.2058,  E(|Yhat-Yhat'|): 0.2059\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0065,  E(|Y-Yhat|): 0.0128,  E(|Yhat-Yhat'|): 0.0125\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4601,  E(|Y-Yhat|): 0.7911,  E(|Yhat-Yhat'|): 0.6622\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2887,  E(|Y-Yhat|): 0.5584,  E(|Yhat-Yhat'|): 0.5395\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2689,  E(|Y-Yhat|): 0.5443,  E(|Yhat-Yhat'|): 0.5507\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2810,  E(|Y-Yhat|): 0.5668,  E(|Yhat-Yhat'|): 0.5716\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9036,  E(|Y-Yhat|): 1.1667,  E(|Yhat-Yhat'|): 0.5262\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9367,  E(|Y-Yhat|): 158.8552,  E(|Yhat-Yhat'|): 313.8369\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8064,  E(|Y-Yhat|): 72.2768,  E(|Yhat-Yhat'|): 142.9407\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.8229,  E(|Y-Yhat|): 231.2262,  E(|Yhat-Yhat'|): 456.8066\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0000,  E(|Y-Yhat|): 0.0046,  E(|Yhat-Yhat'|): 0.0093\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1530,  E(|Y-Yhat|): 0.2680,  E(|Yhat-Yhat'|): 0.2301\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0908,  E(|Y-Yhat|): 0.1795,  E(|Yhat-Yhat'|): 0.1773\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0905,  E(|Y-Yhat|): 0.1823,  E(|Yhat-Yhat'|): 0.1837\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0932,  E(|Y-Yhat|): 0.1887,  E(|Yhat-Yhat'|): 0.1911\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2902,  E(|Y-Yhat|): 0.4664,  E(|Yhat-Yhat'|): 0.3523\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1882,  E(|Y-Yhat|): 0.3893,  E(|Yhat-Yhat'|): 0.4022\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1908,  E(|Y-Yhat|): 0.3889,  E(|Yhat-Yhat'|): 0.3962\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1862,  E(|Y-Yhat|): 0.3848,  E(|Yhat-Yhat'|): 0.3973\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1621,  E(|Y-Yhat|): 0.2932,  E(|Yhat-Yhat'|): 0.2622\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0992,  E(|Y-Yhat|): 0.2008,  E(|Yhat-Yhat'|): 0.2032\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0960,  E(|Y-Yhat|): 0.2004,  E(|Yhat-Yhat'|): 0.2089\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1009,  E(|Y-Yhat|): 0.2015,  E(|Yhat-Yhat'|): 0.2011\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0000,  E(|Y-Yhat|): 0.0000,  E(|Yhat-Yhat'|): 0.0000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=1, dy=1, k=1, true_function =\"square\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(2, -2, 100)\n",
    "    y_eval = M0* (F.relu(A0 * x_eval))**2 / 2.0\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c487e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0017),\n",
       " tensor(0.2515),\n",
       " tensor(0.0057),\n",
       " tensor(0.0022),\n",
       " tensor(0.0027))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316b6fd",
   "metadata": {},
   "source": [
    "## True function: log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88969188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lin(z):\n",
    "    return torch.where(\n",
    "            z <= 2.0, z/3.0 + torch.log(torch.tensor(3.0, device=z.device)) - 2.0/3.0,\n",
    "            torch.log(1.0 + z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c4fe9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5384,  E(|Y-Yhat|): 1.0097,  E(|Yhat-Yhat'|): 0.9426\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3697,  E(|Y-Yhat|): 0.7463,  E(|Yhat-Yhat'|): 0.7533\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3716,  E(|Y-Yhat|): 0.7563,  E(|Yhat-Yhat'|): 0.7695\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3720,  E(|Y-Yhat|): 0.7569,  E(|Yhat-Yhat'|): 0.7698\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0212,  E(|Y-Yhat|): 0.0423,  E(|Yhat-Yhat'|): 0.0422\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7642,  E(|Y-Yhat|): 0.9658,  E(|Yhat-Yhat'|): 0.4031\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8505,  E(|Y-Yhat|): 36.9999,  E(|Yhat-Yhat'|): 72.2987\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.6133,  E(|Y-Yhat|): 39.5558,  E(|Yhat-Yhat'|): 80.3382\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3735,  E(|Y-Yhat|): 18.8305,  E(|Yhat-Yhat'|): 36.9141\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0071,  E(|Y-Yhat|): 0.9095,  E(|Yhat-Yhat'|): 1.8047\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2226,  E(|Y-Yhat|): 0.3889,  E(|Yhat-Yhat'|): 0.3327\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1349,  E(|Y-Yhat|): 0.2749,  E(|Yhat-Yhat'|): 0.2799\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1324,  E(|Y-Yhat|): 0.2708,  E(|Yhat-Yhat'|): 0.2767\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1318,  E(|Y-Yhat|): 0.2633,  E(|Yhat-Yhat'|): 0.2628\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0103,  E(|Y-Yhat|): 0.0205,  E(|Yhat-Yhat'|): 0.0204\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3963,  E(|Y-Yhat|): 0.6758,  E(|Yhat-Yhat'|): 0.5590\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2432,  E(|Y-Yhat|): 0.4857,  E(|Yhat-Yhat'|): 0.4851\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2340,  E(|Y-Yhat|): 0.4798,  E(|Yhat-Yhat'|): 0.4916\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2336,  E(|Y-Yhat|): 0.4791,  E(|Yhat-Yhat'|): 0.4910\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0193,  E(|Y-Yhat|): 0.0401,  E(|Yhat-Yhat'|): 0.0416\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2382,  E(|Y-Yhat|): 0.4157,  E(|Yhat-Yhat'|): 0.3549\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1617,  E(|Y-Yhat|): 0.3260,  E(|Yhat-Yhat'|): 0.3286\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1595,  E(|Y-Yhat|): 0.3220,  E(|Yhat-Yhat'|): 0.3250\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1606,  E(|Y-Yhat|): 0.3232,  E(|Yhat-Yhat'|): 0.3251\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0190,  E(|Y-Yhat|): 0.0381,  E(|Yhat-Yhat'|): 0.0382\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5178,  E(|Y-Yhat|): 0.9708,  E(|Yhat-Yhat'|): 0.9061\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3728,  E(|Y-Yhat|): 0.7551,  E(|Yhat-Yhat'|): 0.7646\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3744,  E(|Y-Yhat|): 0.7542,  E(|Yhat-Yhat'|): 0.7595\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3687,  E(|Y-Yhat|): 0.7395,  E(|Yhat-Yhat'|): 0.7416\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0034,  E(|Y-Yhat|): 0.0071,  E(|Yhat-Yhat'|): 0.0073\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7733,  E(|Y-Yhat|): 1.0396,  E(|Yhat-Yhat'|): 0.5326\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5647,  E(|Y-Yhat|): 7.8566,  E(|Yhat-Yhat'|): 14.5838\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3589,  E(|Y-Yhat|): 29.7130,  E(|Yhat-Yhat'|): 58.7082\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2518,  E(|Y-Yhat|): 31.3372,  E(|Yhat-Yhat'|): 60.1708\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0044,  E(|Y-Yhat|): 0.2141,  E(|Yhat-Yhat'|): 0.4196\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2026,  E(|Y-Yhat|): 0.3828,  E(|Yhat-Yhat'|): 0.3604\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1356,  E(|Y-Yhat|): 0.2708,  E(|Yhat-Yhat'|): 0.2703\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1346,  E(|Y-Yhat|): 0.2728,  E(|Yhat-Yhat'|): 0.2764\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1345,  E(|Y-Yhat|): 0.2711,  E(|Yhat-Yhat'|): 0.2732\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0017,  E(|Y-Yhat|): 0.0035,  E(|Yhat-Yhat'|): 0.0035\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3807,  E(|Y-Yhat|): 0.6882,  E(|Yhat-Yhat'|): 0.6148\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2418,  E(|Y-Yhat|): 0.4926,  E(|Yhat-Yhat'|): 0.5016\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2384,  E(|Y-Yhat|): 0.4919,  E(|Yhat-Yhat'|): 0.5069\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2382,  E(|Y-Yhat|): 0.4727,  E(|Yhat-Yhat'|): 0.4690\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0034,  E(|Y-Yhat|): 0.0070,  E(|Yhat-Yhat'|): 0.0072\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2232,  E(|Y-Yhat|): 0.4254,  E(|Yhat-Yhat'|): 0.4044\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1615,  E(|Y-Yhat|): 0.3313,  E(|Yhat-Yhat'|): 0.3395\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1631,  E(|Y-Yhat|): 0.3256,  E(|Yhat-Yhat'|): 0.3252\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1609,  E(|Y-Yhat|): 0.3260,  E(|Yhat-Yhat'|): 0.3301\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0035,  E(|Y-Yhat|): 0.0072,  E(|Yhat-Yhat'|): 0.0075\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5635,  E(|Y-Yhat|): 0.9524,  E(|Yhat-Yhat'|): 0.7777\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3714,  E(|Y-Yhat|): 0.7526,  E(|Yhat-Yhat'|): 0.7625\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3677,  E(|Y-Yhat|): 0.7415,  E(|Yhat-Yhat'|): 0.7476\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3742,  E(|Y-Yhat|): 0.7435,  E(|Yhat-Yhat'|): 0.7386\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0009,  E(|Y-Yhat|): 0.0018,  E(|Yhat-Yhat'|): 0.0018\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9165,  E(|Y-Yhat|): 1.1151,  E(|Yhat-Yhat'|): 0.3972\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4644,  E(|Y-Yhat|): 6.9041,  E(|Yhat-Yhat'|): 12.8793\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4858,  E(|Y-Yhat|): 6.4698,  E(|Yhat-Yhat'|): 11.9679\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4756,  E(|Y-Yhat|): 9.1400,  E(|Yhat-Yhat'|): 17.3289\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0009,  E(|Y-Yhat|): 0.0178,  E(|Yhat-Yhat'|): 0.0339\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2071,  E(|Y-Yhat|): 0.3706,  E(|Yhat-Yhat'|): 0.3271\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1324,  E(|Y-Yhat|): 0.2761,  E(|Yhat-Yhat'|): 0.2873\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1332,  E(|Y-Yhat|): 0.2736,  E(|Yhat-Yhat'|): 0.2807\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1347,  E(|Y-Yhat|): 0.2730,  E(|Yhat-Yhat'|): 0.2765\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0004,  E(|Y-Yhat|): 0.0009,  E(|Yhat-Yhat'|): 0.0009\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3669,  E(|Y-Yhat|): 0.6854,  E(|Yhat-Yhat'|): 0.6371\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2466,  E(|Y-Yhat|): 0.4909,  E(|Yhat-Yhat'|): 0.4885\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2398,  E(|Y-Yhat|): 0.4839,  E(|Yhat-Yhat'|): 0.4883\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2392,  E(|Y-Yhat|): 0.4919,  E(|Yhat-Yhat'|): 0.5053\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0009,  E(|Y-Yhat|): 0.0019,  E(|Yhat-Yhat'|): 0.0019\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2245,  E(|Y-Yhat|): 0.4186,  E(|Yhat-Yhat'|): 0.3883\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1620,  E(|Y-Yhat|): 0.3242,  E(|Yhat-Yhat'|): 0.3244\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1585,  E(|Y-Yhat|): 0.3210,  E(|Yhat-Yhat'|): 0.3250\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1624,  E(|Y-Yhat|): 0.3206,  E(|Yhat-Yhat'|): 0.3163\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0009,  E(|Y-Yhat|): 0.0018,  E(|Yhat-Yhat'|): 0.0018\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5372,  E(|Y-Yhat|): 0.9721,  E(|Yhat-Yhat'|): 0.8698\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3757,  E(|Y-Yhat|): 0.7534,  E(|Yhat-Yhat'|): 0.7553\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3806,  E(|Y-Yhat|): 0.7686,  E(|Yhat-Yhat'|): 0.7760\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3732,  E(|Y-Yhat|): 0.7517,  E(|Yhat-Yhat'|): 0.7571\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0021,  E(|Y-Yhat|): 0.0044,  E(|Yhat-Yhat'|): 0.0044\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8221,  E(|Y-Yhat|): 1.0941,  E(|Yhat-Yhat'|): 0.5440\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3421,  E(|Y-Yhat|): 3.5086,  E(|Yhat-Yhat'|): 6.3329\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5234,  E(|Y-Yhat|): 11.2677,  E(|Yhat-Yhat'|): 21.4885\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4741,  E(|Y-Yhat|): 9.9559,  E(|Yhat-Yhat'|): 18.9635\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0024,  E(|Y-Yhat|): 0.0475,  E(|Yhat-Yhat'|): 0.0902\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2053,  E(|Y-Yhat|): 0.3752,  E(|Yhat-Yhat'|): 0.3398\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1350,  E(|Y-Yhat|): 0.2687,  E(|Yhat-Yhat'|): 0.2674\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1331,  E(|Y-Yhat|): 0.2756,  E(|Yhat-Yhat'|): 0.2850\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1336,  E(|Y-Yhat|): 0.2691,  E(|Yhat-Yhat'|): 0.2709\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0011,  E(|Y-Yhat|): 0.0022,  E(|Yhat-Yhat'|): 0.0022\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3825,  E(|Y-Yhat|): 0.6883,  E(|Yhat-Yhat'|): 0.6117\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2410,  E(|Y-Yhat|): 0.4925,  E(|Yhat-Yhat'|): 0.5031\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2373,  E(|Y-Yhat|): 0.4887,  E(|Yhat-Yhat'|): 0.5028\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2355,  E(|Y-Yhat|): 0.4817,  E(|Yhat-Yhat'|): 0.4923\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0022,  E(|Y-Yhat|): 0.0044,  E(|Yhat-Yhat'|): 0.0044\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2401,  E(|Y-Yhat|): 0.4287,  E(|Yhat-Yhat'|): 0.3773\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1646,  E(|Y-Yhat|): 0.3269,  E(|Yhat-Yhat'|): 0.3245\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1616,  E(|Y-Yhat|): 0.3240,  E(|Yhat-Yhat'|): 0.3248\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1601,  E(|Y-Yhat|): 0.3283,  E(|Yhat-Yhat'|): 0.3365\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0021,  E(|Y-Yhat|): 0.0044,  E(|Yhat-Yhat'|): 0.0046\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5390,  E(|Y-Yhat|): 0.9897,  E(|Yhat-Yhat'|): 0.9015\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3678,  E(|Y-Yhat|): 0.7437,  E(|Yhat-Yhat'|): 0.7518\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3663,  E(|Y-Yhat|): 0.7475,  E(|Yhat-Yhat'|): 0.7622\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3660,  E(|Y-Yhat|): 0.7364,  E(|Yhat-Yhat'|): 0.7407\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0132,  E(|Y-Yhat|): 0.0270,  E(|Yhat-Yhat'|): 0.0276\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7027,  E(|Y-Yhat|): 1.0117,  E(|Yhat-Yhat'|): 0.6181\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3568,  E(|Y-Yhat|): 6.0686,  E(|Yhat-Yhat'|): 11.4236\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5133,  E(|Y-Yhat|): 12.6227,  E(|Yhat-Yhat'|): 24.2189\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7925,  E(|Y-Yhat|): 13.4151,  E(|Yhat-Yhat'|): 25.2452\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0119,  E(|Y-Yhat|): 0.3964,  E(|Yhat-Yhat'|): 0.7691\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2191,  E(|Y-Yhat|): 0.3859,  E(|Yhat-Yhat'|): 0.3337\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1318,  E(|Y-Yhat|): 0.2674,  E(|Yhat-Yhat'|): 0.2712\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1300,  E(|Y-Yhat|): 0.2655,  E(|Yhat-Yhat'|): 0.2710\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1316,  E(|Y-Yhat|): 0.2640,  E(|Yhat-Yhat'|): 0.2649\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0065,  E(|Y-Yhat|): 0.0134,  E(|Yhat-Yhat'|): 0.0137\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3956,  E(|Y-Yhat|): 0.6957,  E(|Yhat-Yhat'|): 0.6003\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2390,  E(|Y-Yhat|): 0.4927,  E(|Yhat-Yhat'|): 0.5074\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2419,  E(|Y-Yhat|): 0.4898,  E(|Yhat-Yhat'|): 0.4957\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2405,  E(|Y-Yhat|): 0.4751,  E(|Yhat-Yhat'|): 0.4692\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0132,  E(|Y-Yhat|): 0.0259,  E(|Yhat-Yhat'|): 0.0253\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2313,  E(|Y-Yhat|): 0.4261,  E(|Yhat-Yhat'|): 0.3898\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1580,  E(|Y-Yhat|): 0.3190,  E(|Yhat-Yhat'|): 0.3219\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1585,  E(|Y-Yhat|): 0.3193,  E(|Yhat-Yhat'|): 0.3216\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1569,  E(|Y-Yhat|): 0.3170,  E(|Yhat-Yhat'|): 0.3202\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0123,  E(|Y-Yhat|): 0.0254,  E(|Yhat-Yhat'|): 0.0262\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5458,  E(|Y-Yhat|): 0.9608,  E(|Yhat-Yhat'|): 0.8300\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3714,  E(|Y-Yhat|): 0.7468,  E(|Yhat-Yhat'|): 0.7507\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3669,  E(|Y-Yhat|): 0.7401,  E(|Yhat-Yhat'|): 0.7463\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3724,  E(|Y-Yhat|): 0.7525,  E(|Yhat-Yhat'|): 0.7602\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0095,  E(|Y-Yhat|): 0.0192,  E(|Yhat-Yhat'|): 0.0195\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8438,  E(|Y-Yhat|): 1.0367,  E(|Yhat-Yhat'|): 0.3857\n",
      "[Epoch 100 (33%), batch 9] energy-loss: -0.0303,  E(|Y-Yhat|): 35.4075,  E(|Yhat-Yhat'|): 70.8757\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2064,  E(|Y-Yhat|): 21.2498,  E(|Yhat-Yhat'|): 42.0869\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5370,  E(|Y-Yhat|): 10.8964,  E(|Yhat-Yhat'|): 20.7187\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0080,  E(|Y-Yhat|): 0.2460,  E(|Yhat-Yhat'|): 0.4759\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2169,  E(|Y-Yhat|): 0.3730,  E(|Yhat-Yhat'|): 0.3122\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1318,  E(|Y-Yhat|): 0.2646,  E(|Yhat-Yhat'|): 0.2655\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1295,  E(|Y-Yhat|): 0.2657,  E(|Yhat-Yhat'|): 0.2724\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1319,  E(|Y-Yhat|): 0.2686,  E(|Yhat-Yhat'|): 0.2733\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0047,  E(|Y-Yhat|): 0.0097,  E(|Yhat-Yhat'|): 0.0099\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3972,  E(|Y-Yhat|): 0.6941,  E(|Yhat-Yhat'|): 0.5938\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2427,  E(|Y-Yhat|): 0.4935,  E(|Yhat-Yhat'|): 0.5016\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2361,  E(|Y-Yhat|): 0.4793,  E(|Yhat-Yhat'|): 0.4865\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2414,  E(|Y-Yhat|): 0.4820,  E(|Yhat-Yhat'|): 0.4812\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0093,  E(|Y-Yhat|): 0.0185,  E(|Yhat-Yhat'|): 0.0184\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2191,  E(|Y-Yhat|): 0.4106,  E(|Yhat-Yhat'|): 0.3830\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1626,  E(|Y-Yhat|): 0.3198,  E(|Yhat-Yhat'|): 0.3144\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1619,  E(|Y-Yhat|): 0.3231,  E(|Yhat-Yhat'|): 0.3224\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1606,  E(|Y-Yhat|): 0.3224,  E(|Yhat-Yhat'|): 0.3235\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0090,  E(|Y-Yhat|): 0.0182,  E(|Yhat-Yhat'|): 0.0185\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4976,  E(|Y-Yhat|): 0.9621,  E(|Yhat-Yhat'|): 0.9290\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3595,  E(|Y-Yhat|): 0.7413,  E(|Yhat-Yhat'|): 0.7637\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3654,  E(|Y-Yhat|): 0.7336,  E(|Yhat-Yhat'|): 0.7364\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3638,  E(|Y-Yhat|): 0.7322,  E(|Yhat-Yhat'|): 0.7369\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3436,  E(|Y-Yhat|): 0.6925,  E(|Yhat-Yhat'|): 0.6979\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7511,  E(|Y-Yhat|): 0.9592,  E(|Yhat-Yhat'|): 0.4162\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4345,  E(|Y-Yhat|): 4.7226,  E(|Yhat-Yhat'|): 8.5761\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4125,  E(|Y-Yhat|): 2.7793,  E(|Yhat-Yhat'|): 4.7337\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4432,  E(|Y-Yhat|): 2.5975,  E(|Yhat-Yhat'|): 4.3087\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3321,  E(|Y-Yhat|): 2.0488,  E(|Yhat-Yhat'|): 3.4335\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2158,  E(|Y-Yhat|): 0.3880,  E(|Yhat-Yhat'|): 0.3442\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1300,  E(|Y-Yhat|): 0.2613,  E(|Yhat-Yhat'|): 0.2626\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1287,  E(|Y-Yhat|): 0.2576,  E(|Yhat-Yhat'|): 0.2577\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1312,  E(|Y-Yhat|): 0.2658,  E(|Yhat-Yhat'|): 0.2690\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1154,  E(|Y-Yhat|): 0.2331,  E(|Yhat-Yhat'|): 0.2354\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3875,  E(|Y-Yhat|): 0.6536,  E(|Yhat-Yhat'|): 0.5322\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2398,  E(|Y-Yhat|): 0.4811,  E(|Yhat-Yhat'|): 0.4826\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2371,  E(|Y-Yhat|): 0.4843,  E(|Yhat-Yhat'|): 0.4946\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2369,  E(|Y-Yhat|): 0.4783,  E(|Yhat-Yhat'|): 0.4829\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2043,  E(|Y-Yhat|): 0.4110,  E(|Yhat-Yhat'|): 0.4134\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2479,  E(|Y-Yhat|): 0.4414,  E(|Yhat-Yhat'|): 0.3870\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1547,  E(|Y-Yhat|): 0.3150,  E(|Yhat-Yhat'|): 0.3206\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1616,  E(|Y-Yhat|): 0.3185,  E(|Yhat-Yhat'|): 0.3137\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1600,  E(|Y-Yhat|): 0.3218,  E(|Yhat-Yhat'|): 0.3235\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1407,  E(|Y-Yhat|): 0.2859,  E(|Yhat-Yhat'|): 0.2904\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5493,  E(|Y-Yhat|): 0.9759,  E(|Yhat-Yhat'|): 0.8532\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3677,  E(|Y-Yhat|): 0.7534,  E(|Yhat-Yhat'|): 0.7714\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3622,  E(|Y-Yhat|): 0.7387,  E(|Yhat-Yhat'|): 0.7529\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3621,  E(|Y-Yhat|): 0.7494,  E(|Yhat-Yhat'|): 0.7746\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0015,  E(|Y-Yhat|): 0.0031,  E(|Yhat-Yhat'|): 0.0033\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9111,  E(|Y-Yhat|): 1.1027,  E(|Yhat-Yhat'|): 0.3832\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6769,  E(|Y-Yhat|): 25.2315,  E(|Yhat-Yhat'|): 49.1092\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4618,  E(|Y-Yhat|): 11.9430,  E(|Yhat-Yhat'|): 22.9623\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -2.9612,  E(|Y-Yhat|): 115.8888,  E(|Yhat-Yhat'|): 237.7000\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0004,  E(|Y-Yhat|): 0.4293,  E(|Yhat-Yhat'|): 0.8577\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2139,  E(|Y-Yhat|): 0.3762,  E(|Yhat-Yhat'|): 0.3246\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1321,  E(|Y-Yhat|): 0.2724,  E(|Yhat-Yhat'|): 0.2808\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1315,  E(|Y-Yhat|): 0.2643,  E(|Yhat-Yhat'|): 0.2656\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1335,  E(|Y-Yhat|): 0.2669,  E(|Yhat-Yhat'|): 0.2669\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0007,  E(|Y-Yhat|): 0.0015,  E(|Yhat-Yhat'|): 0.0016\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3728,  E(|Y-Yhat|): 0.6773,  E(|Yhat-Yhat'|): 0.6090\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2358,  E(|Y-Yhat|): 0.4864,  E(|Yhat-Yhat'|): 0.5011\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2386,  E(|Y-Yhat|): 0.4723,  E(|Yhat-Yhat'|): 0.4675\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2413,  E(|Y-Yhat|): 0.4784,  E(|Yhat-Yhat'|): 0.4742\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0014,  E(|Y-Yhat|): 0.0030,  E(|Yhat-Yhat'|): 0.0030\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2323,  E(|Y-Yhat|): 0.4247,  E(|Yhat-Yhat'|): 0.3848\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1576,  E(|Y-Yhat|): 0.3191,  E(|Yhat-Yhat'|): 0.3230\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1589,  E(|Y-Yhat|): 0.3229,  E(|Yhat-Yhat'|): 0.3281\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1595,  E(|Y-Yhat|): 0.3226,  E(|Yhat-Yhat'|): 0.3263\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0014,  E(|Y-Yhat|): 0.0030,  E(|Yhat-Yhat'|): 0.0032\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5338,  E(|Y-Yhat|): 0.9403,  E(|Yhat-Yhat'|): 0.8130\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3613,  E(|Y-Yhat|): 0.7336,  E(|Yhat-Yhat'|): 0.7447\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3693,  E(|Y-Yhat|): 0.7385,  E(|Yhat-Yhat'|): 0.7384\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3684,  E(|Y-Yhat|): 0.7509,  E(|Yhat-Yhat'|): 0.7651\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0129,  E(|Y-Yhat|): 0.0268,  E(|Yhat-Yhat'|): 0.0278\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8614,  E(|Y-Yhat|): 1.0737,  E(|Yhat-Yhat'|): 0.4245\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4860,  E(|Y-Yhat|): 9.0278,  E(|Yhat-Yhat'|): 17.0836\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3932,  E(|Y-Yhat|): 6.7184,  E(|Yhat-Yhat'|): 12.6505\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4339,  E(|Y-Yhat|): 2.8082,  E(|Yhat-Yhat'|): 4.7487\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0127,  E(|Y-Yhat|): 0.0857,  E(|Yhat-Yhat'|): 0.1460\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2070,  E(|Y-Yhat|): 0.3689,  E(|Yhat-Yhat'|): 0.3237\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1306,  E(|Y-Yhat|): 0.2623,  E(|Yhat-Yhat'|): 0.2634\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1312,  E(|Y-Yhat|): 0.2666,  E(|Yhat-Yhat'|): 0.2707\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1325,  E(|Y-Yhat|): 0.2654,  E(|Yhat-Yhat'|): 0.2658\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0063,  E(|Y-Yhat|): 0.0127,  E(|Yhat-Yhat'|): 0.0129\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3965,  E(|Y-Yhat|): 0.6730,  E(|Yhat-Yhat'|): 0.5530\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2387,  E(|Y-Yhat|): 0.4847,  E(|Yhat-Yhat'|): 0.4920\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2382,  E(|Y-Yhat|): 0.4772,  E(|Yhat-Yhat'|): 0.4780\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2341,  E(|Y-Yhat|): 0.4866,  E(|Yhat-Yhat'|): 0.5048\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0122,  E(|Y-Yhat|): 0.0261,  E(|Yhat-Yhat'|): 0.0277\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2228,  E(|Y-Yhat|): 0.4230,  E(|Yhat-Yhat'|): 0.4005\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1599,  E(|Y-Yhat|): 0.3228,  E(|Yhat-Yhat'|): 0.3258\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1628,  E(|Y-Yhat|): 0.3257,  E(|Yhat-Yhat'|): 0.3258\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1566,  E(|Y-Yhat|): 0.3188,  E(|Yhat-Yhat'|): 0.3244\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0120,  E(|Y-Yhat|): 0.0244,  E(|Yhat-Yhat'|): 0.0247\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5367,  E(|Y-Yhat|): 0.9830,  E(|Yhat-Yhat'|): 0.8927\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3672,  E(|Y-Yhat|): 0.7378,  E(|Yhat-Yhat'|): 0.7414\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3654,  E(|Y-Yhat|): 0.7507,  E(|Yhat-Yhat'|): 0.7707\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3663,  E(|Y-Yhat|): 0.7441,  E(|Yhat-Yhat'|): 0.7556\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0008,  E(|Y-Yhat|): 0.0016,  E(|Yhat-Yhat'|): 0.0017\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8405,  E(|Y-Yhat|): 1.1075,  E(|Yhat-Yhat'|): 0.5339\n",
      "[Epoch 100 (33%), batch 9] energy-loss: -2.9888,  E(|Y-Yhat|): 443.3170,  E(|Yhat-Yhat'|): 892.6115\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 6.3768,  E(|Y-Yhat|): 643.2135,  E(|Yhat-Yhat'|): 1273.6735\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 7.5749,  E(|Y-Yhat|): 846.7131,  E(|Yhat-Yhat'|): 1678.2765\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0052,  E(|Y-Yhat|): 1.4933,  E(|Yhat-Yhat'|): 2.9971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2138,  E(|Y-Yhat|): 0.3736,  E(|Yhat-Yhat'|): 0.3196\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1335,  E(|Y-Yhat|): 0.2674,  E(|Yhat-Yhat'|): 0.2676\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1342,  E(|Y-Yhat|): 0.2663,  E(|Yhat-Yhat'|): 0.2641\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1353,  E(|Y-Yhat|): 0.2714,  E(|Yhat-Yhat'|): 0.2722\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0004,  E(|Y-Yhat|): 0.0008,  E(|Yhat-Yhat'|): 0.0008\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3960,  E(|Y-Yhat|): 0.6586,  E(|Yhat-Yhat'|): 0.5251\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2404,  E(|Y-Yhat|): 0.4777,  E(|Yhat-Yhat'|): 0.4746\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2425,  E(|Y-Yhat|): 0.4943,  E(|Yhat-Yhat'|): 0.5035\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2397,  E(|Y-Yhat|): 0.4851,  E(|Yhat-Yhat'|): 0.4907\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0008,  E(|Y-Yhat|): 0.0017,  E(|Yhat-Yhat'|): 0.0017\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2240,  E(|Y-Yhat|): 0.4164,  E(|Yhat-Yhat'|): 0.3849\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1630,  E(|Y-Yhat|): 0.3243,  E(|Yhat-Yhat'|): 0.3227\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1614,  E(|Y-Yhat|): 0.3223,  E(|Yhat-Yhat'|): 0.3218\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1640,  E(|Y-Yhat|): 0.3230,  E(|Yhat-Yhat'|): 0.3181\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0008,  E(|Y-Yhat|): 0.0016,  E(|Yhat-Yhat'|): 0.0016\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=1, dy=1, k=1, true_function =\"log\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(2, -2, 100)\n",
    "    y_eval = M0* (log_lin(A0 * x_eval))\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1a517dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0001),\n",
       " tensor(0.0044),\n",
       " tensor(0.0002),\n",
       " tensor(0.0002),\n",
       " tensor(0.0004))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37e321",
   "metadata": {},
   "source": [
    "# pre ANM, comparing 4 loss functions under different true fucntions ($X, Y \\in R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daccc0fe",
   "metadata": {},
   "source": [
    "## True function: softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c74c694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7883,  E(|Y-Yhat|): 1.3785,  E(|Yhat-Yhat'|): 1.1804\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4496,  E(|Y-Yhat|): 0.9141,  E(|Yhat-Yhat'|): 0.9290\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4320,  E(|Y-Yhat|): 0.8860,  E(|Yhat-Yhat'|): 0.9080\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4466,  E(|Y-Yhat|): 0.9004,  E(|Yhat-Yhat'|): 0.9077\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.1871,  E(|Y-Yhat|): 8.5639,  E(|Yhat-Yhat'|): 8.7537\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5464,  E(|Y-Yhat|): 1.9534,  E(|Yhat-Yhat'|): 0.8139\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9020,  E(|Y-Yhat|): 48.9623,  E(|Yhat-Yhat'|): 94.1206\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -3.6075,  E(|Y-Yhat|): 305.8852,  E(|Yhat-Yhat'|): 618.9855\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0715,  E(|Y-Yhat|): 120.0311,  E(|Yhat-Yhat'|): 237.9193\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 8.6119,  E(|Y-Yhat|): 862.4102,  E(|Yhat-Yhat'|): 1707.5966\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3215,  E(|Y-Yhat|): 0.5445,  E(|Yhat-Yhat'|): 0.4460\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1613,  E(|Y-Yhat|): 0.3259,  E(|Yhat-Yhat'|): 0.3292\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1612,  E(|Y-Yhat|): 0.3270,  E(|Yhat-Yhat'|): 0.3316\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1647,  E(|Y-Yhat|): 0.3315,  E(|Yhat-Yhat'|): 0.3336\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3248,  E(|Y-Yhat|): 0.6491,  E(|Yhat-Yhat'|): 0.6484\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5656,  E(|Y-Yhat|): 0.9943,  E(|Yhat-Yhat'|): 0.8573\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3016,  E(|Y-Yhat|): 0.6109,  E(|Yhat-Yhat'|): 0.6186\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3034,  E(|Y-Yhat|): 0.6128,  E(|Yhat-Yhat'|): 0.6189\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3050,  E(|Y-Yhat|): 0.6151,  E(|Yhat-Yhat'|): 0.6201\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7826,  E(|Y-Yhat|): 1.5946,  E(|Yhat-Yhat'|): 1.6240\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3087,  E(|Y-Yhat|): 0.5629,  E(|Yhat-Yhat'|): 0.5083\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1848,  E(|Y-Yhat|): 0.3812,  E(|Yhat-Yhat'|): 0.3928\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1825,  E(|Y-Yhat|): 0.3709,  E(|Yhat-Yhat'|): 0.3767\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1829,  E(|Y-Yhat|): 0.3716,  E(|Yhat-Yhat'|): 0.3774\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3236,  E(|Y-Yhat|): 0.6443,  E(|Yhat-Yhat'|): 0.6415\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7512,  E(|Y-Yhat|): 1.3177,  E(|Yhat-Yhat'|): 1.1330\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4535,  E(|Y-Yhat|): 0.9323,  E(|Yhat-Yhat'|): 0.9576\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4511,  E(|Y-Yhat|): 0.9115,  E(|Yhat-Yhat'|): 0.9208\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4537,  E(|Y-Yhat|): 0.9131,  E(|Yhat-Yhat'|): 0.9187\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0679,  E(|Y-Yhat|): 0.1383,  E(|Yhat-Yhat'|): 0.1408\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7311,  E(|Y-Yhat|): 2.0454,  E(|Yhat-Yhat'|): 0.6286\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7145,  E(|Y-Yhat|): 11.5682,  E(|Yhat-Yhat'|): 21.7074\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9193,  E(|Y-Yhat|): 29.3029,  E(|Yhat-Yhat'|): 56.7673\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4880,  E(|Y-Yhat|): 58.0703,  E(|Yhat-Yhat'|): 113.1646\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1883,  E(|Y-Yhat|): 5.5953,  E(|Yhat-Yhat'|): 10.8139\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2891,  E(|Y-Yhat|): 0.5286,  E(|Yhat-Yhat'|): 0.4790\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1685,  E(|Y-Yhat|): 0.3436,  E(|Yhat-Yhat'|): 0.3501\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1687,  E(|Y-Yhat|): 0.3361,  E(|Yhat-Yhat'|): 0.3348\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1727,  E(|Y-Yhat|): 0.3437,  E(|Yhat-Yhat'|): 0.3421\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0296,  E(|Y-Yhat|): 0.0602,  E(|Yhat-Yhat'|): 0.0611\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5627,  E(|Y-Yhat|): 0.9933,  E(|Yhat-Yhat'|): 0.8611\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3081,  E(|Y-Yhat|): 0.6307,  E(|Yhat-Yhat'|): 0.6452\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3143,  E(|Y-Yhat|): 0.6359,  E(|Yhat-Yhat'|): 0.6432\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3098,  E(|Y-Yhat|): 0.6235,  E(|Yhat-Yhat'|): 0.6274\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0572,  E(|Y-Yhat|): 0.1152,  E(|Yhat-Yhat'|): 0.1160\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3033,  E(|Y-Yhat|): 0.5544,  E(|Yhat-Yhat'|): 0.5022\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1907,  E(|Y-Yhat|): 0.3849,  E(|Yhat-Yhat'|): 0.3885\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1920,  E(|Y-Yhat|): 0.3792,  E(|Yhat-Yhat'|): 0.3745\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1884,  E(|Y-Yhat|): 0.3812,  E(|Yhat-Yhat'|): 0.3857\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0489,  E(|Y-Yhat|): 0.1014,  E(|Yhat-Yhat'|): 0.1051\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7550,  E(|Y-Yhat|): 1.3108,  E(|Yhat-Yhat'|): 1.1116\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4496,  E(|Y-Yhat|): 0.9133,  E(|Yhat-Yhat'|): 0.9275\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4469,  E(|Y-Yhat|): 0.8955,  E(|Yhat-Yhat'|): 0.8971\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4488,  E(|Y-Yhat|): 0.9145,  E(|Yhat-Yhat'|): 0.9312\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0528,  E(|Y-Yhat|): 0.1096,  E(|Yhat-Yhat'|): 0.1136\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9134,  E(|Y-Yhat|): 2.2265,  E(|Yhat-Yhat'|): 0.6263\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6986,  E(|Y-Yhat|): 16.0364,  E(|Yhat-Yhat'|): 30.6757\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0162,  E(|Y-Yhat|): 207.9156,  E(|Yhat-Yhat'|): 413.7988\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8618,  E(|Y-Yhat|): 77.2892,  E(|Yhat-Yhat'|): 152.8549\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1412,  E(|Y-Yhat|): 6.5043,  E(|Yhat-Yhat'|): 12.7262\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3025,  E(|Y-Yhat|): 0.5388,  E(|Yhat-Yhat'|): 0.4726\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1654,  E(|Y-Yhat|): 0.3335,  E(|Yhat-Yhat'|): 0.3361\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1727,  E(|Y-Yhat|): 0.3361,  E(|Yhat-Yhat'|): 0.3270\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1684,  E(|Y-Yhat|): 0.3388,  E(|Yhat-Yhat'|): 0.3408\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0239,  E(|Y-Yhat|): 0.0497,  E(|Yhat-Yhat'|): 0.0514\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5979,  E(|Y-Yhat|): 0.9819,  E(|Yhat-Yhat'|): 0.7681\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3093,  E(|Y-Yhat|): 0.6298,  E(|Yhat-Yhat'|): 0.6409\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3144,  E(|Y-Yhat|): 0.6324,  E(|Yhat-Yhat'|): 0.6360\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3110,  E(|Y-Yhat|): 0.6192,  E(|Yhat-Yhat'|): 0.6163\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0469,  E(|Y-Yhat|): 0.0923,  E(|Yhat-Yhat'|): 0.0908\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3356,  E(|Y-Yhat|): 0.5805,  E(|Yhat-Yhat'|): 0.4898\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1929,  E(|Y-Yhat|): 0.3816,  E(|Yhat-Yhat'|): 0.3776\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1920,  E(|Y-Yhat|): 0.3805,  E(|Yhat-Yhat'|): 0.3769\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1886,  E(|Y-Yhat|): 0.3738,  E(|Yhat-Yhat'|): 0.3704\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0409,  E(|Y-Yhat|): 0.0822,  E(|Yhat-Yhat'|): 0.0828\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8237,  E(|Y-Yhat|): 1.3655,  E(|Yhat-Yhat'|): 1.0837\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4529,  E(|Y-Yhat|): 0.9205,  E(|Yhat-Yhat'|): 0.9352\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4547,  E(|Y-Yhat|): 0.9231,  E(|Yhat-Yhat'|): 0.9367\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4500,  E(|Y-Yhat|): 0.9066,  E(|Yhat-Yhat'|): 0.9132\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0889,  E(|Y-Yhat|): 0.1801,  E(|Yhat-Yhat'|): 0.1825\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7812,  E(|Y-Yhat|): 2.1171,  E(|Yhat-Yhat'|): 0.6718\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6572,  E(|Y-Yhat|): 66.9641,  E(|Yhat-Yhat'|): 130.6138\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 19.8507,  E(|Y-Yhat|): 78595.8681,  E(|Yhat-Yhat'|): 157152.0347\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -1373.3168,  E(|Y-Yhat|): 108433.2370,  E(|Yhat-Yhat'|): 219613.1076\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 139.7432,  E(|Y-Yhat|): 15114.2939,  E(|Yhat-Yhat'|): 29949.1016\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3163,  E(|Y-Yhat|): 0.5363,  E(|Yhat-Yhat'|): 0.4400\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1676,  E(|Y-Yhat|): 0.3455,  E(|Yhat-Yhat'|): 0.3556\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1665,  E(|Y-Yhat|): 0.3346,  E(|Yhat-Yhat'|): 0.3362\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1653,  E(|Y-Yhat|): 0.3339,  E(|Yhat-Yhat'|): 0.3372\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0407,  E(|Y-Yhat|): 0.0819,  E(|Yhat-Yhat'|): 0.0823\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6026,  E(|Y-Yhat|): 1.0207,  E(|Yhat-Yhat'|): 0.8363\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3074,  E(|Y-Yhat|): 0.6324,  E(|Yhat-Yhat'|): 0.6500\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3104,  E(|Y-Yhat|): 0.6325,  E(|Yhat-Yhat'|): 0.6441\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3170,  E(|Y-Yhat|): 0.6271,  E(|Yhat-Yhat'|): 0.6200\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0713,  E(|Y-Yhat|): 0.1446,  E(|Yhat-Yhat'|): 0.1464\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3048,  E(|Y-Yhat|): 0.5742,  E(|Yhat-Yhat'|): 0.5388\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1921,  E(|Y-Yhat|): 0.3842,  E(|Yhat-Yhat'|): 0.3843\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1872,  E(|Y-Yhat|): 0.3869,  E(|Yhat-Yhat'|): 0.3994\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1904,  E(|Y-Yhat|): 0.3843,  E(|Yhat-Yhat'|): 0.3878\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0599,  E(|Y-Yhat|): 0.1231,  E(|Yhat-Yhat'|): 0.1263\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6734,  E(|Y-Yhat|): 1.1328,  E(|Yhat-Yhat'|): 0.9189\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3904,  E(|Y-Yhat|): 0.7953,  E(|Yhat-Yhat'|): 0.8098\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3808,  E(|Y-Yhat|): 0.7686,  E(|Yhat-Yhat'|): 0.7756\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3876,  E(|Y-Yhat|): 0.7712,  E(|Yhat-Yhat'|): 0.7672\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5750,  E(|Y-Yhat|): 1.1435,  E(|Yhat-Yhat'|): 1.1369\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7612,  E(|Y-Yhat|): 2.1636,  E(|Yhat-Yhat'|): 0.8049\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8831,  E(|Y-Yhat|): 11.2050,  E(|Yhat-Yhat'|): 20.6440\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 30.8090,  E(|Y-Yhat|): 2633.8457,  E(|Yhat-Yhat'|): 5206.0734\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 14.4534,  E(|Y-Yhat|): 791.0273,  E(|Yhat-Yhat'|): 1553.1477\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 9.0131,  E(|Y-Yhat|): 382.4596,  E(|Yhat-Yhat'|): 746.8931\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1893,  E(|Y-Yhat|): 0.3423,  E(|Yhat-Yhat'|): 0.3060\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1186,  E(|Y-Yhat|): 0.2419,  E(|Yhat-Yhat'|): 0.2467\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1125,  E(|Y-Yhat|): 0.2374,  E(|Yhat-Yhat'|): 0.2497\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1121,  E(|Y-Yhat|): 0.2287,  E(|Yhat-Yhat'|): 0.2331\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0867,  E(|Y-Yhat|): 0.1765,  E(|Yhat-Yhat'|): 0.1798\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4245,  E(|Y-Yhat|): 0.7061,  E(|Yhat-Yhat'|): 0.5631\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2523,  E(|Y-Yhat|): 0.5148,  E(|Yhat-Yhat'|): 0.5248\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2489,  E(|Y-Yhat|): 0.4992,  E(|Yhat-Yhat'|): 0.5005\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2541,  E(|Y-Yhat|): 0.5012,  E(|Yhat-Yhat'|): 0.4943\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1789,  E(|Y-Yhat|): 0.3486,  E(|Yhat-Yhat'|): 0.3394\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2457,  E(|Y-Yhat|): 0.4529,  E(|Yhat-Yhat'|): 0.4144\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1250,  E(|Y-Yhat|): 0.2601,  E(|Yhat-Yhat'|): 0.2703\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1218,  E(|Y-Yhat|): 0.2461,  E(|Yhat-Yhat'|): 0.2486\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1236,  E(|Y-Yhat|): 0.2467,  E(|Yhat-Yhat'|): 0.2462\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0983,  E(|Y-Yhat|): 0.1965,  E(|Yhat-Yhat'|): 0.1964\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7144,  E(|Y-Yhat|): 1.2481,  E(|Yhat-Yhat'|): 1.0673\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4259,  E(|Y-Yhat|): 0.8561,  E(|Yhat-Yhat'|): 0.8604\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4100,  E(|Y-Yhat|): 0.8297,  E(|Yhat-Yhat'|): 0.8392\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4193,  E(|Y-Yhat|): 0.8340,  E(|Yhat-Yhat'|): 0.8294\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0091,  E(|Y-Yhat|): 0.0184,  E(|Yhat-Yhat'|): 0.0185\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7348,  E(|Y-Yhat|): 2.0988,  E(|Yhat-Yhat'|): 0.7280\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0043,  E(|Y-Yhat|): 17.3096,  E(|Yhat-Yhat'|): 32.6106\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6758,  E(|Y-Yhat|): 7.8118,  E(|Yhat-Yhat'|): 14.2720\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8289,  E(|Y-Yhat|): 29.2845,  E(|Yhat-Yhat'|): 56.9112\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0109,  E(|Y-Yhat|): 0.3436,  E(|Yhat-Yhat'|): 0.6654\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2482,  E(|Y-Yhat|): 0.4409,  E(|Yhat-Yhat'|): 0.3853\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1362,  E(|Y-Yhat|): 0.2778,  E(|Yhat-Yhat'|): 0.2831\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1404,  E(|Y-Yhat|): 0.2811,  E(|Yhat-Yhat'|): 0.2814\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1357,  E(|Y-Yhat|): 0.2731,  E(|Yhat-Yhat'|): 0.2748\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0044,  E(|Y-Yhat|): 0.0088,  E(|Yhat-Yhat'|): 0.0087\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5318,  E(|Y-Yhat|): 0.8758,  E(|Yhat-Yhat'|): 0.6879\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2719,  E(|Y-Yhat|): 0.5550,  E(|Yhat-Yhat'|): 0.5661\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2751,  E(|Y-Yhat|): 0.5472,  E(|Yhat-Yhat'|): 0.5441\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2724,  E(|Y-Yhat|): 0.5519,  E(|Yhat-Yhat'|): 0.5592\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0086,  E(|Y-Yhat|): 0.0174,  E(|Yhat-Yhat'|): 0.0174\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2735,  E(|Y-Yhat|): 0.4855,  E(|Yhat-Yhat'|): 0.4240\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1530,  E(|Y-Yhat|): 0.3066,  E(|Yhat-Yhat'|): 0.3071\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1577,  E(|Y-Yhat|): 0.3136,  E(|Yhat-Yhat'|): 0.3118\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1559,  E(|Y-Yhat|): 0.3063,  E(|Yhat-Yhat'|): 0.3008\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0078,  E(|Y-Yhat|): 0.0155,  E(|Yhat-Yhat'|): 0.0155\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4771,  E(|Y-Yhat|): 0.7791,  E(|Yhat-Yhat'|): 0.6040\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2954,  E(|Y-Yhat|): 0.5778,  E(|Yhat-Yhat'|): 0.5649\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2783,  E(|Y-Yhat|): 0.5768,  E(|Yhat-Yhat'|): 0.5970\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2935,  E(|Y-Yhat|): 0.5790,  E(|Yhat-Yhat'|): 0.5710\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0327,  E(|Y-Yhat|): 0.0615,  E(|Yhat-Yhat'|): 0.0576\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9137,  E(|Y-Yhat|): 2.2181,  E(|Yhat-Yhat'|): 0.6089\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6149,  E(|Y-Yhat|): 27.4333,  E(|Yhat-Yhat'|): 51.6367\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -2.1828,  E(|Y-Yhat|): 279.7703,  E(|Yhat-Yhat'|): 563.9063\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -0.6068,  E(|Y-Yhat|): 572.6294,  E(|Yhat-Yhat'|): 1146.4724\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3457,  E(|Y-Yhat|): 14.9671,  E(|Yhat-Yhat'|): 29.2427\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1251,  E(|Y-Yhat|): 0.2475,  E(|Yhat-Yhat'|): 0.2447\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0710,  E(|Y-Yhat|): 0.1450,  E(|Yhat-Yhat'|): 0.1480\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0693,  E(|Y-Yhat|): 0.1431,  E(|Yhat-Yhat'|): 0.1476\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0708,  E(|Y-Yhat|): 0.1406,  E(|Yhat-Yhat'|): 0.1395\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0114,  E(|Y-Yhat|): 0.0187,  E(|Yhat-Yhat'|): 0.0146\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3145,  E(|Y-Yhat|): 0.5725,  E(|Yhat-Yhat'|): 0.5161\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1653,  E(|Y-Yhat|): 0.3397,  E(|Yhat-Yhat'|): 0.3488\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1704,  E(|Y-Yhat|): 0.3525,  E(|Yhat-Yhat'|): 0.3642\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1681,  E(|Y-Yhat|): 0.3563,  E(|Yhat-Yhat'|): 0.3765\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0211,  E(|Y-Yhat|): 0.0422,  E(|Yhat-Yhat'|): 0.0422\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1441,  E(|Y-Yhat|): 0.2959,  E(|Yhat-Yhat'|): 0.3035\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0763,  E(|Y-Yhat|): 0.1606,  E(|Yhat-Yhat'|): 0.1686\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0763,  E(|Y-Yhat|): 0.1556,  E(|Yhat-Yhat'|): 0.1586\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0756,  E(|Y-Yhat|): 0.1545,  E(|Yhat-Yhat'|): 0.1579\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0148,  E(|Y-Yhat|): 0.0272,  E(|Yhat-Yhat'|): 0.0247\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7449,  E(|Y-Yhat|): 1.3421,  E(|Yhat-Yhat'|): 1.1943\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4519,  E(|Y-Yhat|): 0.9139,  E(|Yhat-Yhat'|): 0.9240\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4469,  E(|Y-Yhat|): 0.9109,  E(|Yhat-Yhat'|): 0.9280\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4457,  E(|Y-Yhat|): 0.9191,  E(|Yhat-Yhat'|): 0.9468\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3697,  E(|Y-Yhat|): 0.7584,  E(|Yhat-Yhat'|): 0.7773\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7229,  E(|Y-Yhat|): 2.0029,  E(|Yhat-Yhat'|): 0.5599\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4386,  E(|Y-Yhat|): 26.4534,  E(|Yhat-Yhat'|): 52.0296\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.1368,  E(|Y-Yhat|): 80.8475,  E(|Yhat-Yhat'|): 157.4215\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2052,  E(|Y-Yhat|): 100.8579,  E(|Yhat-Yhat'|): 199.3055\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0986,  E(|Y-Yhat|): 55.3577,  E(|Yhat-Yhat'|): 110.9126\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2942,  E(|Y-Yhat|): 0.5391,  E(|Yhat-Yhat'|): 0.4898\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1668,  E(|Y-Yhat|): 0.3408,  E(|Yhat-Yhat'|): 0.3480\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1689,  E(|Y-Yhat|): 0.3349,  E(|Yhat-Yhat'|): 0.3321\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1663,  E(|Y-Yhat|): 0.3373,  E(|Yhat-Yhat'|): 0.3421\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1181,  E(|Y-Yhat|): 0.2361,  E(|Yhat-Yhat'|): 0.2359\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6543,  E(|Y-Yhat|): 1.0814,  E(|Yhat-Yhat'|): 0.8541\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3088,  E(|Y-Yhat|): 0.6296,  E(|Yhat-Yhat'|): 0.6417\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3154,  E(|Y-Yhat|): 0.6250,  E(|Yhat-Yhat'|): 0.6191\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3144,  E(|Y-Yhat|): 0.6356,  E(|Yhat-Yhat'|): 0.6423\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2143,  E(|Y-Yhat|): 0.4358,  E(|Yhat-Yhat'|): 0.4430\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3069,  E(|Y-Yhat|): 0.5596,  E(|Yhat-Yhat'|): 0.5053\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1918,  E(|Y-Yhat|): 0.3808,  E(|Yhat-Yhat'|): 0.3781\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1884,  E(|Y-Yhat|): 0.3793,  E(|Yhat-Yhat'|): 0.3819\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1912,  E(|Y-Yhat|): 0.3820,  E(|Yhat-Yhat'|): 0.3817\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1439,  E(|Y-Yhat|): 0.2903,  E(|Yhat-Yhat'|): 0.2929\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6493,  E(|Y-Yhat|): 1.0778,  E(|Yhat-Yhat'|): 0.8571\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4089,  E(|Y-Yhat|): 0.8162,  E(|Yhat-Yhat'|): 0.8145\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4040,  E(|Y-Yhat|): 0.8333,  E(|Yhat-Yhat'|): 0.8585\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4028,  E(|Y-Yhat|): 0.8079,  E(|Yhat-Yhat'|): 0.8102\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0382,  E(|Y-Yhat|): 0.0760,  E(|Yhat-Yhat'|): 0.0756\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8175,  E(|Y-Yhat|): 2.1606,  E(|Yhat-Yhat'|): 0.6862\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0959,  E(|Y-Yhat|): 4.8375,  E(|Yhat-Yhat'|): 7.4831\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9009,  E(|Y-Yhat|): 6.6139,  E(|Yhat-Yhat'|): 11.4260\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9332,  E(|Y-Yhat|): 7.1773,  E(|Yhat-Yhat'|): 12.4881\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0364,  E(|Y-Yhat|): 0.2945,  E(|Yhat-Yhat'|): 0.5161\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2064,  E(|Y-Yhat|): 0.3764,  E(|Yhat-Yhat'|): 0.3400\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1246,  E(|Y-Yhat|): 0.2666,  E(|Yhat-Yhat'|): 0.2839\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1296,  E(|Y-Yhat|): 0.2665,  E(|Yhat-Yhat'|): 0.2739\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1274,  E(|Y-Yhat|): 0.2605,  E(|Yhat-Yhat'|): 0.2663\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0157,  E(|Y-Yhat|): 0.0319,  E(|Yhat-Yhat'|): 0.0325\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4936,  E(|Y-Yhat|): 0.8081,  E(|Yhat-Yhat'|): 0.6290\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2728,  E(|Y-Yhat|): 0.5372,  E(|Yhat-Yhat'|): 0.5287\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2646,  E(|Y-Yhat|): 0.5437,  E(|Yhat-Yhat'|): 0.5582\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2604,  E(|Y-Yhat|): 0.5361,  E(|Yhat-Yhat'|): 0.5514\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0281,  E(|Y-Yhat|): 0.0560,  E(|Yhat-Yhat'|): 0.0557\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2505,  E(|Y-Yhat|): 0.4730,  E(|Yhat-Yhat'|): 0.4450\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1424,  E(|Y-Yhat|): 0.2835,  E(|Yhat-Yhat'|): 0.2822\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1396,  E(|Y-Yhat|): 0.2825,  E(|Yhat-Yhat'|): 0.2859\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1375,  E(|Y-Yhat|): 0.2807,  E(|Yhat-Yhat'|): 0.2864\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0237,  E(|Y-Yhat|): 0.0492,  E(|Yhat-Yhat'|): 0.0510\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7911,  E(|Y-Yhat|): 1.3661,  E(|Yhat-Yhat'|): 1.1500\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4568,  E(|Y-Yhat|): 0.9244,  E(|Yhat-Yhat'|): 0.9352\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4469,  E(|Y-Yhat|): 0.8958,  E(|Yhat-Yhat'|): 0.8978\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4450,  E(|Y-Yhat|): 0.9096,  E(|Yhat-Yhat'|): 0.9292\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2570,  E(|Y-Yhat|): 0.5261,  E(|Yhat-Yhat'|): 0.5383\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8810,  E(|Y-Yhat|): 2.1547,  E(|Yhat-Yhat'|): 0.5473\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6823,  E(|Y-Yhat|): 87.7233,  E(|Yhat-Yhat'|): 172.0821\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9958,  E(|Y-Yhat|): 89.7262,  E(|Yhat-Yhat'|): 175.4607\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6610,  E(|Y-Yhat|): 16.0399,  E(|Yhat-Yhat'|): 30.7578\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1782,  E(|Y-Yhat|): 6.5282,  E(|Yhat-Yhat'|): 12.7000\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3181,  E(|Y-Yhat|): 0.5537,  E(|Yhat-Yhat'|): 0.4712\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1688,  E(|Y-Yhat|): 0.3397,  E(|Yhat-Yhat'|): 0.3418\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1666,  E(|Y-Yhat|): 0.3267,  E(|Yhat-Yhat'|): 0.3202\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1650,  E(|Y-Yhat|): 0.3321,  E(|Yhat-Yhat'|): 0.3342\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0900,  E(|Y-Yhat|): 0.1839,  E(|Yhat-Yhat'|): 0.1877\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6590,  E(|Y-Yhat|): 1.0773,  E(|Yhat-Yhat'|): 0.8367\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3175,  E(|Y-Yhat|): 0.6207,  E(|Yhat-Yhat'|): 0.6064\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3118,  E(|Y-Yhat|): 0.6361,  E(|Yhat-Yhat'|): 0.6486\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3101,  E(|Y-Yhat|): 0.6244,  E(|Yhat-Yhat'|): 0.6287\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1662,  E(|Y-Yhat|): 0.3411,  E(|Yhat-Yhat'|): 0.3498\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3241,  E(|Y-Yhat|): 0.5739,  E(|Yhat-Yhat'|): 0.4996\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1887,  E(|Y-Yhat|): 0.3783,  E(|Yhat-Yhat'|): 0.3793\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1868,  E(|Y-Yhat|): 0.3764,  E(|Yhat-Yhat'|): 0.3791\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1904,  E(|Y-Yhat|): 0.3810,  E(|Yhat-Yhat'|): 0.3813\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1199,  E(|Y-Yhat|): 0.2421,  E(|Yhat-Yhat'|): 0.2444\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 100)\n",
    "    x2 = torch.linspace(0, 5, 100)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = F.softplus(Z)   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a9ba2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0019),\n",
       " tensor(2.4601),\n",
       " tensor(0.0037),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c901040",
   "metadata": {},
   "source": [
    "## True function: cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "811119aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5730,  E(|Y-Yhat|): 0.9264,  E(|Yhat-Yhat'|): 0.7068\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4555,  E(|Y-Yhat|): 0.8951,  E(|Yhat-Yhat'|): 0.8792\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4450,  E(|Y-Yhat|): 0.8938,  E(|Yhat-Yhat'|): 0.8976\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4506,  E(|Y-Yhat|): 0.8974,  E(|Yhat-Yhat'|): 0.8936\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 94.9253,  E(|Y-Yhat|): 181.5517,  E(|Yhat-Yhat'|): 173.2528\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7626,  E(|Y-Yhat|): 2.1406,  E(|Yhat-Yhat'|): 0.7560\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4942,  E(|Y-Yhat|): 22.0840,  E(|Yhat-Yhat'|): 41.1795\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9444,  E(|Y-Yhat|): 21.7332,  E(|Yhat-Yhat'|): 41.5776\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3530,  E(|Y-Yhat|): 27.0575,  E(|Yhat-Yhat'|): 51.4091\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 80.3064,  E(|Y-Yhat|): 1840.3640,  E(|Yhat-Yhat'|): 3520.1152\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1731,  E(|Y-Yhat|): 0.3166,  E(|Yhat-Yhat'|): 0.2870\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1320,  E(|Y-Yhat|): 0.2668,  E(|Yhat-Yhat'|): 0.2696\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1328,  E(|Y-Yhat|): 0.2701,  E(|Yhat-Yhat'|): 0.2746\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1323,  E(|Y-Yhat|): 0.2615,  E(|Yhat-Yhat'|): 0.2584\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3579,  E(|Y-Yhat|): 0.7412,  E(|Yhat-Yhat'|): 0.7666\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3664,  E(|Y-Yhat|): 0.6137,  E(|Yhat-Yhat'|): 0.4945\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2824,  E(|Y-Yhat|): 0.5879,  E(|Yhat-Yhat'|): 0.6111\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2911,  E(|Y-Yhat|): 0.5924,  E(|Yhat-Yhat'|): 0.6025\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2959,  E(|Y-Yhat|): 0.6012,  E(|Yhat-Yhat'|): 0.6106\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.3704,  E(|Y-Yhat|): 2.7916,  E(|Yhat-Yhat'|): 2.8424\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1797,  E(|Y-Yhat|): 0.3407,  E(|Yhat-Yhat'|): 0.3219\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1441,  E(|Y-Yhat|): 0.2866,  E(|Yhat-Yhat'|): 0.2849\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1418,  E(|Y-Yhat|): 0.2886,  E(|Yhat-Yhat'|): 0.2935\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1455,  E(|Y-Yhat|): 0.2877,  E(|Yhat-Yhat'|): 0.2844\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3539,  E(|Y-Yhat|): 0.7203,  E(|Yhat-Yhat'|): 0.7329\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5566,  E(|Y-Yhat|): 0.9114,  E(|Yhat-Yhat'|): 0.7097\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4437,  E(|Y-Yhat|): 0.8643,  E(|Yhat-Yhat'|): 0.8412\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4516,  E(|Y-Yhat|): 0.8889,  E(|Yhat-Yhat'|): 0.8746\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4332,  E(|Y-Yhat|): 0.8737,  E(|Yhat-Yhat'|): 0.8810\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0642,  E(|Y-Yhat|): 0.1168,  E(|Yhat-Yhat'|): 0.1053\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8814,  E(|Y-Yhat|): 2.1711,  E(|Yhat-Yhat'|): 0.5793\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2044,  E(|Y-Yhat|): 17.3661,  E(|Yhat-Yhat'|): 32.3235\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9746,  E(|Y-Yhat|): 21.1551,  E(|Yhat-Yhat'|): 40.3610\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.5694,  E(|Y-Yhat|): 29.3348,  E(|Yhat-Yhat'|): 55.5308\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0666,  E(|Y-Yhat|): 1.3392,  E(|Yhat-Yhat'|): 2.5451\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1609,  E(|Y-Yhat|): 0.3094,  E(|Yhat-Yhat'|): 0.2969\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1287,  E(|Y-Yhat|): 0.2677,  E(|Yhat-Yhat'|): 0.2779\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1327,  E(|Y-Yhat|): 0.2609,  E(|Yhat-Yhat'|): 0.2565\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1298,  E(|Y-Yhat|): 0.2604,  E(|Yhat-Yhat'|): 0.2611\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0234,  E(|Y-Yhat|): 0.0432,  E(|Yhat-Yhat'|): 0.0396\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3612,  E(|Y-Yhat|): 0.5943,  E(|Yhat-Yhat'|): 0.4664\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2881,  E(|Y-Yhat|): 0.5747,  E(|Yhat-Yhat'|): 0.5733\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2911,  E(|Y-Yhat|): 0.5625,  E(|Yhat-Yhat'|): 0.5427\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2864,  E(|Y-Yhat|): 0.5681,  E(|Yhat-Yhat'|): 0.5634\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0428,  E(|Y-Yhat|): 0.0831,  E(|Yhat-Yhat'|): 0.0806\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1848,  E(|Y-Yhat|): 0.3498,  E(|Yhat-Yhat'|): 0.3301\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1442,  E(|Y-Yhat|): 0.2843,  E(|Yhat-Yhat'|): 0.2802\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1406,  E(|Y-Yhat|): 0.2851,  E(|Yhat-Yhat'|): 0.2889\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1409,  E(|Y-Yhat|): 0.2894,  E(|Yhat-Yhat'|): 0.2969\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0335,  E(|Y-Yhat|): 0.0648,  E(|Yhat-Yhat'|): 0.0626\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5513,  E(|Y-Yhat|): 0.8776,  E(|Yhat-Yhat'|): 0.6524\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4409,  E(|Y-Yhat|): 0.8405,  E(|Yhat-Yhat'|): 0.7993\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4178,  E(|Y-Yhat|): 0.8457,  E(|Yhat-Yhat'|): 0.8558\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4250,  E(|Y-Yhat|): 0.8518,  E(|Yhat-Yhat'|): 0.8535\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0174,  E(|Y-Yhat|): 0.0324,  E(|Yhat-Yhat'|): 0.0301\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9877,  E(|Y-Yhat|): 2.3126,  E(|Yhat-Yhat'|): 0.6497\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2222,  E(|Y-Yhat|): 10.2524,  E(|Yhat-Yhat'|): 18.0603\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2707,  E(|Y-Yhat|): 14.0485,  E(|Yhat-Yhat'|): 25.5555\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3509,  E(|Y-Yhat|): 6.7018,  E(|Yhat-Yhat'|): 10.7018\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0186,  E(|Y-Yhat|): 0.0880,  E(|Yhat-Yhat'|): 0.1389\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1611,  E(|Y-Yhat|): 0.3065,  E(|Yhat-Yhat'|): 0.2909\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1267,  E(|Y-Yhat|): 0.2544,  E(|Yhat-Yhat'|): 0.2554\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1273,  E(|Y-Yhat|): 0.2503,  E(|Yhat-Yhat'|): 0.2459\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1252,  E(|Y-Yhat|): 0.2523,  E(|Yhat-Yhat'|): 0.2543\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0078,  E(|Y-Yhat|): 0.0136,  E(|Yhat-Yhat'|): 0.0116\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3673,  E(|Y-Yhat|): 0.5917,  E(|Yhat-Yhat'|): 0.4488\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2686,  E(|Y-Yhat|): 0.5298,  E(|Yhat-Yhat'|): 0.5224\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2777,  E(|Y-Yhat|): 0.5610,  E(|Yhat-Yhat'|): 0.5668\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2807,  E(|Y-Yhat|): 0.5415,  E(|Yhat-Yhat'|): 0.5215\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0148,  E(|Y-Yhat|): 0.0265,  E(|Yhat-Yhat'|): 0.0234\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2171,  E(|Y-Yhat|): 0.4033,  E(|Yhat-Yhat'|): 0.3725\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1387,  E(|Y-Yhat|): 0.2765,  E(|Yhat-Yhat'|): 0.2756\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1367,  E(|Y-Yhat|): 0.2696,  E(|Yhat-Yhat'|): 0.2658\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1401,  E(|Y-Yhat|): 0.2825,  E(|Yhat-Yhat'|): 0.2847\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0129,  E(|Y-Yhat|): 0.0235,  E(|Yhat-Yhat'|): 0.0212\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5919,  E(|Y-Yhat|): 0.9594,  E(|Yhat-Yhat'|): 0.7349\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4628,  E(|Y-Yhat|): 0.8893,  E(|Yhat-Yhat'|): 0.8530\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4556,  E(|Y-Yhat|): 0.9047,  E(|Yhat-Yhat'|): 0.8982\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4471,  E(|Y-Yhat|): 0.8975,  E(|Yhat-Yhat'|): 0.9008\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1540,  E(|Y-Yhat|): 0.2853,  E(|Yhat-Yhat'|): 0.2626\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9131,  E(|Y-Yhat|): 2.2468,  E(|Yhat-Yhat'|): 0.6676\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3885,  E(|Y-Yhat|): 8.2480,  E(|Yhat-Yhat'|): 13.7188\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0160,  E(|Y-Yhat|): 12.6738,  E(|Yhat-Yhat'|): 23.3156\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.7153,  E(|Y-Yhat|): 12.5750,  E(|Yhat-Yhat'|): 21.7194\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1277,  E(|Y-Yhat|): 1.2975,  E(|Yhat-Yhat'|): 2.3397\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1676,  E(|Y-Yhat|): 0.3093,  E(|Yhat-Yhat'|): 0.2833\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1327,  E(|Y-Yhat|): 0.2630,  E(|Yhat-Yhat'|): 0.2605\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1350,  E(|Y-Yhat|): 0.2672,  E(|Yhat-Yhat'|): 0.2643\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1327,  E(|Y-Yhat|): 0.2641,  E(|Yhat-Yhat'|): 0.2628\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0430,  E(|Y-Yhat|): 0.0776,  E(|Yhat-Yhat'|): 0.0691\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3716,  E(|Y-Yhat|): 0.6294,  E(|Yhat-Yhat'|): 0.5156\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2981,  E(|Y-Yhat|): 0.5888,  E(|Yhat-Yhat'|): 0.5814\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2935,  E(|Y-Yhat|): 0.5807,  E(|Yhat-Yhat'|): 0.5743\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2952,  E(|Y-Yhat|): 0.5768,  E(|Yhat-Yhat'|): 0.5631\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0794,  E(|Y-Yhat|): 0.1522,  E(|Yhat-Yhat'|): 0.1457\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1834,  E(|Y-Yhat|): 0.3464,  E(|Yhat-Yhat'|): 0.3260\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1445,  E(|Y-Yhat|): 0.2882,  E(|Yhat-Yhat'|): 0.2874\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1433,  E(|Y-Yhat|): 0.2845,  E(|Yhat-Yhat'|): 0.2824\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1466,  E(|Y-Yhat|): 0.2849,  E(|Yhat-Yhat'|): 0.2767\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0557,  E(|Y-Yhat|): 0.1033,  E(|Yhat-Yhat'|): 0.0952\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5754,  E(|Y-Yhat|): 0.9369,  E(|Yhat-Yhat'|): 0.7231\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4539,  E(|Y-Yhat|): 0.8689,  E(|Yhat-Yhat'|): 0.8301\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4442,  E(|Y-Yhat|): 0.8726,  E(|Yhat-Yhat'|): 0.8568\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4484,  E(|Y-Yhat|): 0.8826,  E(|Yhat-Yhat'|): 0.8684\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 143.0826,  E(|Y-Yhat|): 252.7375,  E(|Yhat-Yhat'|): 219.3098\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7807,  E(|Y-Yhat|): 2.1599,  E(|Yhat-Yhat'|): 0.7583\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3828,  E(|Y-Yhat|): 14.5797,  E(|Yhat-Yhat'|): 26.3938\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4213,  E(|Y-Yhat|): 10.8882,  E(|Yhat-Yhat'|): 18.9338\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4804,  E(|Y-Yhat|): 9.9183,  E(|Yhat-Yhat'|): 16.8757\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 125.5219,  E(|Y-Yhat|): 807.8266,  E(|Yhat-Yhat'|): 1364.6095\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1574,  E(|Y-Yhat|): 0.2970,  E(|Yhat-Yhat'|): 0.2791\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1320,  E(|Y-Yhat|): 0.2646,  E(|Yhat-Yhat'|): 0.2652\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1320,  E(|Y-Yhat|): 0.2571,  E(|Yhat-Yhat'|): 0.2502\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1298,  E(|Y-Yhat|): 0.2583,  E(|Yhat-Yhat'|): 0.2570\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3702,  E(|Y-Yhat|): 0.7734,  E(|Yhat-Yhat'|): 0.8063\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3902,  E(|Y-Yhat|): 0.6607,  E(|Yhat-Yhat'|): 0.5411\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2902,  E(|Y-Yhat|): 0.5741,  E(|Yhat-Yhat'|): 0.5676\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2931,  E(|Y-Yhat|): 0.5825,  E(|Yhat-Yhat'|): 0.5788\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2880,  E(|Y-Yhat|): 0.5514,  E(|Yhat-Yhat'|): 0.5269\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.4750,  E(|Y-Yhat|): 2.9677,  E(|Yhat-Yhat'|): 2.9856\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1917,  E(|Y-Yhat|): 0.3607,  E(|Yhat-Yhat'|): 0.3379\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1445,  E(|Y-Yhat|): 0.2943,  E(|Yhat-Yhat'|): 0.2996\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1418,  E(|Y-Yhat|): 0.2798,  E(|Yhat-Yhat'|): 0.2759\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1437,  E(|Y-Yhat|): 0.2904,  E(|Yhat-Yhat'|): 0.2935\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3665,  E(|Y-Yhat|): 0.7585,  E(|Yhat-Yhat'|): 0.7841\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5633,  E(|Y-Yhat|): 0.9415,  E(|Yhat-Yhat'|): 0.7564\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4240,  E(|Y-Yhat|): 0.8184,  E(|Yhat-Yhat'|): 0.7888\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4251,  E(|Y-Yhat|): 0.8320,  E(|Yhat-Yhat'|): 0.8138\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4159,  E(|Y-Yhat|): 0.8086,  E(|Yhat-Yhat'|): 0.7854\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2510,  E(|Y-Yhat|): 0.4339,  E(|Yhat-Yhat'|): 0.3659\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8407,  E(|Y-Yhat|): 2.2144,  E(|Yhat-Yhat'|): 0.7474\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8847,  E(|Y-Yhat|): 21.3559,  E(|Yhat-Yhat'|): 38.9424\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5557,  E(|Y-Yhat|): 14.2956,  E(|Yhat-Yhat'|): 25.4799\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0379,  E(|Y-Yhat|): 79.8382,  E(|Yhat-Yhat'|): 157.6005\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1248,  E(|Y-Yhat|): 14.1898,  E(|Yhat-Yhat'|): 28.1299\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1647,  E(|Y-Yhat|): 0.3074,  E(|Yhat-Yhat'|): 0.2855\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1222,  E(|Y-Yhat|): 0.2496,  E(|Yhat-Yhat'|): 0.2547\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1203,  E(|Y-Yhat|): 0.2456,  E(|Yhat-Yhat'|): 0.2505\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1231,  E(|Y-Yhat|): 0.2425,  E(|Yhat-Yhat'|): 0.2388\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0541,  E(|Y-Yhat|): 0.1008,  E(|Yhat-Yhat'|): 0.0935\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3743,  E(|Y-Yhat|): 0.6281,  E(|Yhat-Yhat'|): 0.5078\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2654,  E(|Y-Yhat|): 0.5376,  E(|Yhat-Yhat'|): 0.5444\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2592,  E(|Y-Yhat|): 0.5174,  E(|Yhat-Yhat'|): 0.5164\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2678,  E(|Y-Yhat|): 0.5323,  E(|Yhat-Yhat'|): 0.5291\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1018,  E(|Y-Yhat|): 0.1954,  E(|Yhat-Yhat'|): 0.1872\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1758,  E(|Y-Yhat|): 0.3271,  E(|Yhat-Yhat'|): 0.3025\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1341,  E(|Y-Yhat|): 0.2638,  E(|Yhat-Yhat'|): 0.2595\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1303,  E(|Y-Yhat|): 0.2647,  E(|Yhat-Yhat'|): 0.2688\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1318,  E(|Y-Yhat|): 0.2655,  E(|Yhat-Yhat'|): 0.2674\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0653,  E(|Y-Yhat|): 0.1278,  E(|Yhat-Yhat'|): 0.1251\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5718,  E(|Y-Yhat|): 0.9005,  E(|Yhat-Yhat'|): 0.6573\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4302,  E(|Y-Yhat|): 0.8636,  E(|Yhat-Yhat'|): 0.8669\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4245,  E(|Y-Yhat|): 0.8587,  E(|Yhat-Yhat'|): 0.8684\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4253,  E(|Y-Yhat|): 0.8404,  E(|Yhat-Yhat'|): 0.8302\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 129.3153,  E(|Y-Yhat|): 234.7262,  E(|Yhat-Yhat'|): 210.8217\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8247,  E(|Y-Yhat|): 2.1577,  E(|Yhat-Yhat'|): 0.6661\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3295,  E(|Y-Yhat|): 10.1503,  E(|Yhat-Yhat'|): 17.6417\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3281,  E(|Y-Yhat|): 3.5544,  E(|Yhat-Yhat'|): 4.4525\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3260,  E(|Y-Yhat|): 4.2808,  E(|Yhat-Yhat'|): 5.9097\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 138.7574,  E(|Y-Yhat|): 431.4302,  E(|Yhat-Yhat'|): 585.3456\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1596,  E(|Y-Yhat|): 0.3008,  E(|Yhat-Yhat'|): 0.2823\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1256,  E(|Y-Yhat|): 0.2539,  E(|Yhat-Yhat'|): 0.2565\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1273,  E(|Y-Yhat|): 0.2554,  E(|Yhat-Yhat'|): 0.2564\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1279,  E(|Y-Yhat|): 0.2537,  E(|Yhat-Yhat'|): 0.2515\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3672,  E(|Y-Yhat|): 0.7686,  E(|Yhat-Yhat'|): 0.8028\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3729,  E(|Y-Yhat|): 0.6426,  E(|Yhat-Yhat'|): 0.5396\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2774,  E(|Y-Yhat|): 0.5480,  E(|Yhat-Yhat'|): 0.5412\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2732,  E(|Y-Yhat|): 0.5546,  E(|Yhat-Yhat'|): 0.5628\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2715,  E(|Y-Yhat|): 0.5486,  E(|Yhat-Yhat'|): 0.5543\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.4288,  E(|Y-Yhat|): 2.9466,  E(|Yhat-Yhat'|): 3.0355\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1802,  E(|Y-Yhat|): 0.3426,  E(|Yhat-Yhat'|): 0.3246\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1411,  E(|Y-Yhat|): 0.2782,  E(|Yhat-Yhat'|): 0.2741\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1390,  E(|Y-Yhat|): 0.2810,  E(|Yhat-Yhat'|): 0.2840\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1395,  E(|Y-Yhat|): 0.2715,  E(|Yhat-Yhat'|): 0.2640\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3547,  E(|Y-Yhat|): 0.7213,  E(|Yhat-Yhat'|): 0.7331\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5693,  E(|Y-Yhat|): 0.9262,  E(|Yhat-Yhat'|): 0.7138\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4545,  E(|Y-Yhat|): 0.8799,  E(|Yhat-Yhat'|): 0.8510\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4535,  E(|Y-Yhat|): 0.9018,  E(|Yhat-Yhat'|): 0.8966\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4505,  E(|Y-Yhat|): 0.8887,  E(|Yhat-Yhat'|): 0.8763\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7062,  E(|Y-Yhat|): 1.2619,  E(|Yhat-Yhat'|): 1.1114\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8746,  E(|Y-Yhat|): 2.1540,  E(|Yhat-Yhat'|): 0.5587\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1909,  E(|Y-Yhat|): 14.9771,  E(|Yhat-Yhat'|): 27.5723\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2202,  E(|Y-Yhat|): 17.5993,  E(|Yhat-Yhat'|): 32.7582\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3937,  E(|Y-Yhat|): 7.9470,  E(|Yhat-Yhat'|): 13.1066\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7137,  E(|Y-Yhat|): 3.7900,  E(|Yhat-Yhat'|): 6.1527\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1710,  E(|Y-Yhat|): 0.3238,  E(|Yhat-Yhat'|): 0.3056\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1340,  E(|Y-Yhat|): 0.2722,  E(|Yhat-Yhat'|): 0.2764\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1338,  E(|Y-Yhat|): 0.2696,  E(|Yhat-Yhat'|): 0.2718\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1298,  E(|Y-Yhat|): 0.2697,  E(|Yhat-Yhat'|): 0.2798\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0992,  E(|Y-Yhat|): 0.2018,  E(|Yhat-Yhat'|): 0.2052\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4233,  E(|Y-Yhat|): 0.7241,  E(|Yhat-Yhat'|): 0.6015\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2884,  E(|Y-Yhat|): 0.5823,  E(|Yhat-Yhat'|): 0.5879\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3017,  E(|Y-Yhat|): 0.5959,  E(|Yhat-Yhat'|): 0.5885\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2980,  E(|Y-Yhat|): 0.5977,  E(|Yhat-Yhat'|): 0.5993\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2063,  E(|Y-Yhat|): 0.4123,  E(|Yhat-Yhat'|): 0.4122\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1865,  E(|Y-Yhat|): 0.3589,  E(|Yhat-Yhat'|): 0.3446\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1462,  E(|Y-Yhat|): 0.2893,  E(|Yhat-Yhat'|): 0.2861\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1481,  E(|Y-Yhat|): 0.2927,  E(|Yhat-Yhat'|): 0.2891\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1464,  E(|Y-Yhat|): 0.2896,  E(|Yhat-Yhat'|): 0.2865\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1122,  E(|Y-Yhat|): 0.2222,  E(|Yhat-Yhat'|): 0.2199\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5490,  E(|Y-Yhat|): 0.9019,  E(|Yhat-Yhat'|): 0.7059\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4432,  E(|Y-Yhat|): 0.8455,  E(|Yhat-Yhat'|): 0.8046\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4492,  E(|Y-Yhat|): 0.8584,  E(|Yhat-Yhat'|): 0.8183\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4333,  E(|Y-Yhat|): 0.8621,  E(|Yhat-Yhat'|): 0.8575\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.9762,  E(|Y-Yhat|): 5.3843,  E(|Yhat-Yhat'|): 4.8162\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9162,  E(|Y-Yhat|): 2.2811,  E(|Yhat-Yhat'|): 0.7298\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3033,  E(|Y-Yhat|): 9.3399,  E(|Yhat-Yhat'|): 16.0732\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4275,  E(|Y-Yhat|): 12.4150,  E(|Yhat-Yhat'|): 21.9750\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3908,  E(|Y-Yhat|): 57.8742,  E(|Yhat-Yhat'|): 112.9668\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.8659,  E(|Y-Yhat|): 132.1084,  E(|Yhat-Yhat'|): 256.4850\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1682,  E(|Y-Yhat|): 0.3173,  E(|Yhat-Yhat'|): 0.2982\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1270,  E(|Y-Yhat|): 0.2516,  E(|Yhat-Yhat'|): 0.2492\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1300,  E(|Y-Yhat|): 0.2578,  E(|Yhat-Yhat'|): 0.2556\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1284,  E(|Y-Yhat|): 0.2544,  E(|Yhat-Yhat'|): 0.2521\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1659,  E(|Y-Yhat|): 0.3340,  E(|Yhat-Yhat'|): 0.3362\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3643,  E(|Y-Yhat|): 0.6065,  E(|Yhat-Yhat'|): 0.4844\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2798,  E(|Y-Yhat|): 0.5512,  E(|Yhat-Yhat'|): 0.5428\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2789,  E(|Y-Yhat|): 0.5573,  E(|Yhat-Yhat'|): 0.5568\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2754,  E(|Y-Yhat|): 0.5563,  E(|Yhat-Yhat'|): 0.5619\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3998,  E(|Y-Yhat|): 0.7977,  E(|Yhat-Yhat'|): 0.7959\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1875,  E(|Y-Yhat|): 0.3599,  E(|Yhat-Yhat'|): 0.3447\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1393,  E(|Y-Yhat|): 0.2818,  E(|Yhat-Yhat'|): 0.2850\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1402,  E(|Y-Yhat|): 0.2802,  E(|Yhat-Yhat'|): 0.2800\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1386,  E(|Y-Yhat|): 0.2770,  E(|Yhat-Yhat'|): 0.2769\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1779,  E(|Y-Yhat|): 0.3615,  E(|Yhat-Yhat'|): 0.3672\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5649,  E(|Y-Yhat|): 0.9127,  E(|Yhat-Yhat'|): 0.6956\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4487,  E(|Y-Yhat|): 0.8651,  E(|Yhat-Yhat'|): 0.8329\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4468,  E(|Y-Yhat|): 0.8779,  E(|Yhat-Yhat'|): 0.8622\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4350,  E(|Y-Yhat|): 0.8691,  E(|Yhat-Yhat'|): 0.8681\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 11.4855,  E(|Y-Yhat|): 21.5364,  E(|Yhat-Yhat'|): 20.1018\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9849,  E(|Y-Yhat|): 2.2574,  E(|Yhat-Yhat'|): 0.5449\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0882,  E(|Y-Yhat|): 26.2003,  E(|Yhat-Yhat'|): 50.2242\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3076,  E(|Y-Yhat|): 200.4144,  E(|Yhat-Yhat'|): 398.2136\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7229,  E(|Y-Yhat|): 95.6881,  E(|Yhat-Yhat'|): 189.9305\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 12.6426,  E(|Y-Yhat|): 810.3545,  E(|Yhat-Yhat'|): 1595.4238\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1722,  E(|Y-Yhat|): 0.3222,  E(|Yhat-Yhat'|): 0.2999\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1309,  E(|Y-Yhat|): 0.2616,  E(|Yhat-Yhat'|): 0.2613\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1291,  E(|Y-Yhat|): 0.2576,  E(|Yhat-Yhat'|): 0.2571\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1322,  E(|Y-Yhat|): 0.2600,  E(|Yhat-Yhat'|): 0.2557\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2413,  E(|Y-Yhat|): 0.4972,  E(|Yhat-Yhat'|): 0.5119\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4017,  E(|Y-Yhat|): 0.6685,  E(|Yhat-Yhat'|): 0.5335\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2906,  E(|Y-Yhat|): 0.5588,  E(|Yhat-Yhat'|): 0.5363\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2854,  E(|Y-Yhat|): 0.5655,  E(|Yhat-Yhat'|): 0.5601\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2892,  E(|Y-Yhat|): 0.5672,  E(|Yhat-Yhat'|): 0.5559\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7082,  E(|Y-Yhat|): 1.4049,  E(|Yhat-Yhat'|): 1.3935\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1889,  E(|Y-Yhat|): 0.3537,  E(|Yhat-Yhat'|): 0.3296\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1437,  E(|Y-Yhat|): 0.2857,  E(|Yhat-Yhat'|): 0.2840\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1386,  E(|Y-Yhat|): 0.2803,  E(|Yhat-Yhat'|): 0.2836\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1400,  E(|Y-Yhat|): 0.2800,  E(|Yhat-Yhat'|): 0.2799\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2478,  E(|Y-Yhat|): 0.5056,  E(|Yhat-Yhat'|): 0.5156\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"cubic\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = Z ** 3 / 3.0   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b474741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0364),\n",
       " tensor(2.4757),\n",
       " tensor(0.0656),\n",
       " tensor(0.0483),\n",
       " tensor(0.0402))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116eedf2",
   "metadata": {},
   "source": [
    "## True function: square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76b24b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6203,  E(|Y-Yhat|): 0.9981,  E(|Yhat-Yhat'|): 0.7557\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4045,  E(|Y-Yhat|): 0.8041,  E(|Yhat-Yhat'|): 0.7992\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3873,  E(|Y-Yhat|): 0.8033,  E(|Yhat-Yhat'|): 0.8321\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4031,  E(|Y-Yhat|): 0.8030,  E(|Yhat-Yhat'|): 0.7997\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 7.2799,  E(|Y-Yhat|): 13.7437,  E(|Yhat-Yhat'|): 12.9277\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7753,  E(|Y-Yhat|): 2.1410,  E(|Yhat-Yhat'|): 0.7315\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6870,  E(|Y-Yhat|): 19.3547,  E(|Yhat-Yhat'|): 35.3354\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -3.6332,  E(|Y-Yhat|): 232.7744,  E(|Yhat-Yhat'|): 472.8152\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9768,  E(|Y-Yhat|): 10.1863,  E(|Yhat-Yhat'|): 18.4190\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 7.5928,  E(|Y-Yhat|): 78.2812,  E(|Yhat-Yhat'|): 141.3768\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2093,  E(|Y-Yhat|): 0.3661,  E(|Yhat-Yhat'|): 0.3137\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1178,  E(|Y-Yhat|): 0.2397,  E(|Yhat-Yhat'|): 0.2438\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1175,  E(|Y-Yhat|): 0.2383,  E(|Yhat-Yhat'|): 0.2417\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1171,  E(|Y-Yhat|): 0.2389,  E(|Yhat-Yhat'|): 0.2437\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1823,  E(|Y-Yhat|): 0.3922,  E(|Yhat-Yhat'|): 0.4198\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3726,  E(|Y-Yhat|): 0.6202,  E(|Yhat-Yhat'|): 0.4952\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2752,  E(|Y-Yhat|): 0.5518,  E(|Yhat-Yhat'|): 0.5532\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2695,  E(|Y-Yhat|): 0.5379,  E(|Yhat-Yhat'|): 0.5368\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2722,  E(|Y-Yhat|): 0.5309,  E(|Yhat-Yhat'|): 0.5174\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5301,  E(|Y-Yhat|): 1.0503,  E(|Yhat-Yhat'|): 1.0403\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2032,  E(|Y-Yhat|): 0.3853,  E(|Yhat-Yhat'|): 0.3642\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1226,  E(|Y-Yhat|): 0.2463,  E(|Yhat-Yhat'|): 0.2474\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1240,  E(|Y-Yhat|): 0.2530,  E(|Yhat-Yhat'|): 0.2580\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1215,  E(|Y-Yhat|): 0.2504,  E(|Yhat-Yhat'|): 0.2579\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1837,  E(|Y-Yhat|): 0.3811,  E(|Yhat-Yhat'|): 0.3949\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6152,  E(|Y-Yhat|): 1.0190,  E(|Yhat-Yhat'|): 0.8075\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3945,  E(|Y-Yhat|): 0.7880,  E(|Yhat-Yhat'|): 0.7869\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3958,  E(|Y-Yhat|): 0.7982,  E(|Yhat-Yhat'|): 0.8048\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4048,  E(|Y-Yhat|): 0.7922,  E(|Yhat-Yhat'|): 0.7749\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0242,  E(|Y-Yhat|): 0.0460,  E(|Yhat-Yhat'|): 0.0436\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8776,  E(|Y-Yhat|): 2.1896,  E(|Yhat-Yhat'|): 0.6240\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2662,  E(|Y-Yhat|): 5.8807,  E(|Yhat-Yhat'|): 9.2288\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7195,  E(|Y-Yhat|): 22.3626,  E(|Yhat-Yhat'|): 43.2860\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2837,  E(|Y-Yhat|): 22.6101,  E(|Yhat-Yhat'|): 42.6527\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0309,  E(|Y-Yhat|): 0.4808,  E(|Yhat-Yhat'|): 0.8998\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1781,  E(|Y-Yhat|): 0.3344,  E(|Yhat-Yhat'|): 0.3125\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1145,  E(|Y-Yhat|): 0.2371,  E(|Yhat-Yhat'|): 0.2451\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1205,  E(|Y-Yhat|): 0.2430,  E(|Yhat-Yhat'|): 0.2450\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1210,  E(|Y-Yhat|): 0.2406,  E(|Yhat-Yhat'|): 0.2392\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0104,  E(|Y-Yhat|): 0.0203,  E(|Yhat-Yhat'|): 0.0198\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3763,  E(|Y-Yhat|): 0.6050,  E(|Yhat-Yhat'|): 0.4575\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2693,  E(|Y-Yhat|): 0.5433,  E(|Yhat-Yhat'|): 0.5480\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2696,  E(|Y-Yhat|): 0.5487,  E(|Yhat-Yhat'|): 0.5582\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2672,  E(|Y-Yhat|): 0.5278,  E(|Yhat-Yhat'|): 0.5212\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0206,  E(|Y-Yhat|): 0.0374,  E(|Yhat-Yhat'|): 0.0336\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1788,  E(|Y-Yhat|): 0.3293,  E(|Yhat-Yhat'|): 0.3010\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1247,  E(|Y-Yhat|): 0.2557,  E(|Yhat-Yhat'|): 0.2619\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1230,  E(|Y-Yhat|): 0.2503,  E(|Yhat-Yhat'|): 0.2546\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1214,  E(|Y-Yhat|): 0.2518,  E(|Yhat-Yhat'|): 0.2608\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0174,  E(|Y-Yhat|): 0.0338,  E(|Yhat-Yhat'|): 0.0328\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6054,  E(|Y-Yhat|): 0.9783,  E(|Yhat-Yhat'|): 0.7457\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3918,  E(|Y-Yhat|): 0.7655,  E(|Yhat-Yhat'|): 0.7474\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3830,  E(|Y-Yhat|): 0.7758,  E(|Yhat-Yhat'|): 0.7856\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3834,  E(|Y-Yhat|): 0.7707,  E(|Yhat-Yhat'|): 0.7747\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0158,  E(|Y-Yhat|): 0.0307,  E(|Yhat-Yhat'|): 0.0298\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0358,  E(|Y-Yhat|): 2.3549,  E(|Yhat-Yhat'|): 0.6381\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9345,  E(|Y-Yhat|): 23.6700,  E(|Yhat-Yhat'|): 45.4710\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.1767,  E(|Y-Yhat|): 1890.7626,  E(|Yhat-Yhat'|): 3777.1718\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 13.4802,  E(|Y-Yhat|): 1498.0256,  E(|Yhat-Yhat'|): 2969.0908\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4216,  E(|Y-Yhat|): 20.7930,  E(|Yhat-Yhat'|): 40.7429\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1919,  E(|Y-Yhat|): 0.3555,  E(|Yhat-Yhat'|): 0.3271\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1143,  E(|Y-Yhat|): 0.2372,  E(|Yhat-Yhat'|): 0.2458\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1169,  E(|Y-Yhat|): 0.2354,  E(|Yhat-Yhat'|): 0.2369\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1147,  E(|Y-Yhat|): 0.2372,  E(|Yhat-Yhat'|): 0.2451\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0065,  E(|Y-Yhat|): 0.0137,  E(|Yhat-Yhat'|): 0.0143\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3792,  E(|Y-Yhat|): 0.6072,  E(|Yhat-Yhat'|): 0.4561\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2547,  E(|Y-Yhat|): 0.5267,  E(|Yhat-Yhat'|): 0.5439\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2608,  E(|Y-Yhat|): 0.5329,  E(|Yhat-Yhat'|): 0.5441\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2619,  E(|Y-Yhat|): 0.5107,  E(|Yhat-Yhat'|): 0.4975\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0129,  E(|Y-Yhat|): 0.0254,  E(|Yhat-Yhat'|): 0.0250\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2246,  E(|Y-Yhat|): 0.4207,  E(|Yhat-Yhat'|): 0.3922\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1245,  E(|Y-Yhat|): 0.2533,  E(|Yhat-Yhat'|): 0.2575\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1226,  E(|Y-Yhat|): 0.2501,  E(|Yhat-Yhat'|): 0.2550\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1231,  E(|Y-Yhat|): 0.2566,  E(|Yhat-Yhat'|): 0.2669\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0119,  E(|Y-Yhat|): 0.0242,  E(|Yhat-Yhat'|): 0.0247\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6172,  E(|Y-Yhat|): 1.0024,  E(|Yhat-Yhat'|): 0.7703\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3965,  E(|Y-Yhat|): 0.7909,  E(|Yhat-Yhat'|): 0.7888\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4019,  E(|Y-Yhat|): 0.8142,  E(|Yhat-Yhat'|): 0.8245\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4033,  E(|Y-Yhat|): 0.7938,  E(|Yhat-Yhat'|): 0.7811\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0446,  E(|Y-Yhat|): 0.0880,  E(|Yhat-Yhat'|): 0.0869\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9099,  E(|Y-Yhat|): 2.2416,  E(|Yhat-Yhat'|): 0.6634\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2881,  E(|Y-Yhat|): 4.3626,  E(|Yhat-Yhat'|): 6.1490\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2872,  E(|Y-Yhat|): 32.9488,  E(|Yhat-Yhat'|): 63.3233\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9126,  E(|Y-Yhat|): 57.2775,  E(|Yhat-Yhat'|): 112.7298\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0607,  E(|Y-Yhat|): 2.1351,  E(|Yhat-Yhat'|): 4.1487\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1796,  E(|Y-Yhat|): 0.3329,  E(|Yhat-Yhat'|): 0.3065\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1183,  E(|Y-Yhat|): 0.2436,  E(|Yhat-Yhat'|): 0.2507\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1184,  E(|Y-Yhat|): 0.2387,  E(|Yhat-Yhat'|): 0.2406\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1153,  E(|Y-Yhat|): 0.2307,  E(|Yhat-Yhat'|): 0.2307\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0178,  E(|Y-Yhat|): 0.0323,  E(|Yhat-Yhat'|): 0.0290\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3784,  E(|Y-Yhat|): 0.6433,  E(|Yhat-Yhat'|): 0.5300\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2689,  E(|Y-Yhat|): 0.5416,  E(|Yhat-Yhat'|): 0.5455\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2619,  E(|Y-Yhat|): 0.5376,  E(|Yhat-Yhat'|): 0.5515\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2613,  E(|Y-Yhat|): 0.5285,  E(|Yhat-Yhat'|): 0.5344\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0307,  E(|Y-Yhat|): 0.0637,  E(|Yhat-Yhat'|): 0.0661\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1772,  E(|Y-Yhat|): 0.3361,  E(|Yhat-Yhat'|): 0.3178\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1231,  E(|Y-Yhat|): 0.2511,  E(|Yhat-Yhat'|): 0.2560\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1227,  E(|Y-Yhat|): 0.2505,  E(|Yhat-Yhat'|): 0.2556\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1226,  E(|Y-Yhat|): 0.2434,  E(|Yhat-Yhat'|): 0.2416\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0260,  E(|Y-Yhat|): 0.0479,  E(|Yhat-Yhat'|): 0.0437\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6550,  E(|Y-Yhat|): 1.0932,  E(|Yhat-Yhat'|): 0.8765\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3947,  E(|Y-Yhat|): 0.8027,  E(|Yhat-Yhat'|): 0.8160\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3996,  E(|Y-Yhat|): 0.7967,  E(|Yhat-Yhat'|): 0.7942\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3890,  E(|Y-Yhat|): 0.8006,  E(|Yhat-Yhat'|): 0.8233\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 9.0395,  E(|Y-Yhat|): 17.6112,  E(|Yhat-Yhat'|): 17.1432\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8117,  E(|Y-Yhat|): 2.2159,  E(|Yhat-Yhat'|): 0.8084\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0804,  E(|Y-Yhat|): 13.7167,  E(|Yhat-Yhat'|): 25.2725\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 27.6183,  E(|Y-Yhat|): 1858.9545,  E(|Yhat-Yhat'|): 3662.6724\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 8.9199,  E(|Y-Yhat|): 723.8940,  E(|Yhat-Yhat'|): 1429.9482\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 94.1606,  E(|Y-Yhat|): 4824.1616,  E(|Yhat-Yhat'|): 9460.0020\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1728,  E(|Y-Yhat|): 0.3163,  E(|Yhat-Yhat'|): 0.2871\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1158,  E(|Y-Yhat|): 0.2352,  E(|Yhat-Yhat'|): 0.2388\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1156,  E(|Y-Yhat|): 0.2393,  E(|Yhat-Yhat'|): 0.2475\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1193,  E(|Y-Yhat|): 0.2331,  E(|Yhat-Yhat'|): 0.2275\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1858,  E(|Y-Yhat|): 0.3903,  E(|Yhat-Yhat'|): 0.4090\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3977,  E(|Y-Yhat|): 0.6563,  E(|Yhat-Yhat'|): 0.5171\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2679,  E(|Y-Yhat|): 0.5331,  E(|Yhat-Yhat'|): 0.5303\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2682,  E(|Y-Yhat|): 0.5397,  E(|Yhat-Yhat'|): 0.5431\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2687,  E(|Y-Yhat|): 0.5452,  E(|Yhat-Yhat'|): 0.5529\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5747,  E(|Y-Yhat|): 1.1532,  E(|Yhat-Yhat'|): 1.1570\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2325,  E(|Y-Yhat|): 0.4330,  E(|Yhat-Yhat'|): 0.4010\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1253,  E(|Y-Yhat|): 0.2512,  E(|Yhat-Yhat'|): 0.2517\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1246,  E(|Y-Yhat|): 0.2517,  E(|Yhat-Yhat'|): 0.2541\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1230,  E(|Y-Yhat|): 0.2499,  E(|Yhat-Yhat'|): 0.2538\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1888,  E(|Y-Yhat|): 0.3876,  E(|Yhat-Yhat'|): 0.3976\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6425,  E(|Y-Yhat|): 1.0996,  E(|Yhat-Yhat'|): 0.9143\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3923,  E(|Y-Yhat|): 0.7759,  E(|Yhat-Yhat'|): 0.7673\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3903,  E(|Y-Yhat|): 0.7627,  E(|Yhat-Yhat'|): 0.7446\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3882,  E(|Y-Yhat|): 0.7778,  E(|Yhat-Yhat'|): 0.7791\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0704,  E(|Y-Yhat|): 0.1360,  E(|Yhat-Yhat'|): 0.1311\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8899,  E(|Y-Yhat|): 2.2502,  E(|Yhat-Yhat'|): 0.7206\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.5721,  E(|Y-Yhat|): 19.2169,  E(|Yhat-Yhat'|): 35.2895\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2609,  E(|Y-Yhat|): 5.8343,  E(|Yhat-Yhat'|): 9.1469\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.1849,  E(|Y-Yhat|): 17.2395,  E(|Yhat-Yhat'|): 32.1092\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0825,  E(|Y-Yhat|): 0.9938,  E(|Yhat-Yhat'|): 1.8226\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1817,  E(|Y-Yhat|): 0.3394,  E(|Yhat-Yhat'|): 0.3155\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1153,  E(|Y-Yhat|): 0.2352,  E(|Yhat-Yhat'|): 0.2398\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1134,  E(|Y-Yhat|): 0.2296,  E(|Yhat-Yhat'|): 0.2324\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1142,  E(|Y-Yhat|): 0.2332,  E(|Yhat-Yhat'|): 0.2380\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0248,  E(|Y-Yhat|): 0.0485,  E(|Yhat-Yhat'|): 0.0473\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4294,  E(|Y-Yhat|): 0.7149,  E(|Yhat-Yhat'|): 0.5710\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2597,  E(|Y-Yhat|): 0.5295,  E(|Yhat-Yhat'|): 0.5396\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2599,  E(|Y-Yhat|): 0.5197,  E(|Yhat-Yhat'|): 0.5197\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2505,  E(|Y-Yhat|): 0.5117,  E(|Yhat-Yhat'|): 0.5225\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0439,  E(|Y-Yhat|): 0.0888,  E(|Yhat-Yhat'|): 0.0900\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2237,  E(|Y-Yhat|): 0.4098,  E(|Yhat-Yhat'|): 0.3723\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1197,  E(|Y-Yhat|): 0.2447,  E(|Yhat-Yhat'|): 0.2501\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1229,  E(|Y-Yhat|): 0.2511,  E(|Yhat-Yhat'|): 0.2564\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1222,  E(|Y-Yhat|): 0.2374,  E(|Yhat-Yhat'|): 0.2305\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0335,  E(|Y-Yhat|): 0.0645,  E(|Yhat-Yhat'|): 0.0621\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6261,  E(|Y-Yhat|): 1.0120,  E(|Yhat-Yhat'|): 0.7719\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3913,  E(|Y-Yhat|): 0.7846,  E(|Yhat-Yhat'|): 0.7866\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3808,  E(|Y-Yhat|): 0.7657,  E(|Yhat-Yhat'|): 0.7698\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4001,  E(|Y-Yhat|): 0.7770,  E(|Yhat-Yhat'|): 0.7538\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.9774,  E(|Y-Yhat|): 9.3413,  E(|Yhat-Yhat'|): 8.7278\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8557,  E(|Y-Yhat|): 2.1625,  E(|Yhat-Yhat'|): 0.6135\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1901,  E(|Y-Yhat|): 6.3494,  E(|Yhat-Yhat'|): 10.3185\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3208,  E(|Y-Yhat|): 5.5224,  E(|Yhat-Yhat'|): 8.4031\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2909,  E(|Y-Yhat|): 3.0381,  E(|Yhat-Yhat'|): 3.4944\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 5.0158,  E(|Y-Yhat|): 11.9764,  E(|Yhat-Yhat'|): 13.9213\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1943,  E(|Y-Yhat|): 0.3530,  E(|Yhat-Yhat'|): 0.3175\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1141,  E(|Y-Yhat|): 0.2345,  E(|Yhat-Yhat'|): 0.2408\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1175,  E(|Y-Yhat|): 0.2322,  E(|Yhat-Yhat'|): 0.2293\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1173,  E(|Y-Yhat|): 0.2389,  E(|Yhat-Yhat'|): 0.2431\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1673,  E(|Y-Yhat|): 0.3585,  E(|Yhat-Yhat'|): 0.3824\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4496,  E(|Y-Yhat|): 0.7581,  E(|Yhat-Yhat'|): 0.6170\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2641,  E(|Y-Yhat|): 0.5367,  E(|Yhat-Yhat'|): 0.5452\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2610,  E(|Y-Yhat|): 0.5220,  E(|Yhat-Yhat'|): 0.5219\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2581,  E(|Y-Yhat|): 0.5285,  E(|Yhat-Yhat'|): 0.5407\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4612,  E(|Y-Yhat|): 0.9359,  E(|Yhat-Yhat'|): 0.9495\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2138,  E(|Y-Yhat|): 0.4051,  E(|Yhat-Yhat'|): 0.3825\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1186,  E(|Y-Yhat|): 0.2482,  E(|Yhat-Yhat'|): 0.2592\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1191,  E(|Y-Yhat|): 0.2465,  E(|Yhat-Yhat'|): 0.2548\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1181,  E(|Y-Yhat|): 0.2391,  E(|Yhat-Yhat'|): 0.2420\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1649,  E(|Y-Yhat|): 0.3435,  E(|Yhat-Yhat'|): 0.3571\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6132,  E(|Y-Yhat|): 1.0069,  E(|Yhat-Yhat'|): 0.7874\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4127,  E(|Y-Yhat|): 0.8006,  E(|Yhat-Yhat'|): 0.7758\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4016,  E(|Y-Yhat|): 0.8313,  E(|Yhat-Yhat'|): 0.8594\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4020,  E(|Y-Yhat|): 0.8384,  E(|Yhat-Yhat'|): 0.8728\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2101,  E(|Y-Yhat|): 0.4369,  E(|Yhat-Yhat'|): 0.4536\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8755,  E(|Y-Yhat|): 2.1562,  E(|Yhat-Yhat'|): 0.5612\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3678,  E(|Y-Yhat|): 6.6784,  E(|Yhat-Yhat'|): 10.6213\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2906,  E(|Y-Yhat|): 5.4427,  E(|Yhat-Yhat'|): 8.3043\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3576,  E(|Y-Yhat|): 5.8234,  E(|Yhat-Yhat'|): 8.9317\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2086,  E(|Y-Yhat|): 0.8400,  E(|Yhat-Yhat'|): 1.2629\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1642,  E(|Y-Yhat|): 0.3066,  E(|Yhat-Yhat'|): 0.2847\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1193,  E(|Y-Yhat|): 0.2489,  E(|Yhat-Yhat'|): 0.2592\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1188,  E(|Y-Yhat|): 0.2454,  E(|Yhat-Yhat'|): 0.2531\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1192,  E(|Y-Yhat|): 0.2425,  E(|Yhat-Yhat'|): 0.2466\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0527,  E(|Y-Yhat|): 0.1030,  E(|Yhat-Yhat'|): 0.1007\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4461,  E(|Y-Yhat|): 0.7653,  E(|Yhat-Yhat'|): 0.6383\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2736,  E(|Y-Yhat|): 0.5710,  E(|Yhat-Yhat'|): 0.5948\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2733,  E(|Y-Yhat|): 0.5571,  E(|Yhat-Yhat'|): 0.5676\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2706,  E(|Y-Yhat|): 0.5516,  E(|Yhat-Yhat'|): 0.5622\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0949,  E(|Y-Yhat|): 0.1912,  E(|Yhat-Yhat'|): 0.1925\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1896,  E(|Y-Yhat|): 0.3602,  E(|Yhat-Yhat'|): 0.3412\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1268,  E(|Y-Yhat|): 0.2525,  E(|Yhat-Yhat'|): 0.2513\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1207,  E(|Y-Yhat|): 0.2493,  E(|Yhat-Yhat'|): 0.2573\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1243,  E(|Y-Yhat|): 0.2517,  E(|Yhat-Yhat'|): 0.2550\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0609,  E(|Y-Yhat|): 0.1225,  E(|Yhat-Yhat'|): 0.1232\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5782,  E(|Y-Yhat|): 0.9199,  E(|Yhat-Yhat'|): 0.6834\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3916,  E(|Y-Yhat|): 0.7717,  E(|Yhat-Yhat'|): 0.7604\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4009,  E(|Y-Yhat|): 0.7767,  E(|Yhat-Yhat'|): 0.7516\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3956,  E(|Y-Yhat|): 0.7821,  E(|Yhat-Yhat'|): 0.7731\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3170,  E(|Y-Yhat|): 0.6492,  E(|Yhat-Yhat'|): 0.6645\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9210,  E(|Y-Yhat|): 2.2526,  E(|Yhat-Yhat'|): 0.6632\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4977,  E(|Y-Yhat|): 7.4776,  E(|Yhat-Yhat'|): 11.9597\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3033,  E(|Y-Yhat|): 5.8693,  E(|Yhat-Yhat'|): 9.1321\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3948,  E(|Y-Yhat|): 4.6601,  E(|Yhat-Yhat'|): 6.5306\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3220,  E(|Y-Yhat|): 1.1122,  E(|Yhat-Yhat'|): 1.5804\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1632,  E(|Y-Yhat|): 0.3011,  E(|Yhat-Yhat'|): 0.2759\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1169,  E(|Y-Yhat|): 0.2433,  E(|Yhat-Yhat'|): 0.2527\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1162,  E(|Y-Yhat|): 0.2412,  E(|Yhat-Yhat'|): 0.2499\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1174,  E(|Y-Yhat|): 0.2350,  E(|Yhat-Yhat'|): 0.2351\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0633,  E(|Y-Yhat|): 0.1285,  E(|Yhat-Yhat'|): 0.1303\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4084,  E(|Y-Yhat|): 0.6658,  E(|Yhat-Yhat'|): 0.5149\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2690,  E(|Y-Yhat|): 0.5337,  E(|Yhat-Yhat'|): 0.5294\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2605,  E(|Y-Yhat|): 0.5248,  E(|Yhat-Yhat'|): 0.5286\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2645,  E(|Y-Yhat|): 0.5389,  E(|Yhat-Yhat'|): 0.5487\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1236,  E(|Y-Yhat|): 0.2533,  E(|Yhat-Yhat'|): 0.2593\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2112,  E(|Y-Yhat|): 0.4040,  E(|Yhat-Yhat'|): 0.3856\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1232,  E(|Y-Yhat|): 0.2479,  E(|Yhat-Yhat'|): 0.2494\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1220,  E(|Y-Yhat|): 0.2481,  E(|Yhat-Yhat'|): 0.2522\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1248,  E(|Y-Yhat|): 0.2511,  E(|Yhat-Yhat'|): 0.2525\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0731,  E(|Y-Yhat|): 0.1457,  E(|Yhat-Yhat'|): 0.1452\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6194,  E(|Y-Yhat|): 1.0270,  E(|Yhat-Yhat'|): 0.8153\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4129,  E(|Y-Yhat|): 0.8050,  E(|Yhat-Yhat'|): 0.7842\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4020,  E(|Y-Yhat|): 0.8082,  E(|Yhat-Yhat'|): 0.8125\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3983,  E(|Y-Yhat|): 0.7937,  E(|Yhat-Yhat'|): 0.7907\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5922,  E(|Y-Yhat|): 1.1302,  E(|Yhat-Yhat'|): 1.0759\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9528,  E(|Y-Yhat|): 2.2387,  E(|Yhat-Yhat'|): 0.5718\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.7446,  E(|Y-Yhat|): 42.6202,  E(|Yhat-Yhat'|): 81.7513\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.1412,  E(|Y-Yhat|): 17.8440,  E(|Yhat-Yhat'|): 33.4055\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4288,  E(|Y-Yhat|): 5.3819,  E(|Yhat-Yhat'|): 7.9061\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5695,  E(|Y-Yhat|): 2.3093,  E(|Yhat-Yhat'|): 3.4797\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2132,  E(|Y-Yhat|): 0.3829,  E(|Yhat-Yhat'|): 0.3394\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1200,  E(|Y-Yhat|): 0.2404,  E(|Yhat-Yhat'|): 0.2408\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1137,  E(|Y-Yhat|): 0.2372,  E(|Yhat-Yhat'|): 0.2469\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1162,  E(|Y-Yhat|): 0.2332,  E(|Yhat-Yhat'|): 0.2340\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0851,  E(|Y-Yhat|): 0.1768,  E(|Yhat-Yhat'|): 0.1833\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4211,  E(|Y-Yhat|): 0.7054,  E(|Yhat-Yhat'|): 0.5686\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2707,  E(|Y-Yhat|): 0.5239,  E(|Yhat-Yhat'|): 0.5063\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2702,  E(|Y-Yhat|): 0.5424,  E(|Yhat-Yhat'|): 0.5443\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2629,  E(|Y-Yhat|): 0.5286,  E(|Yhat-Yhat'|): 0.5314\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1806,  E(|Y-Yhat|): 0.3501,  E(|Yhat-Yhat'|): 0.3391\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2179,  E(|Y-Yhat|): 0.4115,  E(|Yhat-Yhat'|): 0.3872\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1223,  E(|Y-Yhat|): 0.2545,  E(|Yhat-Yhat'|): 0.2646\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1220,  E(|Y-Yhat|): 0.2469,  E(|Yhat-Yhat'|): 0.2496\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1197,  E(|Y-Yhat|): 0.2463,  E(|Yhat-Yhat'|): 0.2531\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0952,  E(|Y-Yhat|): 0.1925,  E(|Yhat-Yhat'|): 0.1946\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"square\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = (F.relu(Z))**2 / 2.0\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6aafdeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0029),\n",
       " tensor(0.5823),\n",
       " tensor(0.0055),\n",
       " tensor(0.0039),\n",
       " tensor(0.0027))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce0cd6",
   "metadata": {},
   "source": [
    "## True function: log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3c7b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7989,  E(|Y-Yhat|): 1.3761,  E(|Yhat-Yhat'|): 1.1545\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5222,  E(|Y-Yhat|): 1.0686,  E(|Yhat-Yhat'|): 1.0929\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5151,  E(|Y-Yhat|): 1.0574,  E(|Yhat-Yhat'|): 1.0846\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5117,  E(|Y-Yhat|): 1.0437,  E(|Yhat-Yhat'|): 1.0639\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6608,  E(|Y-Yhat|): 1.3340,  E(|Yhat-Yhat'|): 1.3464\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5990,  E(|Y-Yhat|): 1.9974,  E(|Yhat-Yhat'|): 0.7967\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3285,  E(|Y-Yhat|): 31.8511,  E(|Yhat-Yhat'|): 61.0453\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -3.6040,  E(|Y-Yhat|): 481.6953,  E(|Yhat-Yhat'|): 970.5986\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 6.2478,  E(|Y-Yhat|): 604.8654,  E(|Yhat-Yhat'|): 1197.2351\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -1.0905,  E(|Y-Yhat|): 477.5324,  E(|Yhat-Yhat'|): 957.2457\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3091,  E(|Y-Yhat|): 0.5377,  E(|Yhat-Yhat'|): 0.4571\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1956,  E(|Y-Yhat|): 0.3964,  E(|Yhat-Yhat'|): 0.4017\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1997,  E(|Y-Yhat|): 0.3981,  E(|Yhat-Yhat'|): 0.3969\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1987,  E(|Y-Yhat|): 0.3975,  E(|Yhat-Yhat'|): 0.3975\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1738,  E(|Y-Yhat|): 0.3437,  E(|Yhat-Yhat'|): 0.3399\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5783,  E(|Y-Yhat|): 0.9980,  E(|Yhat-Yhat'|): 0.8394\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3631,  E(|Y-Yhat|): 0.7458,  E(|Yhat-Yhat'|): 0.7653\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3612,  E(|Y-Yhat|): 0.7420,  E(|Yhat-Yhat'|): 0.7615\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3708,  E(|Y-Yhat|): 0.7473,  E(|Yhat-Yhat'|): 0.7530\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3167,  E(|Y-Yhat|): 0.6392,  E(|Yhat-Yhat'|): 0.6451\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3057,  E(|Y-Yhat|): 0.5593,  E(|Yhat-Yhat'|): 0.5070\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2117,  E(|Y-Yhat|): 0.4303,  E(|Yhat-Yhat'|): 0.4371\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2143,  E(|Y-Yhat|): 0.4368,  E(|Yhat-Yhat'|): 0.4451\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2178,  E(|Y-Yhat|): 0.4286,  E(|Yhat-Yhat'|): 0.4216\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1909,  E(|Y-Yhat|): 0.3865,  E(|Yhat-Yhat'|): 0.3913\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7620,  E(|Y-Yhat|): 1.3250,  E(|Yhat-Yhat'|): 1.1260\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5246,  E(|Y-Yhat|): 1.0891,  E(|Yhat-Yhat'|): 1.1288\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5264,  E(|Y-Yhat|): 1.0520,  E(|Yhat-Yhat'|): 1.0511\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5312,  E(|Y-Yhat|): 1.0533,  E(|Yhat-Yhat'|): 1.0443\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0109,  E(|Y-Yhat|): 0.0223,  E(|Yhat-Yhat'|): 0.0229\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7699,  E(|Y-Yhat|): 2.0771,  E(|Yhat-Yhat'|): 0.6144\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8948,  E(|Y-Yhat|): 10.7388,  E(|Yhat-Yhat'|): 19.6879\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9955,  E(|Y-Yhat|): 20.0186,  E(|Yhat-Yhat'|): 38.0461\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9563,  E(|Y-Yhat|): 27.9764,  E(|Yhat-Yhat'|): 54.0402\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0136,  E(|Y-Yhat|): 0.3532,  E(|Yhat-Yhat'|): 0.6793\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2893,  E(|Y-Yhat|): 0.5275,  E(|Yhat-Yhat'|): 0.4764\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1986,  E(|Y-Yhat|): 0.4025,  E(|Yhat-Yhat'|): 0.4078\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1939,  E(|Y-Yhat|): 0.3917,  E(|Yhat-Yhat'|): 0.3958\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2015,  E(|Y-Yhat|): 0.3942,  E(|Yhat-Yhat'|): 0.3854\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0054,  E(|Y-Yhat|): 0.0109,  E(|Yhat-Yhat'|): 0.0110\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5769,  E(|Y-Yhat|): 0.9975,  E(|Yhat-Yhat'|): 0.8413\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3705,  E(|Y-Yhat|): 0.7588,  E(|Yhat-Yhat'|): 0.7766\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3770,  E(|Y-Yhat|): 0.7548,  E(|Yhat-Yhat'|): 0.7555\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3788,  E(|Y-Yhat|): 0.7479,  E(|Yhat-Yhat'|): 0.7382\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0107,  E(|Y-Yhat|): 0.0216,  E(|Yhat-Yhat'|): 0.0218\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3043,  E(|Y-Yhat|): 0.5533,  E(|Yhat-Yhat'|): 0.4980\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2117,  E(|Y-Yhat|): 0.4337,  E(|Yhat-Yhat'|): 0.4439\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2160,  E(|Y-Yhat|): 0.4375,  E(|Yhat-Yhat'|): 0.4430\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2180,  E(|Y-Yhat|): 0.4389,  E(|Yhat-Yhat'|): 0.4418\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0103,  E(|Y-Yhat|): 0.0216,  E(|Yhat-Yhat'|): 0.0227\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7689,  E(|Y-Yhat|): 1.3286,  E(|Yhat-Yhat'|): 1.1195\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5225,  E(|Y-Yhat|): 1.0676,  E(|Yhat-Yhat'|): 1.0902\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5201,  E(|Y-Yhat|): 1.0364,  E(|Yhat-Yhat'|): 1.0327\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5246,  E(|Y-Yhat|): 1.0405,  E(|Yhat-Yhat'|): 1.0319\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0175,  E(|Y-Yhat|): 0.0348,  E(|Yhat-Yhat'|): 0.0347\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9368,  E(|Y-Yhat|): 2.2518,  E(|Yhat-Yhat'|): 0.6301\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8115,  E(|Y-Yhat|): 11.3800,  E(|Yhat-Yhat'|): 21.1371\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0242,  E(|Y-Yhat|): 121.1744,  E(|Yhat-Yhat'|): 238.3004\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -0.6444,  E(|Y-Yhat|): 175.5097,  E(|Yhat-Yhat'|): 352.3083\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0565,  E(|Y-Yhat|): 3.9060,  E(|Yhat-Yhat'|): 7.6991\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3036,  E(|Y-Yhat|): 0.5443,  E(|Yhat-Yhat'|): 0.4813\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1965,  E(|Y-Yhat|): 0.3940,  E(|Yhat-Yhat'|): 0.3951\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1977,  E(|Y-Yhat|): 0.3945,  E(|Yhat-Yhat'|): 0.3935\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1952,  E(|Y-Yhat|): 0.3978,  E(|Yhat-Yhat'|): 0.4051\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0089,  E(|Y-Yhat|): 0.0186,  E(|Yhat-Yhat'|): 0.0194\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6125,  E(|Y-Yhat|): 1.0027,  E(|Yhat-Yhat'|): 0.7804\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3692,  E(|Y-Yhat|): 0.7570,  E(|Yhat-Yhat'|): 0.7755\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3688,  E(|Y-Yhat|): 0.7487,  E(|Yhat-Yhat'|): 0.7600\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3774,  E(|Y-Yhat|): 0.7506,  E(|Yhat-Yhat'|): 0.7463\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0166,  E(|Y-Yhat|): 0.0329,  E(|Yhat-Yhat'|): 0.0326\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3402,  E(|Y-Yhat|): 0.5850,  E(|Yhat-Yhat'|): 0.4897\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2109,  E(|Y-Yhat|): 0.4258,  E(|Yhat-Yhat'|): 0.4298\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2165,  E(|Y-Yhat|): 0.4319,  E(|Yhat-Yhat'|): 0.4309\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2150,  E(|Y-Yhat|): 0.4328,  E(|Yhat-Yhat'|): 0.4356\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0156,  E(|Y-Yhat|): 0.0329,  E(|Yhat-Yhat'|): 0.0345\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8280,  E(|Y-Yhat|): 1.3647,  E(|Yhat-Yhat'|): 1.0734\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5266,  E(|Y-Yhat|): 1.0635,  E(|Yhat-Yhat'|): 1.0738\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5293,  E(|Y-Yhat|): 1.0461,  E(|Yhat-Yhat'|): 1.0337\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5207,  E(|Y-Yhat|): 1.0614,  E(|Yhat-Yhat'|): 1.0813\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0144,  E(|Y-Yhat|): 0.0291,  E(|Yhat-Yhat'|): 0.0295\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8052,  E(|Y-Yhat|): 2.1419,  E(|Yhat-Yhat'|): 0.6733\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1060,  E(|Y-Yhat|): 26.4818,  E(|Yhat-Yhat'|): 50.7517\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -479.7786,  E(|Y-Yhat|): 35888.2995,  E(|Yhat-Yhat'|): 72736.1562\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -939.9848,  E(|Y-Yhat|): 46968.9262,  E(|Yhat-Yhat'|): 95817.8220\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 7.4184,  E(|Y-Yhat|): 831.2773,  E(|Yhat-Yhat'|): 1647.7178\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3126,  E(|Y-Yhat|): 0.5314,  E(|Yhat-Yhat'|): 0.4376\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2011,  E(|Y-Yhat|): 0.4059,  E(|Yhat-Yhat'|): 0.4095\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2012,  E(|Y-Yhat|): 0.3967,  E(|Yhat-Yhat'|): 0.3910\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1974,  E(|Y-Yhat|): 0.3970,  E(|Yhat-Yhat'|): 0.3992\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0070,  E(|Y-Yhat|): 0.0140,  E(|Yhat-Yhat'|): 0.0142\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6088,  E(|Y-Yhat|): 1.0222,  E(|Yhat-Yhat'|): 0.8267\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3713,  E(|Y-Yhat|): 0.7618,  E(|Yhat-Yhat'|): 0.7810\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3751,  E(|Y-Yhat|): 0.7506,  E(|Yhat-Yhat'|): 0.7511\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3703,  E(|Y-Yhat|): 0.7466,  E(|Yhat-Yhat'|): 0.7526\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0140,  E(|Y-Yhat|): 0.0283,  E(|Yhat-Yhat'|): 0.0286\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3067,  E(|Y-Yhat|): 0.5714,  E(|Yhat-Yhat'|): 0.5293\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2172,  E(|Y-Yhat|): 0.4306,  E(|Yhat-Yhat'|): 0.4266\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2180,  E(|Y-Yhat|): 0.4375,  E(|Yhat-Yhat'|): 0.4391\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2147,  E(|Y-Yhat|): 0.4342,  E(|Yhat-Yhat'|): 0.4391\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0132,  E(|Y-Yhat|): 0.0275,  E(|Yhat-Yhat'|): 0.0285\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7851,  E(|Y-Yhat|): 1.3529,  E(|Yhat-Yhat'|): 1.1357\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5273,  E(|Y-Yhat|): 1.0640,  E(|Yhat-Yhat'|): 1.0734\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5277,  E(|Y-Yhat|): 1.0546,  E(|Yhat-Yhat'|): 1.0539\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5194,  E(|Y-Yhat|): 1.0569,  E(|Yhat-Yhat'|): 1.0750\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8024,  E(|Y-Yhat|): 1.7064,  E(|Yhat-Yhat'|): 1.8080\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7161,  E(|Y-Yhat|): 2.0946,  E(|Yhat-Yhat'|): 0.7572\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6921,  E(|Y-Yhat|): 21.7032,  E(|Yhat-Yhat'|): 42.0221\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9887,  E(|Y-Yhat|): 22.5146,  E(|Yhat-Yhat'|): 43.0518\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7625,  E(|Y-Yhat|): 132.4379,  E(|Yhat-Yhat'|): 263.3509\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8334,  E(|Y-Yhat|): 133.0685,  E(|Yhat-Yhat'|): 264.4702\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2968,  E(|Y-Yhat|): 0.5283,  E(|Yhat-Yhat'|): 0.4630\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2000,  E(|Y-Yhat|): 0.4007,  E(|Yhat-Yhat'|): 0.4015\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1994,  E(|Y-Yhat|): 0.3979,  E(|Yhat-Yhat'|): 0.3971\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1974,  E(|Y-Yhat|): 0.4023,  E(|Yhat-Yhat'|): 0.4097\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1881,  E(|Y-Yhat|): 0.3821,  E(|Yhat-Yhat'|): 0.3880\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6269,  E(|Y-Yhat|): 1.0376,  E(|Yhat-Yhat'|): 0.8213\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3776,  E(|Y-Yhat|): 0.7800,  E(|Yhat-Yhat'|): 0.8048\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3782,  E(|Y-Yhat|): 0.7565,  E(|Yhat-Yhat'|): 0.7565\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3812,  E(|Y-Yhat|): 0.7635,  E(|Yhat-Yhat'|): 0.7646\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3642,  E(|Y-Yhat|): 0.7240,  E(|Yhat-Yhat'|): 0.7196\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3122,  E(|Y-Yhat|): 0.5645,  E(|Yhat-Yhat'|): 0.5046\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2182,  E(|Y-Yhat|): 0.4385,  E(|Yhat-Yhat'|): 0.4405\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2187,  E(|Y-Yhat|): 0.4426,  E(|Yhat-Yhat'|): 0.4478\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2145,  E(|Y-Yhat|): 0.4349,  E(|Yhat-Yhat'|): 0.4408\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2120,  E(|Y-Yhat|): 0.4218,  E(|Yhat-Yhat'|): 0.4196\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8009,  E(|Y-Yhat|): 1.3689,  E(|Yhat-Yhat'|): 1.1360\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5320,  E(|Y-Yhat|): 1.0550,  E(|Yhat-Yhat'|): 1.0460\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5217,  E(|Y-Yhat|): 1.0577,  E(|Yhat-Yhat'|): 1.0719\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5275,  E(|Y-Yhat|): 1.0608,  E(|Yhat-Yhat'|): 1.0666\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0256,  E(|Y-Yhat|): 0.0524,  E(|Yhat-Yhat'|): 0.0536\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6981,  E(|Y-Yhat|): 2.0714,  E(|Yhat-Yhat'|): 0.7464\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9154,  E(|Y-Yhat|): 9.3658,  E(|Yhat-Yhat'|): 16.9007\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 4.5861,  E(|Y-Yhat|): 135.7158,  E(|Yhat-Yhat'|): 262.2594\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 3.5307,  E(|Y-Yhat|): 251.7157,  E(|Yhat-Yhat'|): 496.3699\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0080,  E(|Y-Yhat|): 7.1846,  E(|Yhat-Yhat'|): 14.3532\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3131,  E(|Y-Yhat|): 0.5303,  E(|Yhat-Yhat'|): 0.4345\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2037,  E(|Y-Yhat|): 0.3988,  E(|Yhat-Yhat'|): 0.3903\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1984,  E(|Y-Yhat|): 0.3997,  E(|Yhat-Yhat'|): 0.4025\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1944,  E(|Y-Yhat|): 0.3946,  E(|Yhat-Yhat'|): 0.4004\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0123,  E(|Y-Yhat|): 0.0257,  E(|Yhat-Yhat'|): 0.0268\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6253,  E(|Y-Yhat|): 1.0203,  E(|Yhat-Yhat'|): 0.7900\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3675,  E(|Y-Yhat|): 0.7515,  E(|Yhat-Yhat'|): 0.7681\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3770,  E(|Y-Yhat|): 0.7559,  E(|Yhat-Yhat'|): 0.7578\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3746,  E(|Y-Yhat|): 0.7515,  E(|Yhat-Yhat'|): 0.7537\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0240,  E(|Y-Yhat|): 0.0498,  E(|Yhat-Yhat'|): 0.0515\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3024,  E(|Y-Yhat|): 0.5476,  E(|Yhat-Yhat'|): 0.4904\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2139,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.4368\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2157,  E(|Y-Yhat|): 0.4350,  E(|Yhat-Yhat'|): 0.4386\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2183,  E(|Y-Yhat|): 0.4334,  E(|Yhat-Yhat'|): 0.4303\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0229,  E(|Y-Yhat|): 0.0453,  E(|Yhat-Yhat'|): 0.0447\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8131,  E(|Y-Yhat|): 1.3587,  E(|Yhat-Yhat'|): 1.0911\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5218,  E(|Y-Yhat|): 1.0557,  E(|Yhat-Yhat'|): 1.0677\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5217,  E(|Y-Yhat|): 1.0627,  E(|Yhat-Yhat'|): 1.0821\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5181,  E(|Y-Yhat|): 1.0606,  E(|Yhat-Yhat'|): 1.0851\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2348,  E(|Y-Yhat|): 0.4964,  E(|Yhat-Yhat'|): 0.5232\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6673,  E(|Y-Yhat|): 1.9943,  E(|Yhat-Yhat'|): 0.6541\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7439,  E(|Y-Yhat|): 11.1461,  E(|Yhat-Yhat'|): 20.8044\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9078,  E(|Y-Yhat|): 12.2675,  E(|Yhat-Yhat'|): 22.7193\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7270,  E(|Y-Yhat|): 5.6209,  E(|Yhat-Yhat'|): 9.7878\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2311,  E(|Y-Yhat|): 1.6864,  E(|Yhat-Yhat'|): 2.9107\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3057,  E(|Y-Yhat|): 0.5400,  E(|Yhat-Yhat'|): 0.4686\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1982,  E(|Y-Yhat|): 0.3937,  E(|Yhat-Yhat'|): 0.3909\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1944,  E(|Y-Yhat|): 0.3948,  E(|Yhat-Yhat'|): 0.4007\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1946,  E(|Y-Yhat|): 0.3977,  E(|Yhat-Yhat'|): 0.4062\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0882,  E(|Y-Yhat|): 0.1786,  E(|Yhat-Yhat'|): 0.1810\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6037,  E(|Y-Yhat|): 1.0409,  E(|Yhat-Yhat'|): 0.8743\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3633,  E(|Y-Yhat|): 0.7486,  E(|Yhat-Yhat'|): 0.7706\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3692,  E(|Y-Yhat|): 0.7564,  E(|Yhat-Yhat'|): 0.7743\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3738,  E(|Y-Yhat|): 0.7578,  E(|Yhat-Yhat'|): 0.7681\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1515,  E(|Y-Yhat|): 0.3135,  E(|Yhat-Yhat'|): 0.3240\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3118,  E(|Y-Yhat|): 0.5709,  E(|Yhat-Yhat'|): 0.5183\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2164,  E(|Y-Yhat|): 0.4414,  E(|Yhat-Yhat'|): 0.4498\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2121,  E(|Y-Yhat|): 0.4304,  E(|Yhat-Yhat'|): 0.4367\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2170,  E(|Y-Yhat|): 0.4362,  E(|Yhat-Yhat'|): 0.4385\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1172,  E(|Y-Yhat|): 0.2372,  E(|Yhat-Yhat'|): 0.2400\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7529,  E(|Y-Yhat|): 1.3501,  E(|Yhat-Yhat'|): 1.1943\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5271,  E(|Y-Yhat|): 1.0934,  E(|Yhat-Yhat'|): 1.1327\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5214,  E(|Y-Yhat|): 1.0635,  E(|Yhat-Yhat'|): 1.0842\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5240,  E(|Y-Yhat|): 1.0437,  E(|Yhat-Yhat'|): 1.0393\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0744,  E(|Y-Yhat|): 0.1501,  E(|Yhat-Yhat'|): 0.1513\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7591,  E(|Y-Yhat|): 2.0329,  E(|Yhat-Yhat'|): 0.5476\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8766,  E(|Y-Yhat|): 21.6980,  E(|Yhat-Yhat'|): 41.6429\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5721,  E(|Y-Yhat|): 75.9564,  E(|Yhat-Yhat'|): 148.7684\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9020,  E(|Y-Yhat|): 131.5842,  E(|Yhat-Yhat'|): 259.3645\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0024,  E(|Y-Yhat|): 11.4581,  E(|Yhat-Yhat'|): 22.9210\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2980,  E(|Y-Yhat|): 0.5442,  E(|Yhat-Yhat'|): 0.4924\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2010,  E(|Y-Yhat|): 0.3992,  E(|Yhat-Yhat'|): 0.3964\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1958,  E(|Y-Yhat|): 0.3972,  E(|Yhat-Yhat'|): 0.4028\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1947,  E(|Y-Yhat|): 0.3956,  E(|Yhat-Yhat'|): 0.4018\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0339,  E(|Y-Yhat|): 0.0692,  E(|Yhat-Yhat'|): 0.0705\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6565,  E(|Y-Yhat|): 1.0795,  E(|Yhat-Yhat'|): 0.8460\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3735,  E(|Y-Yhat|): 0.7458,  E(|Yhat-Yhat'|): 0.7447\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3684,  E(|Y-Yhat|): 0.7447,  E(|Yhat-Yhat'|): 0.7525\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3785,  E(|Y-Yhat|): 0.7539,  E(|Yhat-Yhat'|): 0.7507\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0639,  E(|Y-Yhat|): 0.1299,  E(|Yhat-Yhat'|): 0.1321\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3073,  E(|Y-Yhat|): 0.5586,  E(|Yhat-Yhat'|): 0.5027\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2144,  E(|Y-Yhat|): 0.4325,  E(|Yhat-Yhat'|): 0.4362\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2163,  E(|Y-Yhat|): 0.4410,  E(|Yhat-Yhat'|): 0.4493\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2159,  E(|Y-Yhat|): 0.4386,  E(|Yhat-Yhat'|): 0.4454\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0553,  E(|Y-Yhat|): 0.1102,  E(|Yhat-Yhat'|): 0.1099\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7715,  E(|Y-Yhat|): 1.3443,  E(|Yhat-Yhat'|): 1.1456\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5169,  E(|Y-Yhat|): 1.0595,  E(|Yhat-Yhat'|): 1.0852\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5187,  E(|Y-Yhat|): 1.0751,  E(|Yhat-Yhat'|): 1.1128\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5158,  E(|Y-Yhat|): 1.0339,  E(|Yhat-Yhat'|): 1.0362\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0394,  E(|Y-Yhat|): 0.0820,  E(|Yhat-Yhat'|): 0.0854\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7533,  E(|Y-Yhat|): 2.1201,  E(|Yhat-Yhat'|): 0.7337\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8157,  E(|Y-Yhat|): 4.4951,  E(|Yhat-Yhat'|): 7.3588\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8974,  E(|Y-Yhat|): 5.8020,  E(|Yhat-Yhat'|): 9.8093\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7695,  E(|Y-Yhat|): 255.8441,  E(|Yhat-Yhat'|): 510.1492\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0352,  E(|Y-Yhat|): 12.9447,  E(|Yhat-Yhat'|): 25.9597\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3045,  E(|Y-Yhat|): 0.5425,  E(|Yhat-Yhat'|): 0.4761\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1969,  E(|Y-Yhat|): 0.3955,  E(|Yhat-Yhat'|): 0.3971\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1921,  E(|Y-Yhat|): 0.3871,  E(|Yhat-Yhat'|): 0.3899\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1961,  E(|Y-Yhat|): 0.3938,  E(|Yhat-Yhat'|): 0.3954\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0185,  E(|Y-Yhat|): 0.0375,  E(|Yhat-Yhat'|): 0.0379\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6192,  E(|Y-Yhat|): 1.0402,  E(|Yhat-Yhat'|): 0.8420\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3661,  E(|Y-Yhat|): 0.7461,  E(|Yhat-Yhat'|): 0.7602\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3711,  E(|Y-Yhat|): 0.7543,  E(|Yhat-Yhat'|): 0.7663\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3707,  E(|Y-Yhat|): 0.7393,  E(|Yhat-Yhat'|): 0.7372\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0352,  E(|Y-Yhat|): 0.0703,  E(|Yhat-Yhat'|): 0.0703\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3168,  E(|Y-Yhat|): 0.5683,  E(|Yhat-Yhat'|): 0.5031\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2144,  E(|Y-Yhat|): 0.4341,  E(|Yhat-Yhat'|): 0.4393\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2152,  E(|Y-Yhat|): 0.4284,  E(|Yhat-Yhat'|): 0.4263\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2152,  E(|Y-Yhat|): 0.4308,  E(|Yhat-Yhat'|): 0.4313\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0328,  E(|Y-Yhat|): 0.0655,  E(|Yhat-Yhat'|): 0.0655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7938,  E(|Y-Yhat|): 1.3629,  E(|Yhat-Yhat'|): 1.1383\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5255,  E(|Y-Yhat|): 1.0624,  E(|Yhat-Yhat'|): 1.0737\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5254,  E(|Y-Yhat|): 1.0538,  E(|Yhat-Yhat'|): 1.0568\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5209,  E(|Y-Yhat|): 1.0644,  E(|Yhat-Yhat'|): 1.0869\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0329,  E(|Y-Yhat|): 0.0671,  E(|Yhat-Yhat'|): 0.0685\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9027,  E(|Y-Yhat|): 2.1763,  E(|Yhat-Yhat'|): 0.5472\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6535,  E(|Y-Yhat|): 67.4611,  E(|Yhat-Yhat'|): 131.6152\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 9.0018,  E(|Y-Yhat|): 277.2247,  E(|Yhat-Yhat'|): 536.4458\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6331,  E(|Y-Yhat|): 66.5788,  E(|Yhat-Yhat'|): 131.8916\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0138,  E(|Y-Yhat|): 2.7229,  E(|Yhat-Yhat'|): 5.4182\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3135,  E(|Y-Yhat|): 0.5471,  E(|Yhat-Yhat'|): 0.4673\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1981,  E(|Y-Yhat|): 0.3997,  E(|Yhat-Yhat'|): 0.4032\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1978,  E(|Y-Yhat|): 0.3936,  E(|Yhat-Yhat'|): 0.3915\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1950,  E(|Y-Yhat|): 0.3952,  E(|Yhat-Yhat'|): 0.4003\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0155,  E(|Y-Yhat|): 0.0318,  E(|Yhat-Yhat'|): 0.0328\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6582,  E(|Y-Yhat|): 1.0694,  E(|Yhat-Yhat'|): 0.8225\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3795,  E(|Y-Yhat|): 0.7460,  E(|Yhat-Yhat'|): 0.7331\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3758,  E(|Y-Yhat|): 0.7662,  E(|Yhat-Yhat'|): 0.7808\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3727,  E(|Y-Yhat|): 0.7521,  E(|Yhat-Yhat'|): 0.7586\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0305,  E(|Y-Yhat|): 0.0627,  E(|Yhat-Yhat'|): 0.0644\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3208,  E(|Y-Yhat|): 0.5680,  E(|Yhat-Yhat'|): 0.4945\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2181,  E(|Y-Yhat|): 0.4342,  E(|Yhat-Yhat'|): 0.4321\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2141,  E(|Y-Yhat|): 0.4347,  E(|Yhat-Yhat'|): 0.4413\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2142,  E(|Y-Yhat|): 0.4314,  E(|Yhat-Yhat'|): 0.4344\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0285,  E(|Y-Yhat|): 0.0562,  E(|Yhat-Yhat'|): 0.0553\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"log\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = log_lin(Z)\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bf9e763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0004),\n",
       " tensor(0.3281),\n",
       " tensor(0.0006),\n",
       " tensor(0.0005),\n",
       " tensor(0.0005))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f10583",
   "metadata": {},
   "source": [
    "# pre ANM, comparing 4 loss functions under different true fucntions ($X, Y \\in R^2$, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35aecf",
   "metadata": {},
   "source": [
    "## True function: softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26d56a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8071,  E(|Y-Yhat|): 1.3805,  E(|Yhat-Yhat'|): 1.1468\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5766,  E(|Y-Yhat|): 1.1399,  E(|Yhat-Yhat'|): 1.1266\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5530,  E(|Y-Yhat|): 1.1199,  E(|Yhat-Yhat'|): 1.1338\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5545,  E(|Y-Yhat|): 1.1284,  E(|Yhat-Yhat'|): 1.1478\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5321,  E(|Y-Yhat|): 1.0579,  E(|Yhat-Yhat'|): 1.0517\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8143,  E(|Y-Yhat|): 2.1797,  E(|Yhat-Yhat'|): 0.7309\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.7615,  E(|Y-Yhat|): 38.4422,  E(|Yhat-Yhat'|): 73.3614\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2724,  E(|Y-Yhat|): 17.1999,  E(|Yhat-Yhat'|): 31.8550\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4842,  E(|Y-Yhat|): 131.8671,  E(|Yhat-Yhat'|): 260.7659\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6372,  E(|Y-Yhat|): 60.3780,  E(|Yhat-Yhat'|): 119.4815\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3180,  E(|Y-Yhat|): 0.5239,  E(|Yhat-Yhat'|): 0.4119\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1974,  E(|Y-Yhat|): 0.4035,  E(|Yhat-Yhat'|): 0.4122\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1938,  E(|Y-Yhat|): 0.3965,  E(|Yhat-Yhat'|): 0.4053\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1975,  E(|Y-Yhat|): 0.4026,  E(|Yhat-Yhat'|): 0.4103\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1363,  E(|Y-Yhat|): 0.2802,  E(|Yhat-Yhat'|): 0.2878\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5837,  E(|Y-Yhat|): 0.9923,  E(|Yhat-Yhat'|): 0.8173\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3871,  E(|Y-Yhat|): 0.7967,  E(|Yhat-Yhat'|): 0.8191\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3940,  E(|Y-Yhat|): 0.7900,  E(|Yhat-Yhat'|): 0.7921\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3996,  E(|Y-Yhat|): 0.8124,  E(|Yhat-Yhat'|): 0.8256\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2566,  E(|Y-Yhat|): 0.5254,  E(|Yhat-Yhat'|): 0.5376\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3095,  E(|Y-Yhat|): 0.5583,  E(|Yhat-Yhat'|): 0.4976\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2143,  E(|Y-Yhat|): 0.4291,  E(|Yhat-Yhat'|): 0.4296\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2123,  E(|Y-Yhat|): 0.4277,  E(|Yhat-Yhat'|): 0.4307\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2117,  E(|Y-Yhat|): 0.4237,  E(|Yhat-Yhat'|): 0.4238\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1602,  E(|Y-Yhat|): 0.3202,  E(|Yhat-Yhat'|): 0.3200\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8437,  E(|Y-Yhat|): 1.4218,  E(|Yhat-Yhat'|): 1.1564\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4701,  E(|Y-Yhat|): 0.9522,  E(|Yhat-Yhat'|): 0.9643\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4698,  E(|Y-Yhat|): 0.9460,  E(|Yhat-Yhat'|): 0.9524\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4728,  E(|Y-Yhat|): 0.9363,  E(|Yhat-Yhat'|): 0.9270\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1830,  E(|Y-Yhat|): 0.3812,  E(|Yhat-Yhat'|): 0.3965\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7561,  E(|Y-Yhat|): 2.0979,  E(|Yhat-Yhat'|): 0.6835\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9196,  E(|Y-Yhat|): 67.2797,  E(|Yhat-Yhat'|): 130.7202\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9082,  E(|Y-Yhat|): 92.6405,  E(|Yhat-Yhat'|): 183.4647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3519,  E(|Y-Yhat|): 42.0602,  E(|Yhat-Yhat'|): 83.4167\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1880,  E(|Y-Yhat|): 11.8424,  E(|Yhat-Yhat'|): 23.3087\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3093,  E(|Y-Yhat|): 0.5573,  E(|Yhat-Yhat'|): 0.4961\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1740,  E(|Y-Yhat|): 0.3589,  E(|Yhat-Yhat'|): 0.3698\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1734,  E(|Y-Yhat|): 0.3502,  E(|Yhat-Yhat'|): 0.3535\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1770,  E(|Y-Yhat|): 0.3514,  E(|Yhat-Yhat'|): 0.3488\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0741,  E(|Y-Yhat|): 0.1495,  E(|Yhat-Yhat'|): 0.1508\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6445,  E(|Y-Yhat|): 1.0490,  E(|Yhat-Yhat'|): 0.8090\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3250,  E(|Y-Yhat|): 0.6547,  E(|Yhat-Yhat'|): 0.6594\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3189,  E(|Y-Yhat|): 0.6451,  E(|Yhat-Yhat'|): 0.6525\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3242,  E(|Y-Yhat|): 0.6514,  E(|Yhat-Yhat'|): 0.6544\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1350,  E(|Y-Yhat|): 0.2679,  E(|Yhat-Yhat'|): 0.2659\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3186,  E(|Y-Yhat|): 0.5755,  E(|Yhat-Yhat'|): 0.5139\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1978,  E(|Y-Yhat|): 0.4008,  E(|Yhat-Yhat'|): 0.4059\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2006,  E(|Y-Yhat|): 0.4011,  E(|Yhat-Yhat'|): 0.4010\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1982,  E(|Y-Yhat|): 0.4010,  E(|Yhat-Yhat'|): 0.4057\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1043,  E(|Y-Yhat|): 0.2103,  E(|Yhat-Yhat'|): 0.2119\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8160,  E(|Y-Yhat|): 1.3852,  E(|Yhat-Yhat'|): 1.1383\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4882,  E(|Y-Yhat|): 1.0028,  E(|Yhat-Yhat'|): 1.0292\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4791,  E(|Y-Yhat|): 0.9657,  E(|Yhat-Yhat'|): 0.9732\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4857,  E(|Y-Yhat|): 0.9824,  E(|Yhat-Yhat'|): 0.9935\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0317,  E(|Y-Yhat|): 0.0643,  E(|Yhat-Yhat'|): 0.0652\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7947,  E(|Y-Yhat|): 2.1508,  E(|Yhat-Yhat'|): 0.7122\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4153,  E(|Y-Yhat|): 52.1966,  E(|Yhat-Yhat'|): 103.5626\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6580,  E(|Y-Yhat|): 42.1059,  E(|Yhat-Yhat'|): 82.8957\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5951,  E(|Y-Yhat|): 59.0779,  E(|Yhat-Yhat'|): 116.9655\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0360,  E(|Y-Yhat|): 2.1069,  E(|Yhat-Yhat'|): 4.1418\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2988,  E(|Y-Yhat|): 0.5308,  E(|Yhat-Yhat'|): 0.4640\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1812,  E(|Y-Yhat|): 0.3663,  E(|Yhat-Yhat'|): 0.3703\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1818,  E(|Y-Yhat|): 0.3626,  E(|Yhat-Yhat'|): 0.3617\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1820,  E(|Y-Yhat|): 0.3696,  E(|Yhat-Yhat'|): 0.3753\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0146,  E(|Y-Yhat|): 0.0301,  E(|Yhat-Yhat'|): 0.0309\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6302,  E(|Y-Yhat|): 1.0268,  E(|Yhat-Yhat'|): 0.7931\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3307,  E(|Y-Yhat|): 0.6674,  E(|Yhat-Yhat'|): 0.6735\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3501,  E(|Y-Yhat|): 0.6957,  E(|Yhat-Yhat'|): 0.6911\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3414,  E(|Y-Yhat|): 0.6875,  E(|Yhat-Yhat'|): 0.6924\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0287,  E(|Y-Yhat|): 0.0572,  E(|Yhat-Yhat'|): 0.0570\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3349,  E(|Y-Yhat|): 0.5838,  E(|Yhat-Yhat'|): 0.4977\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2057,  E(|Y-Yhat|): 0.4118,  E(|Yhat-Yhat'|): 0.4122\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2058,  E(|Y-Yhat|): 0.4104,  E(|Yhat-Yhat'|): 0.4093\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2084,  E(|Y-Yhat|): 0.4143,  E(|Yhat-Yhat'|): 0.4117\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0263,  E(|Y-Yhat|): 0.0526,  E(|Yhat-Yhat'|): 0.0526\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8595,  E(|Y-Yhat|): 1.4335,  E(|Yhat-Yhat'|): 1.1479\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4851,  E(|Y-Yhat|): 0.9905,  E(|Yhat-Yhat'|): 1.0108\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4899,  E(|Y-Yhat|): 0.9845,  E(|Yhat-Yhat'|): 0.9891\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4862,  E(|Y-Yhat|): 0.9827,  E(|Yhat-Yhat'|): 0.9931\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0424,  E(|Y-Yhat|): 0.0860,  E(|Yhat-Yhat'|): 0.0873\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6712,  E(|Y-Yhat|): 2.0241,  E(|Yhat-Yhat'|): 0.7057\n",
      "[Epoch 100 (33%), batch 9] energy-loss: -0.3232,  E(|Y-Yhat|): 103.3192,  E(|Yhat-Yhat'|): 207.2848\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.9903,  E(|Y-Yhat|): 201.3212,  E(|Yhat-Yhat'|): 404.6229\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 39.2955,  E(|Y-Yhat|): 1813.8958,  E(|Yhat-Yhat'|): 3549.2005\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.7239,  E(|Y-Yhat|): 105.6334,  E(|Yhat-Yhat'|): 207.8191\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3315,  E(|Y-Yhat|): 0.5590,  E(|Yhat-Yhat'|): 0.4550\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1820,  E(|Y-Yhat|): 0.3692,  E(|Yhat-Yhat'|): 0.3744\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1822,  E(|Y-Yhat|): 0.3635,  E(|Yhat-Yhat'|): 0.3625\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1808,  E(|Y-Yhat|): 0.3593,  E(|Yhat-Yhat'|): 0.3569\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0199,  E(|Y-Yhat|): 0.0391,  E(|Yhat-Yhat'|): 0.0383\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6412,  E(|Y-Yhat|): 1.0605,  E(|Yhat-Yhat'|): 0.8385\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3336,  E(|Y-Yhat|): 0.6791,  E(|Yhat-Yhat'|): 0.6908\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3393,  E(|Y-Yhat|): 0.6747,  E(|Yhat-Yhat'|): 0.6706\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3362,  E(|Y-Yhat|): 0.6803,  E(|Yhat-Yhat'|): 0.6883\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0377,  E(|Y-Yhat|): 0.0769,  E(|Yhat-Yhat'|): 0.0785\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3312,  E(|Y-Yhat|): 0.5971,  E(|Yhat-Yhat'|): 0.5318\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2083,  E(|Y-Yhat|): 0.4220,  E(|Yhat-Yhat'|): 0.4274\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2087,  E(|Y-Yhat|): 0.4211,  E(|Yhat-Yhat'|): 0.4249\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2099,  E(|Y-Yhat|): 0.4221,  E(|Yhat-Yhat'|): 0.4242\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0346,  E(|Y-Yhat|): 0.0712,  E(|Yhat-Yhat'|): 0.0733\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8382,  E(|Y-Yhat|): 1.3613,  E(|Yhat-Yhat'|): 1.0463\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4631,  E(|Y-Yhat|): 0.9373,  E(|Yhat-Yhat'|): 0.9483\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4670,  E(|Y-Yhat|): 0.9237,  E(|Yhat-Yhat'|): 0.9134\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4620,  E(|Y-Yhat|): 0.9154,  E(|Yhat-Yhat'|): 0.9067\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9507,  E(|Y-Yhat|): 3.7353,  E(|Yhat-Yhat'|): 3.5692\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6165,  E(|Y-Yhat|): 1.9883,  E(|Yhat-Yhat'|): 0.7436\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5807,  E(|Y-Yhat|): 38.7078,  E(|Yhat-Yhat'|): 76.2543\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 16.9243,  E(|Y-Yhat|): 1330.0659,  E(|Yhat-Yhat'|): 2626.2831\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 3.4702,  E(|Y-Yhat|): 244.8782,  E(|Yhat-Yhat'|): 482.8160\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.6552,  E(|Y-Yhat|): 729.2009,  E(|Yhat-Yhat'|): 1451.0913\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2939,  E(|Y-Yhat|): 0.5177,  E(|Yhat-Yhat'|): 0.4476\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1734,  E(|Y-Yhat|): 0.3447,  E(|Yhat-Yhat'|): 0.3426\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1684,  E(|Y-Yhat|): 0.3404,  E(|Yhat-Yhat'|): 0.3440\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1731,  E(|Y-Yhat|): 0.3425,  E(|Yhat-Yhat'|): 0.3389\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2655,  E(|Y-Yhat|): 0.5360,  E(|Yhat-Yhat'|): 0.5411\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6062,  E(|Y-Yhat|): 1.0076,  E(|Yhat-Yhat'|): 0.8028\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3151,  E(|Y-Yhat|): 0.6555,  E(|Yhat-Yhat'|): 0.6809\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3172,  E(|Y-Yhat|): 0.6357,  E(|Yhat-Yhat'|): 0.6369\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3152,  E(|Y-Yhat|): 0.6397,  E(|Yhat-Yhat'|): 0.6491\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5644,  E(|Y-Yhat|): 1.1357,  E(|Yhat-Yhat'|): 1.1425\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3193,  E(|Y-Yhat|): 0.5647,  E(|Yhat-Yhat'|): 0.4909\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1943,  E(|Y-Yhat|): 0.3880,  E(|Yhat-Yhat'|): 0.3874\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1911,  E(|Y-Yhat|): 0.3846,  E(|Yhat-Yhat'|): 0.3869\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1921,  E(|Y-Yhat|): 0.3852,  E(|Yhat-Yhat'|): 0.3862\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2725,  E(|Y-Yhat|): 0.5456,  E(|Yhat-Yhat'|): 0.5463\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7788,  E(|Y-Yhat|): 1.3528,  E(|Yhat-Yhat'|): 1.1479\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4964,  E(|Y-Yhat|): 0.9812,  E(|Yhat-Yhat'|): 0.9698\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4768,  E(|Y-Yhat|): 0.9730,  E(|Yhat-Yhat'|): 0.9925\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4784,  E(|Y-Yhat|): 0.9791,  E(|Yhat-Yhat'|): 1.0015\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2903,  E(|Y-Yhat|): 0.6054,  E(|Yhat-Yhat'|): 0.6302\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7332,  E(|Y-Yhat|): 2.1227,  E(|Yhat-Yhat'|): 0.7790\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7223,  E(|Y-Yhat|): 15.3941,  E(|Yhat-Yhat'|): 29.3435\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -2.8404,  E(|Y-Yhat|): 129.2555,  E(|Yhat-Yhat'|): 264.1917\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 72.7551,  E(|Y-Yhat|): 4963.4679,  E(|Yhat-Yhat'|): 9781.4257\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 37.0729,  E(|Y-Yhat|): 1991.8436,  E(|Yhat-Yhat'|): 3909.5415\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2995,  E(|Y-Yhat|): 0.5206,  E(|Yhat-Yhat'|): 0.4422\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1762,  E(|Y-Yhat|): 0.3620,  E(|Yhat-Yhat'|): 0.3717\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1736,  E(|Y-Yhat|): 0.3550,  E(|Yhat-Yhat'|): 0.3628\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1749,  E(|Y-Yhat|): 0.3492,  E(|Yhat-Yhat'|): 0.3486\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0974,  E(|Y-Yhat|): 0.1971,  E(|Yhat-Yhat'|): 0.1993\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6255,  E(|Y-Yhat|): 1.0267,  E(|Yhat-Yhat'|): 0.8024\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3268,  E(|Y-Yhat|): 0.6776,  E(|Yhat-Yhat'|): 0.7015\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3311,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6707\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3292,  E(|Y-Yhat|): 0.6735,  E(|Yhat-Yhat'|): 0.6885\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1726,  E(|Y-Yhat|): 0.3579,  E(|Yhat-Yhat'|): 0.3707\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3101,  E(|Y-Yhat|): 0.5597,  E(|Yhat-Yhat'|): 0.4991\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2007,  E(|Y-Yhat|): 0.4031,  E(|Yhat-Yhat'|): 0.4049\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1968,  E(|Y-Yhat|): 0.3988,  E(|Yhat-Yhat'|): 0.4040\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2021,  E(|Y-Yhat|): 0.4031,  E(|Yhat-Yhat'|): 0.4019\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1230,  E(|Y-Yhat|): 0.2459,  E(|Yhat-Yhat'|): 0.2459\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6987,  E(|Y-Yhat|): 1.1860,  E(|Yhat-Yhat'|): 0.9746\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4394,  E(|Y-Yhat|): 0.8885,  E(|Yhat-Yhat'|): 0.8982\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4458,  E(|Y-Yhat|): 0.9039,  E(|Yhat-Yhat'|): 0.9163\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4489,  E(|Y-Yhat|): 0.9084,  E(|Yhat-Yhat'|): 0.9191\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0999,  E(|Y-Yhat|): 0.1891,  E(|Yhat-Yhat'|): 0.1784\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8438,  E(|Y-Yhat|): 2.2132,  E(|Yhat-Yhat'|): 0.7389\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3244,  E(|Y-Yhat|): 18.1163,  E(|Yhat-Yhat'|): 33.5837\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2165,  E(|Y-Yhat|): 96.1057,  E(|Yhat-Yhat'|): 191.7783\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -0.1602,  E(|Y-Yhat|): 82.2836,  E(|Yhat-Yhat'|): 164.8877\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2304,  E(|Y-Yhat|): 7.8474,  E(|Yhat-Yhat'|): 15.2340\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2344,  E(|Y-Yhat|): 0.4245,  E(|Yhat-Yhat'|): 0.3802\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1391,  E(|Y-Yhat|): 0.2879,  E(|Yhat-Yhat'|): 0.2977\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1394,  E(|Y-Yhat|): 0.2817,  E(|Yhat-Yhat'|): 0.2845\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1406,  E(|Y-Yhat|): 0.2812,  E(|Yhat-Yhat'|): 0.2813\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0316,  E(|Y-Yhat|): 0.0613,  E(|Yhat-Yhat'|): 0.0593\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4553,  E(|Y-Yhat|): 0.8117,  E(|Yhat-Yhat'|): 0.7127\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2866,  E(|Y-Yhat|): 0.5784,  E(|Yhat-Yhat'|): 0.5835\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2823,  E(|Y-Yhat|): 0.5706,  E(|Yhat-Yhat'|): 0.5767\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2835,  E(|Y-Yhat|): 0.5643,  E(|Yhat-Yhat'|): 0.5616\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0602,  E(|Y-Yhat|): 0.1205,  E(|Yhat-Yhat'|): 0.1206\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2570,  E(|Y-Yhat|): 0.4855,  E(|Yhat-Yhat'|): 0.4570\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1629,  E(|Y-Yhat|): 0.3304,  E(|Yhat-Yhat'|): 0.3350\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1607,  E(|Y-Yhat|): 0.3242,  E(|Yhat-Yhat'|): 0.3269\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1636,  E(|Y-Yhat|): 0.3295,  E(|Yhat-Yhat'|): 0.3318\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0467,  E(|Y-Yhat|): 0.0915,  E(|Yhat-Yhat'|): 0.0897\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7398,  E(|Y-Yhat|): 1.3033,  E(|Yhat-Yhat'|): 1.1270\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4613,  E(|Y-Yhat|): 0.9374,  E(|Yhat-Yhat'|): 0.9523\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4468,  E(|Y-Yhat|): 0.9122,  E(|Yhat-Yhat'|): 0.9308\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4524,  E(|Y-Yhat|): 0.9233,  E(|Yhat-Yhat'|): 0.9419\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8963,  E(|Y-Yhat|): 1.8264,  E(|Yhat-Yhat'|): 1.8603\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6887,  E(|Y-Yhat|): 1.9768,  E(|Yhat-Yhat'|): 0.5762\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6948,  E(|Y-Yhat|): 30.5512,  E(|Yhat-Yhat'|): 59.7127\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 6.9827,  E(|Y-Yhat|): 540.3053,  E(|Yhat-Yhat'|): 1066.6452\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 14.6429,  E(|Y-Yhat|): 1029.2432,  E(|Yhat-Yhat'|): 2029.2007\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -5.6797,  E(|Y-Yhat|): 1399.0424,  E(|Yhat-Yhat'|): 2809.4441\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2984,  E(|Y-Yhat|): 0.5279,  E(|Yhat-Yhat'|): 0.4590\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1691,  E(|Y-Yhat|): 0.3419,  E(|Yhat-Yhat'|): 0.3456\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1647,  E(|Y-Yhat|): 0.3412,  E(|Yhat-Yhat'|): 0.3530\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1675,  E(|Y-Yhat|): 0.3403,  E(|Yhat-Yhat'|): 0.3455\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1937,  E(|Y-Yhat|): 0.3926,  E(|Yhat-Yhat'|): 0.3979\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6412,  E(|Y-Yhat|): 1.0599,  E(|Yhat-Yhat'|): 0.8375\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3086,  E(|Y-Yhat|): 0.6400,  E(|Yhat-Yhat'|): 0.6627\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3128,  E(|Y-Yhat|): 0.6278,  E(|Yhat-Yhat'|): 0.6299\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3238,  E(|Y-Yhat|): 0.6383,  E(|Yhat-Yhat'|): 0.6290\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3743,  E(|Y-Yhat|): 0.7515,  E(|Yhat-Yhat'|): 0.7543\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2952,  E(|Y-Yhat|): 0.5431,  E(|Yhat-Yhat'|): 0.4957\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1890,  E(|Y-Yhat|): 0.3835,  E(|Yhat-Yhat'|): 0.3890\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1892,  E(|Y-Yhat|): 0.3797,  E(|Yhat-Yhat'|): 0.3809\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1895,  E(|Y-Yhat|): 0.3794,  E(|Yhat-Yhat'|): 0.3797\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2127,  E(|Y-Yhat|): 0.4274,  E(|Yhat-Yhat'|): 0.4295\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8257,  E(|Y-Yhat|): 1.3610,  E(|Yhat-Yhat'|): 1.0706\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5018,  E(|Y-Yhat|): 1.0196,  E(|Yhat-Yhat'|): 1.0356\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5025,  E(|Y-Yhat|): 1.0020,  E(|Yhat-Yhat'|): 0.9991\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4936,  E(|Y-Yhat|): 0.9985,  E(|Yhat-Yhat'|): 1.0098\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0356,  E(|Y-Yhat|): 0.0713,  E(|Yhat-Yhat'|): 0.0714\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9214,  E(|Y-Yhat|): 2.2557,  E(|Yhat-Yhat'|): 0.6686\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.8346,  E(|Y-Yhat|): 444.3706,  E(|Yhat-Yhat'|): 883.0720\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -2.7645,  E(|Y-Yhat|): 348.0023,  E(|Yhat-Yhat'|): 701.5338\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 11.4770,  E(|Y-Yhat|): 707.0272,  E(|Yhat-Yhat'|): 1391.1005\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0469,  E(|Y-Yhat|): 34.0089,  E(|Yhat-Yhat'|): 67.9241\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3139,  E(|Y-Yhat|): 0.5396,  E(|Yhat-Yhat'|): 0.4514\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1825,  E(|Y-Yhat|): 0.3705,  E(|Yhat-Yhat'|): 0.3760\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1843,  E(|Y-Yhat|): 0.3708,  E(|Yhat-Yhat'|): 0.3732\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1762,  E(|Y-Yhat|): 0.3591,  E(|Yhat-Yhat'|): 0.3658\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0162,  E(|Y-Yhat|): 0.0325,  E(|Yhat-Yhat'|): 0.0325\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5908,  E(|Y-Yhat|): 1.0199,  E(|Yhat-Yhat'|): 0.8584\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3437,  E(|Y-Yhat|): 0.7036,  E(|Yhat-Yhat'|): 0.7198\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3401,  E(|Y-Yhat|): 0.6926,  E(|Yhat-Yhat'|): 0.7049\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3474,  E(|Y-Yhat|): 0.6942,  E(|Yhat-Yhat'|): 0.6937\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0314,  E(|Y-Yhat|): 0.0634,  E(|Yhat-Yhat'|): 0.0640\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3163,  E(|Y-Yhat|): 0.5691,  E(|Yhat-Yhat'|): 0.5057\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2024,  E(|Y-Yhat|): 0.4093,  E(|Yhat-Yhat'|): 0.4138\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1994,  E(|Y-Yhat|): 0.4067,  E(|Yhat-Yhat'|): 0.4146\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1999,  E(|Y-Yhat|): 0.4085,  E(|Yhat-Yhat'|): 0.4171\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0282,  E(|Y-Yhat|): 0.0577,  E(|Yhat-Yhat'|): 0.0589\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8224,  E(|Y-Yhat|): 1.3973,  E(|Yhat-Yhat'|): 1.1497\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4506,  E(|Y-Yhat|): 0.9238,  E(|Yhat-Yhat'|): 0.9464\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4487,  E(|Y-Yhat|): 0.9093,  E(|Yhat-Yhat'|): 0.9212\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4553,  E(|Y-Yhat|): 0.9109,  E(|Yhat-Yhat'|): 0.9113\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.4991,  E(|Y-Yhat|): 3.0267,  E(|Yhat-Yhat'|): 3.0551\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7778,  E(|Y-Yhat|): 2.0426,  E(|Yhat-Yhat'|): 0.5296\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8063,  E(|Y-Yhat|): 26.6757,  E(|Yhat-Yhat'|): 51.7387\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 7.5924,  E(|Y-Yhat|): 491.7718,  E(|Yhat-Yhat'|): 968.3589\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -1.1317,  E(|Y-Yhat|): 244.6572,  E(|Yhat-Yhat'|): 491.5778\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9586,  E(|Y-Yhat|): 525.8848,  E(|Yhat-Yhat'|): 1047.8525\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3191,  E(|Y-Yhat|): 0.5518,  E(|Yhat-Yhat'|): 0.4654\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1678,  E(|Y-Yhat|): 0.3348,  E(|Yhat-Yhat'|): 0.3340\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1673,  E(|Y-Yhat|): 0.3297,  E(|Yhat-Yhat'|): 0.3249\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1660,  E(|Y-Yhat|): 0.3321,  E(|Yhat-Yhat'|): 0.3324\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2435,  E(|Y-Yhat|): 0.4916,  E(|Yhat-Yhat'|): 0.4963\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6264,  E(|Y-Yhat|): 1.0550,  E(|Yhat-Yhat'|): 0.8571\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3224,  E(|Y-Yhat|): 0.6395,  E(|Yhat-Yhat'|): 0.6343\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3102,  E(|Y-Yhat|): 0.6221,  E(|Yhat-Yhat'|): 0.6238\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3167,  E(|Y-Yhat|): 0.6273,  E(|Yhat-Yhat'|): 0.6211\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4910,  E(|Y-Yhat|): 0.9962,  E(|Yhat-Yhat'|): 1.0103\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2985,  E(|Y-Yhat|): 0.5562,  E(|Yhat-Yhat'|): 0.5155\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1892,  E(|Y-Yhat|): 0.3809,  E(|Yhat-Yhat'|): 0.3834\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1859,  E(|Y-Yhat|): 0.3821,  E(|Yhat-Yhat'|): 0.3925\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1934,  E(|Y-Yhat|): 0.3795,  E(|Yhat-Yhat'|): 0.3723\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2562,  E(|Y-Yhat|): 0.5137,  E(|Yhat-Yhat'|): 0.5148\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0025),\n",
       " tensor(2.7211),\n",
       " tensor(0.0022),\n",
       " tensor(0.0027),\n",
       " tensor(0.0018))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 50)\n",
    "    x2 = torch.linspace(0, 5, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = F.softplus(Z)   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee820b4",
   "metadata": {},
   "source": [
    "## True function: cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed8affbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5660,  E(|Y-Yhat|): 0.9050,  E(|Yhat-Yhat'|): 0.6780\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4496,  E(|Y-Yhat|): 0.8776,  E(|Yhat-Yhat'|): 0.8561\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4370,  E(|Y-Yhat|): 0.8835,  E(|Yhat-Yhat'|): 0.8930\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4471,  E(|Y-Yhat|): 0.8640,  E(|Yhat-Yhat'|): 0.8339\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 128.6783,  E(|Y-Yhat|): 228.3666,  E(|Yhat-Yhat'|): 199.3766\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7511,  E(|Y-Yhat|): 2.2145,  E(|Yhat-Yhat'|): 0.9267\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1202,  E(|Y-Yhat|): 29.8608,  E(|Yhat-Yhat'|): 57.4811\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7896,  E(|Y-Yhat|): 50.9105,  E(|Yhat-Yhat'|): 100.2418\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8489,  E(|Y-Yhat|): 955.4449,  E(|Yhat-Yhat'|): 1909.1919\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 162.4375,  E(|Y-Yhat|): 92529.6094,  E(|Yhat-Yhat'|): 184734.3438\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1687,  E(|Y-Yhat|): 0.3067,  E(|Yhat-Yhat'|): 0.2759\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1306,  E(|Y-Yhat|): 0.2632,  E(|Yhat-Yhat'|): 0.2652\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1316,  E(|Y-Yhat|): 0.2657,  E(|Yhat-Yhat'|): 0.2681\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1301,  E(|Y-Yhat|): 0.2646,  E(|Yhat-Yhat'|): 0.2690\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3707,  E(|Y-Yhat|): 0.7705,  E(|Yhat-Yhat'|): 0.7997\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3841,  E(|Y-Yhat|): 0.6275,  E(|Yhat-Yhat'|): 0.4869\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2801,  E(|Y-Yhat|): 0.5577,  E(|Yhat-Yhat'|): 0.5551\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2902,  E(|Y-Yhat|): 0.5891,  E(|Yhat-Yhat'|): 0.5979\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2839,  E(|Y-Yhat|): 0.5789,  E(|Yhat-Yhat'|): 0.5900\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.4494,  E(|Y-Yhat|): 2.9844,  E(|Yhat-Yhat'|): 3.0701\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1790,  E(|Y-Yhat|): 0.3356,  E(|Yhat-Yhat'|): 0.3133\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1438,  E(|Y-Yhat|): 0.2839,  E(|Yhat-Yhat'|): 0.2801\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1433,  E(|Y-Yhat|): 0.2865,  E(|Yhat-Yhat'|): 0.2865\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1399,  E(|Y-Yhat|): 0.2836,  E(|Yhat-Yhat'|): 0.2875\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3599,  E(|Y-Yhat|): 0.7353,  E(|Yhat-Yhat'|): 0.7509\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6154,  E(|Y-Yhat|): 0.9998,  E(|Yhat-Yhat'|): 0.7688\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4731,  E(|Y-Yhat|): 0.9231,  E(|Yhat-Yhat'|): 0.9000\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4623,  E(|Y-Yhat|): 0.9446,  E(|Yhat-Yhat'|): 0.9645\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4789,  E(|Y-Yhat|): 0.9335,  E(|Yhat-Yhat'|): 0.9093\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2065,  E(|Y-Yhat|): 0.3633,  E(|Yhat-Yhat'|): 0.3137\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9116,  E(|Y-Yhat|): 2.2417,  E(|Yhat-Yhat'|): 0.6602\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2992,  E(|Y-Yhat|): 12.4610,  E(|Yhat-Yhat'|): 22.3235\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0658,  E(|Y-Yhat|): 67.2023,  E(|Yhat-Yhat'|): 132.2731\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.1839,  E(|Y-Yhat|): 31.5789,  E(|Yhat-Yhat'|): 60.7899\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2579,  E(|Y-Yhat|): 5.8418,  E(|Yhat-Yhat'|): 11.1678\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1747,  E(|Y-Yhat|): 0.3335,  E(|Yhat-Yhat'|): 0.3175\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1434,  E(|Y-Yhat|): 0.2877,  E(|Yhat-Yhat'|): 0.2885\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1439,  E(|Y-Yhat|): 0.2782,  E(|Yhat-Yhat'|): 0.2686\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1437,  E(|Y-Yhat|): 0.2841,  E(|Yhat-Yhat'|): 0.2808\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0515,  E(|Y-Yhat|): 0.1003,  E(|Yhat-Yhat'|): 0.0976\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4025,  E(|Y-Yhat|): 0.6512,  E(|Yhat-Yhat'|): 0.4975\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3107,  E(|Y-Yhat|): 0.6192,  E(|Yhat-Yhat'|): 0.6169\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3016,  E(|Y-Yhat|): 0.6109,  E(|Yhat-Yhat'|): 0.6186\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3088,  E(|Y-Yhat|): 0.6086,  E(|Yhat-Yhat'|): 0.5996\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0974,  E(|Y-Yhat|): 0.1861,  E(|Yhat-Yhat'|): 0.1774\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2010,  E(|Y-Yhat|): 0.3803,  E(|Yhat-Yhat'|): 0.3586\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1585,  E(|Y-Yhat|): 0.3132,  E(|Yhat-Yhat'|): 0.3095\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1575,  E(|Y-Yhat|): 0.3089,  E(|Yhat-Yhat'|): 0.3030\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1557,  E(|Y-Yhat|): 0.3086,  E(|Yhat-Yhat'|): 0.3058\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0639,  E(|Y-Yhat|): 0.1250,  E(|Yhat-Yhat'|): 0.1222\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5689,  E(|Y-Yhat|): 0.9192,  E(|Yhat-Yhat'|): 0.7006\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4602,  E(|Y-Yhat|): 0.8950,  E(|Yhat-Yhat'|): 0.8696\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4570,  E(|Y-Yhat|): 0.9094,  E(|Yhat-Yhat'|): 0.9048\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4601,  E(|Y-Yhat|): 0.9070,  E(|Yhat-Yhat'|): 0.8937\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.5353,  E(|Y-Yhat|): 2.8287,  E(|Yhat-Yhat'|): 2.5868\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9680,  E(|Y-Yhat|): 2.2691,  E(|Yhat-Yhat'|): 0.6023\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1672,  E(|Y-Yhat|): 21.6129,  E(|Yhat-Yhat'|): 40.8915\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4120,  E(|Y-Yhat|): 12.7652,  E(|Yhat-Yhat'|): 22.7064\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9775,  E(|Y-Yhat|): 37.3620,  E(|Yhat-Yhat'|): 72.7688\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.2028,  E(|Y-Yhat|): 45.3462,  E(|Yhat-Yhat'|): 86.2868\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1680,  E(|Y-Yhat|): 0.3138,  E(|Yhat-Yhat'|): 0.2916\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1341,  E(|Y-Yhat|): 0.2678,  E(|Yhat-Yhat'|): 0.2674\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1352,  E(|Y-Yhat|): 0.2741,  E(|Yhat-Yhat'|): 0.2780\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1339,  E(|Y-Yhat|): 0.2716,  E(|Yhat-Yhat'|): 0.2755\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1439,  E(|Y-Yhat|): 0.2896,  E(|Yhat-Yhat'|): 0.2913\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3843,  E(|Y-Yhat|): 0.6280,  E(|Yhat-Yhat'|): 0.4874\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2991,  E(|Y-Yhat|): 0.5838,  E(|Yhat-Yhat'|): 0.5694\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2873,  E(|Y-Yhat|): 0.5775,  E(|Yhat-Yhat'|): 0.5803\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2957,  E(|Y-Yhat|): 0.5926,  E(|Yhat-Yhat'|): 0.5938\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3187,  E(|Y-Yhat|): 0.6270,  E(|Yhat-Yhat'|): 0.6164\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2084,  E(|Y-Yhat|): 0.3923,  E(|Yhat-Yhat'|): 0.3677\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1470,  E(|Y-Yhat|): 0.2994,  E(|Yhat-Yhat'|): 0.3047\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1463,  E(|Y-Yhat|): 0.2923,  E(|Yhat-Yhat'|): 0.2920\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1472,  E(|Y-Yhat|): 0.2983,  E(|Yhat-Yhat'|): 0.3021\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1543,  E(|Y-Yhat|): 0.3137,  E(|Yhat-Yhat'|): 0.3188\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6770,  E(|Y-Yhat|): 1.1009,  E(|Yhat-Yhat'|): 0.8478\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5336,  E(|Y-Yhat|): 1.0302,  E(|Yhat-Yhat'|): 0.9933\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5335,  E(|Y-Yhat|): 1.0550,  E(|Yhat-Yhat'|): 1.0430\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5343,  E(|Y-Yhat|): 1.0598,  E(|Yhat-Yhat'|): 1.0511\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1197,  E(|Y-Yhat|): 0.2242,  E(|Yhat-Yhat'|): 0.2090\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8836,  E(|Y-Yhat|): 2.2222,  E(|Yhat-Yhat'|): 0.6771\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3545,  E(|Y-Yhat|): 23.2675,  E(|Yhat-Yhat'|): 43.8260\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3433,  E(|Y-Yhat|): 11.3306,  E(|Yhat-Yhat'|): 19.9747\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3485,  E(|Y-Yhat|): 25.4924,  E(|Yhat-Yhat'|): 48.2877\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1313,  E(|Y-Yhat|): 2.3281,  E(|Yhat-Yhat'|): 4.3935\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2061,  E(|Y-Yhat|): 0.3683,  E(|Yhat-Yhat'|): 0.3243\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1671,  E(|Y-Yhat|): 0.3286,  E(|Yhat-Yhat'|): 0.3230\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1690,  E(|Y-Yhat|): 0.3347,  E(|Yhat-Yhat'|): 0.3315\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1666,  E(|Y-Yhat|): 0.3317,  E(|Yhat-Yhat'|): 0.3302\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0405,  E(|Y-Yhat|): 0.0755,  E(|Yhat-Yhat'|): 0.0699\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4432,  E(|Y-Yhat|): 0.7409,  E(|Yhat-Yhat'|): 0.5954\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3604,  E(|Y-Yhat|): 0.7024,  E(|Yhat-Yhat'|): 0.6840\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3535,  E(|Y-Yhat|): 0.7042,  E(|Yhat-Yhat'|): 0.7014\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3511,  E(|Y-Yhat|): 0.6973,  E(|Yhat-Yhat'|): 0.6924\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0753,  E(|Y-Yhat|): 0.1463,  E(|Yhat-Yhat'|): 0.1419\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2192,  E(|Y-Yhat|): 0.4108,  E(|Yhat-Yhat'|): 0.3831\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1844,  E(|Y-Yhat|): 0.3642,  E(|Yhat-Yhat'|): 0.3597\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1849,  E(|Y-Yhat|): 0.3639,  E(|Yhat-Yhat'|): 0.3580\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1814,  E(|Y-Yhat|): 0.3664,  E(|Yhat-Yhat'|): 0.3700\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0558,  E(|Y-Yhat|): 0.1099,  E(|Yhat-Yhat'|): 0.1082\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6076,  E(|Y-Yhat|): 0.9662,  E(|Yhat-Yhat'|): 0.7172\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4580,  E(|Y-Yhat|): 0.8962,  E(|Yhat-Yhat'|): 0.8763\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4446,  E(|Y-Yhat|): 0.8833,  E(|Yhat-Yhat'|): 0.8775\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4455,  E(|Y-Yhat|): 0.8854,  E(|Yhat-Yhat'|): 0.8798\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 273.4949,  E(|Y-Yhat|): 482.9874,  E(|Yhat-Yhat'|): 418.9850\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7514,  E(|Y-Yhat|): 2.1285,  E(|Yhat-Yhat'|): 0.7542\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.1161,  E(|Y-Yhat|): 39.1807,  E(|Yhat-Yhat'|): 74.1291\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4743,  E(|Y-Yhat|): 7.5499,  E(|Yhat-Yhat'|): 12.1513\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.5493,  E(|Y-Yhat|): 16.8752,  E(|Yhat-Yhat'|): 30.6518\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 201.3171,  E(|Y-Yhat|): 3612.8291,  E(|Yhat-Yhat'|): 6823.0239\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1599,  E(|Y-Yhat|): 0.3038,  E(|Yhat-Yhat'|): 0.2877\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1308,  E(|Y-Yhat|): 0.2693,  E(|Yhat-Yhat'|): 0.2769\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1336,  E(|Y-Yhat|): 0.2669,  E(|Yhat-Yhat'|): 0.2667\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1329,  E(|Y-Yhat|): 0.2699,  E(|Yhat-Yhat'|): 0.2740\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4168,  E(|Y-Yhat|): 0.8628,  E(|Yhat-Yhat'|): 0.8921\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4008,  E(|Y-Yhat|): 0.6764,  E(|Yhat-Yhat'|): 0.5512\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2865,  E(|Y-Yhat|): 0.5719,  E(|Yhat-Yhat'|): 0.5707\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2982,  E(|Y-Yhat|): 0.5810,  E(|Yhat-Yhat'|): 0.5655\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2939,  E(|Y-Yhat|): 0.5726,  E(|Yhat-Yhat'|): 0.5573\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8220,  E(|Y-Yhat|): 3.6033,  E(|Yhat-Yhat'|): 3.5626\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2022,  E(|Y-Yhat|): 0.3827,  E(|Yhat-Yhat'|): 0.3609\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1473,  E(|Y-Yhat|): 0.2983,  E(|Yhat-Yhat'|): 0.3022\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1450,  E(|Y-Yhat|): 0.2903,  E(|Yhat-Yhat'|): 0.2905\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1517,  E(|Y-Yhat|): 0.2973,  E(|Yhat-Yhat'|): 0.2913\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4088,  E(|Y-Yhat|): 0.8319,  E(|Yhat-Yhat'|): 0.8462\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6536,  E(|Y-Yhat|): 1.0877,  E(|Yhat-Yhat'|): 0.8682\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5226,  E(|Y-Yhat|): 1.0138,  E(|Yhat-Yhat'|): 0.9826\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5298,  E(|Y-Yhat|): 1.0338,  E(|Yhat-Yhat'|): 1.0080\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5188,  E(|Y-Yhat|): 1.0286,  E(|Yhat-Yhat'|): 1.0196\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.5907,  E(|Y-Yhat|): 2.8752,  E(|Yhat-Yhat'|): 2.5689\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8458,  E(|Y-Yhat|): 2.2482,  E(|Yhat-Yhat'|): 0.8049\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2485,  E(|Y-Yhat|): 19.1295,  E(|Yhat-Yhat'|): 35.7619\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2757,  E(|Y-Yhat|): 11.3162,  E(|Yhat-Yhat'|): 20.0809\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4084,  E(|Y-Yhat|): 16.5539,  E(|Yhat-Yhat'|): 30.2909\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8918,  E(|Y-Yhat|): 15.9190,  E(|Yhat-Yhat'|): 28.0543\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2076,  E(|Y-Yhat|): 0.3787,  E(|Yhat-Yhat'|): 0.3422\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1698,  E(|Y-Yhat|): 0.3338,  E(|Yhat-Yhat'|): 0.3280\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1611,  E(|Y-Yhat|): 0.3308,  E(|Yhat-Yhat'|): 0.3394\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1625,  E(|Y-Yhat|): 0.3283,  E(|Yhat-Yhat'|): 0.3316\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1712,  E(|Y-Yhat|): 0.3410,  E(|Yhat-Yhat'|): 0.3396\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4600,  E(|Y-Yhat|): 0.7605,  E(|Yhat-Yhat'|): 0.6010\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3471,  E(|Y-Yhat|): 0.6759,  E(|Yhat-Yhat'|): 0.6576\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3451,  E(|Y-Yhat|): 0.6783,  E(|Yhat-Yhat'|): 0.6663\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3405,  E(|Y-Yhat|): 0.7060,  E(|Yhat-Yhat'|): 0.7310\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3605,  E(|Y-Yhat|): 0.7426,  E(|Yhat-Yhat'|): 0.7642\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2290,  E(|Y-Yhat|): 0.4149,  E(|Yhat-Yhat'|): 0.3717\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1830,  E(|Y-Yhat|): 0.3618,  E(|Yhat-Yhat'|): 0.3576\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1803,  E(|Y-Yhat|): 0.3642,  E(|Yhat-Yhat'|): 0.3678\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1806,  E(|Y-Yhat|): 0.3635,  E(|Yhat-Yhat'|): 0.3658\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1857,  E(|Y-Yhat|): 0.3713,  E(|Yhat-Yhat'|): 0.3712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5815,  E(|Y-Yhat|): 0.9075,  E(|Yhat-Yhat'|): 0.6521\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4318,  E(|Y-Yhat|): 0.8521,  E(|Yhat-Yhat'|): 0.8407\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4250,  E(|Y-Yhat|): 0.8570,  E(|Yhat-Yhat'|): 0.8640\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4273,  E(|Y-Yhat|): 0.8725,  E(|Yhat-Yhat'|): 0.8903\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 284.7857,  E(|Y-Yhat|): 550.5109,  E(|Yhat-Yhat'|): 531.4504\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9398,  E(|Y-Yhat|): 2.2847,  E(|Yhat-Yhat'|): 0.6897\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.5853,  E(|Y-Yhat|): 20.7026,  E(|Yhat-Yhat'|): 38.2344\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3240,  E(|Y-Yhat|): 7.7144,  E(|Yhat-Yhat'|): 12.7808\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3166,  E(|Y-Yhat|): 5.8863,  E(|Yhat-Yhat'|): 9.1394\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 311.6725,  E(|Y-Yhat|): 1324.6552,  E(|Yhat-Yhat'|): 2025.9652\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1570,  E(|Y-Yhat|): 0.2995,  E(|Yhat-Yhat'|): 0.2850\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1254,  E(|Y-Yhat|): 0.2497,  E(|Yhat-Yhat'|): 0.2486\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1278,  E(|Y-Yhat|): 0.2444,  E(|Yhat-Yhat'|): 0.2332\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1248,  E(|Y-Yhat|): 0.2572,  E(|Yhat-Yhat'|): 0.2648\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4011,  E(|Y-Yhat|): 0.8330,  E(|Yhat-Yhat'|): 0.8638\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3687,  E(|Y-Yhat|): 0.6294,  E(|Yhat-Yhat'|): 0.5212\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2729,  E(|Y-Yhat|): 0.5420,  E(|Yhat-Yhat'|): 0.5382\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2762,  E(|Y-Yhat|): 0.5453,  E(|Yhat-Yhat'|): 0.5382\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2791,  E(|Y-Yhat|): 0.5431,  E(|Yhat-Yhat'|): 0.5281\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.7334,  E(|Y-Yhat|): 3.5006,  E(|Yhat-Yhat'|): 3.5343\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1788,  E(|Y-Yhat|): 0.3422,  E(|Yhat-Yhat'|): 0.3269\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1406,  E(|Y-Yhat|): 0.2773,  E(|Yhat-Yhat'|): 0.2735\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1382,  E(|Y-Yhat|): 0.2732,  E(|Yhat-Yhat'|): 0.2701\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1380,  E(|Y-Yhat|): 0.2721,  E(|Yhat-Yhat'|): 0.2683\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4000,  E(|Y-Yhat|): 0.8166,  E(|Yhat-Yhat'|): 0.8332\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5436,  E(|Y-Yhat|): 0.8921,  E(|Yhat-Yhat'|): 0.6969\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4260,  E(|Y-Yhat|): 0.8420,  E(|Yhat-Yhat'|): 0.8320\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4212,  E(|Y-Yhat|): 0.8434,  E(|Yhat-Yhat'|): 0.8444\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4234,  E(|Y-Yhat|): 0.8409,  E(|Yhat-Yhat'|): 0.8351\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 18.7340,  E(|Y-Yhat|): 34.9953,  E(|Yhat-Yhat'|): 32.5227\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7868,  E(|Y-Yhat|): 2.0896,  E(|Yhat-Yhat'|): 0.6056\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3523,  E(|Y-Yhat|): 26.3642,  E(|Yhat-Yhat'|): 50.0238\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3620,  E(|Y-Yhat|): 7.1943,  E(|Yhat-Yhat'|): 11.6647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2729,  E(|Y-Yhat|): 10.8318,  E(|Yhat-Yhat'|): 19.1179\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 21.2425,  E(|Y-Yhat|): 160.2200,  E(|Yhat-Yhat'|): 277.9549\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1621,  E(|Y-Yhat|): 0.3097,  E(|Yhat-Yhat'|): 0.2953\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1236,  E(|Y-Yhat|): 0.2526,  E(|Yhat-Yhat'|): 0.2581\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1206,  E(|Y-Yhat|): 0.2451,  E(|Yhat-Yhat'|): 0.2491\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1222,  E(|Y-Yhat|): 0.2425,  E(|Yhat-Yhat'|): 0.2406\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2746,  E(|Y-Yhat|): 0.5723,  E(|Yhat-Yhat'|): 0.5955\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4002,  E(|Y-Yhat|): 0.6919,  E(|Yhat-Yhat'|): 0.5832\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2634,  E(|Y-Yhat|): 0.5309,  E(|Yhat-Yhat'|): 0.5348\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2745,  E(|Y-Yhat|): 0.5538,  E(|Yhat-Yhat'|): 0.5584\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2656,  E(|Y-Yhat|): 0.5388,  E(|Yhat-Yhat'|): 0.5464\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8331,  E(|Y-Yhat|): 1.6878,  E(|Yhat-Yhat'|): 1.7095\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1772,  E(|Y-Yhat|): 0.3438,  E(|Yhat-Yhat'|): 0.3333\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1378,  E(|Y-Yhat|): 0.2759,  E(|Yhat-Yhat'|): 0.2762\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1357,  E(|Y-Yhat|): 0.2738,  E(|Yhat-Yhat'|): 0.2762\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1361,  E(|Y-Yhat|): 0.2666,  E(|Yhat-Yhat'|): 0.2609\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2753,  E(|Y-Yhat|): 0.5581,  E(|Yhat-Yhat'|): 0.5655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5780,  E(|Y-Yhat|): 0.9319,  E(|Yhat-Yhat'|): 0.7079\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4402,  E(|Y-Yhat|): 0.8675,  E(|Yhat-Yhat'|): 0.8547\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4514,  E(|Y-Yhat|): 0.8838,  E(|Yhat-Yhat'|): 0.8647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4365,  E(|Y-Yhat|): 0.8661,  E(|Yhat-Yhat'|): 0.8593\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.4374,  E(|Y-Yhat|): 2.5152,  E(|Yhat-Yhat'|): 2.1557\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9591,  E(|Y-Yhat|): 2.3298,  E(|Yhat-Yhat'|): 0.7415\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2985,  E(|Y-Yhat|): 23.6720,  E(|Yhat-Yhat'|): 44.7469\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4860,  E(|Y-Yhat|): 40.3613,  E(|Yhat-Yhat'|): 77.7507\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.3160,  E(|Y-Yhat|): 155.3933,  E(|Yhat-Yhat'|): 306.1547\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.5891,  E(|Y-Yhat|): 219.7523,  E(|Yhat-Yhat'|): 430.3264\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1708,  E(|Y-Yhat|): 0.3192,  E(|Yhat-Yhat'|): 0.2968\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1274,  E(|Y-Yhat|): 0.2507,  E(|Yhat-Yhat'|): 0.2465\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1246,  E(|Y-Yhat|): 0.2587,  E(|Yhat-Yhat'|): 0.2681\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1279,  E(|Y-Yhat|): 0.2497,  E(|Yhat-Yhat'|): 0.2435\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1275,  E(|Y-Yhat|): 0.2537,  E(|Yhat-Yhat'|): 0.2524\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3419,  E(|Y-Yhat|): 0.5850,  E(|Yhat-Yhat'|): 0.4863\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2750,  E(|Y-Yhat|): 0.5407,  E(|Yhat-Yhat'|): 0.5314\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2792,  E(|Y-Yhat|): 0.5541,  E(|Yhat-Yhat'|): 0.5497\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2839,  E(|Y-Yhat|): 0.5518,  E(|Yhat-Yhat'|): 0.5359\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2721,  E(|Y-Yhat|): 0.5333,  E(|Yhat-Yhat'|): 0.5225\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1908,  E(|Y-Yhat|): 0.3705,  E(|Yhat-Yhat'|): 0.3593\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1422,  E(|Y-Yhat|): 0.2811,  E(|Yhat-Yhat'|): 0.2779\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1428,  E(|Y-Yhat|): 0.2759,  E(|Yhat-Yhat'|): 0.2661\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1389,  E(|Y-Yhat|): 0.2783,  E(|Yhat-Yhat'|): 0.2789\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1401,  E(|Y-Yhat|): 0.2800,  E(|Yhat-Yhat'|): 0.2798\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5824,  E(|Y-Yhat|): 0.9272,  E(|Yhat-Yhat'|): 0.6895\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4571,  E(|Y-Yhat|): 0.8688,  E(|Yhat-Yhat'|): 0.8234\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4470,  E(|Y-Yhat|): 0.8691,  E(|Yhat-Yhat'|): 0.8442\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4286,  E(|Y-Yhat|): 0.8792,  E(|Yhat-Yhat'|): 0.9012\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 65.5441,  E(|Y-Yhat|): 118.4884,  E(|Yhat-Yhat'|): 105.8886\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9157,  E(|Y-Yhat|): 2.1724,  E(|Yhat-Yhat'|): 0.5134\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.5213,  E(|Y-Yhat|): 13.9692,  E(|Yhat-Yhat'|): 24.8956\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5030,  E(|Y-Yhat|): 5.5207,  E(|Yhat-Yhat'|): 8.0353\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4030,  E(|Y-Yhat|): 6.4707,  E(|Yhat-Yhat'|): 10.1354\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 63.2065,  E(|Y-Yhat|): 280.5665,  E(|Yhat-Yhat'|): 434.7201\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1760,  E(|Y-Yhat|): 0.3326,  E(|Yhat-Yhat'|): 0.3131\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1323,  E(|Y-Yhat|): 0.2613,  E(|Yhat-Yhat'|): 0.2580\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1299,  E(|Y-Yhat|): 0.2611,  E(|Yhat-Yhat'|): 0.2624\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1308,  E(|Y-Yhat|): 0.2557,  E(|Yhat-Yhat'|): 0.2497\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3332,  E(|Y-Yhat|): 0.6944,  E(|Yhat-Yhat'|): 0.7223\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3878,  E(|Y-Yhat|): 0.6456,  E(|Yhat-Yhat'|): 0.5157\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2867,  E(|Y-Yhat|): 0.5568,  E(|Yhat-Yhat'|): 0.5402\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2856,  E(|Y-Yhat|): 0.5693,  E(|Yhat-Yhat'|): 0.5674\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2860,  E(|Y-Yhat|): 0.5523,  E(|Yhat-Yhat'|): 0.5326\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2085,  E(|Y-Yhat|): 2.4081,  E(|Yhat-Yhat'|): 2.3991\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1820,  E(|Y-Yhat|): 0.3426,  E(|Yhat-Yhat'|): 0.3212\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1408,  E(|Y-Yhat|): 0.2873,  E(|Yhat-Yhat'|): 0.2931\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1409,  E(|Y-Yhat|): 0.2808,  E(|Yhat-Yhat'|): 0.2797\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1419,  E(|Y-Yhat|): 0.2824,  E(|Yhat-Yhat'|): 0.2809\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3287,  E(|Y-Yhat|): 0.6621,  E(|Yhat-Yhat'|): 0.6668\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0642),\n",
       " tensor(31.3913),\n",
       " tensor(0.0843),\n",
       " tensor(0.0850),\n",
       " tensor(0.0765))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"cubic\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = Z ** 3 / 3.0   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ecd74c",
   "metadata": {},
   "source": [
    "## True function: square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdc3c454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6562,  E(|Y-Yhat|): 1.0828,  E(|Yhat-Yhat'|): 0.8533\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5225,  E(|Y-Yhat|): 1.0249,  E(|Yhat-Yhat'|): 1.0049\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5195,  E(|Y-Yhat|): 1.0183,  E(|Yhat-Yhat'|): 0.9975\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5013,  E(|Y-Yhat|): 1.0047,  E(|Yhat-Yhat'|): 1.0067\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 8.8141,  E(|Y-Yhat|): 17.5382,  E(|Yhat-Yhat'|): 17.4481\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9034,  E(|Y-Yhat|): 2.3147,  E(|Yhat-Yhat'|): 0.8226\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4947,  E(|Y-Yhat|): 12.0323,  E(|Yhat-Yhat'|): 21.0752\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4190,  E(|Y-Yhat|): 44.5684,  E(|Yhat-Yhat'|): 86.2988\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8612,  E(|Y-Yhat|): 89.8940,  E(|Yhat-Yhat'|): 178.0655\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 6.4822,  E(|Y-Yhat|): 548.0392,  E(|Yhat-Yhat'|): 1083.1140\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2287,  E(|Y-Yhat|): 0.3965,  E(|Yhat-Yhat'|): 0.3356\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1524,  E(|Y-Yhat|): 0.3044,  E(|Yhat-Yhat'|): 0.3039\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1521,  E(|Y-Yhat|): 0.3052,  E(|Yhat-Yhat'|): 0.3063\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1540,  E(|Y-Yhat|): 0.3086,  E(|Yhat-Yhat'|): 0.3091\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2719,  E(|Y-Yhat|): 0.5617,  E(|Yhat-Yhat'|): 0.5795\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4388,  E(|Y-Yhat|): 0.7159,  E(|Yhat-Yhat'|): 0.5542\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3366,  E(|Y-Yhat|): 0.6885,  E(|Yhat-Yhat'|): 0.7039\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3299,  E(|Y-Yhat|): 0.6579,  E(|Yhat-Yhat'|): 0.6560\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3236,  E(|Y-Yhat|): 0.6623,  E(|Yhat-Yhat'|): 0.6774\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7403,  E(|Y-Yhat|): 1.5037,  E(|Yhat-Yhat'|): 1.5268\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2322,  E(|Y-Yhat|): 0.4297,  E(|Yhat-Yhat'|): 0.3949\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1679,  E(|Y-Yhat|): 0.3346,  E(|Yhat-Yhat'|): 0.3334\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1663,  E(|Y-Yhat|): 0.3292,  E(|Yhat-Yhat'|): 0.3258\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1675,  E(|Y-Yhat|): 0.3395,  E(|Yhat-Yhat'|): 0.3440\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2728,  E(|Y-Yhat|): 0.5610,  E(|Yhat-Yhat'|): 0.5764\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6536,  E(|Y-Yhat|): 1.0685,  E(|Yhat-Yhat'|): 0.8298\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4330,  E(|Y-Yhat|): 0.8715,  E(|Yhat-Yhat'|): 0.8770\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4338,  E(|Y-Yhat|): 0.8608,  E(|Yhat-Yhat'|): 0.8540\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4311,  E(|Y-Yhat|): 0.8564,  E(|Yhat-Yhat'|): 0.8506\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0734,  E(|Y-Yhat|): 0.1456,  E(|Yhat-Yhat'|): 0.1443\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9118,  E(|Y-Yhat|): 2.2224,  E(|Yhat-Yhat'|): 0.6213\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2880,  E(|Y-Yhat|): 5.3347,  E(|Yhat-Yhat'|): 8.0934\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.1814,  E(|Y-Yhat|): 4.7120,  E(|Yhat-Yhat'|): 7.0612\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2858,  E(|Y-Yhat|): 4.0671,  E(|Yhat-Yhat'|): 5.5626\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0787,  E(|Y-Yhat|): 0.2177,  E(|Yhat-Yhat'|): 0.2779\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1937,  E(|Y-Yhat|): 0.3595,  E(|Yhat-Yhat'|): 0.3317\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1294,  E(|Y-Yhat|): 0.2642,  E(|Yhat-Yhat'|): 0.2696\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1309,  E(|Y-Yhat|): 0.2637,  E(|Yhat-Yhat'|): 0.2655\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1291,  E(|Y-Yhat|): 0.2666,  E(|Yhat-Yhat'|): 0.2750\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0264,  E(|Y-Yhat|): 0.0541,  E(|Yhat-Yhat'|): 0.0555\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4679,  E(|Y-Yhat|): 0.7558,  E(|Yhat-Yhat'|): 0.5758\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2854,  E(|Y-Yhat|): 0.5839,  E(|Yhat-Yhat'|): 0.5969\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2936,  E(|Y-Yhat|): 0.5864,  E(|Yhat-Yhat'|): 0.5856\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2868,  E(|Y-Yhat|): 0.5800,  E(|Yhat-Yhat'|): 0.5863\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0497,  E(|Y-Yhat|): 0.0977,  E(|Yhat-Yhat'|): 0.0960\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2419,  E(|Y-Yhat|): 0.4479,  E(|Yhat-Yhat'|): 0.4119\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1393,  E(|Y-Yhat|): 0.2850,  E(|Yhat-Yhat'|): 0.2914\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1371,  E(|Y-Yhat|): 0.2779,  E(|Yhat-Yhat'|): 0.2815\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1391,  E(|Y-Yhat|): 0.2793,  E(|Yhat-Yhat'|): 0.2803\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0375,  E(|Y-Yhat|): 0.0708,  E(|Yhat-Yhat'|): 0.0666\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6435,  E(|Y-Yhat|): 1.0575,  E(|Yhat-Yhat'|): 0.8281\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4466,  E(|Y-Yhat|): 0.8685,  E(|Yhat-Yhat'|): 0.8438\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4230,  E(|Y-Yhat|): 0.8619,  E(|Yhat-Yhat'|): 0.8779\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4249,  E(|Y-Yhat|): 0.8460,  E(|Yhat-Yhat'|): 0.8421\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1973,  E(|Y-Yhat|): 0.4016,  E(|Yhat-Yhat'|): 0.4086\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9832,  E(|Y-Yhat|): 2.2786,  E(|Yhat-Yhat'|): 0.5909\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1945,  E(|Y-Yhat|): 8.2641,  E(|Yhat-Yhat'|): 14.1393\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0476,  E(|Y-Yhat|): 24.3459,  E(|Yhat-Yhat'|): 46.5968\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3396,  E(|Y-Yhat|): 7.7138,  E(|Yhat-Yhat'|): 12.7483\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2105,  E(|Y-Yhat|): 1.2155,  E(|Yhat-Yhat'|): 2.0099\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1969,  E(|Y-Yhat|): 0.3583,  E(|Yhat-Yhat'|): 0.3228\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1310,  E(|Y-Yhat|): 0.2675,  E(|Yhat-Yhat'|): 0.2731\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1277,  E(|Y-Yhat|): 0.2600,  E(|Yhat-Yhat'|): 0.2645\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1296,  E(|Y-Yhat|): 0.2604,  E(|Yhat-Yhat'|): 0.2616\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0530,  E(|Y-Yhat|): 0.1042,  E(|Yhat-Yhat'|): 0.1023\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4514,  E(|Y-Yhat|): 0.7353,  E(|Yhat-Yhat'|): 0.5679\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2854,  E(|Y-Yhat|): 0.5795,  E(|Yhat-Yhat'|): 0.5883\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2884,  E(|Y-Yhat|): 0.5817,  E(|Yhat-Yhat'|): 0.5867\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2783,  E(|Y-Yhat|): 0.5732,  E(|Yhat-Yhat'|): 0.5899\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0984,  E(|Y-Yhat|): 0.2038,  E(|Yhat-Yhat'|): 0.2106\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2559,  E(|Y-Yhat|): 0.4672,  E(|Yhat-Yhat'|): 0.4226\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1398,  E(|Y-Yhat|): 0.2803,  E(|Yhat-Yhat'|): 0.2809\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1359,  E(|Y-Yhat|): 0.2772,  E(|Yhat-Yhat'|): 0.2827\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1366,  E(|Y-Yhat|): 0.2749,  E(|Yhat-Yhat'|): 0.2767\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0647,  E(|Y-Yhat|): 0.1287,  E(|Yhat-Yhat'|): 0.1281\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7236,  E(|Y-Yhat|): 1.1929,  E(|Yhat-Yhat'|): 0.9386\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4825,  E(|Y-Yhat|): 0.9732,  E(|Yhat-Yhat'|): 0.9815\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4718,  E(|Y-Yhat|): 0.9517,  E(|Yhat-Yhat'|): 0.9599\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4805,  E(|Y-Yhat|): 0.9415,  E(|Yhat-Yhat'|): 0.9219\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0525,  E(|Y-Yhat|): 0.0995,  E(|Yhat-Yhat'|): 0.0938\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8807,  E(|Y-Yhat|): 2.2359,  E(|Yhat-Yhat'|): 0.7104\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0440,  E(|Y-Yhat|): 37.9581,  E(|Yhat-Yhat'|): 73.8283\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2729,  E(|Y-Yhat|): 16.3971,  E(|Yhat-Yhat'|): 30.2485\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.7816,  E(|Y-Yhat|): 16.1595,  E(|Yhat-Yhat'|): 28.7559\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0566,  E(|Y-Yhat|): 0.5990,  E(|Yhat-Yhat'|): 1.0848\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2237,  E(|Y-Yhat|): 0.3860,  E(|Yhat-Yhat'|): 0.3248\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1509,  E(|Y-Yhat|): 0.3070,  E(|Yhat-Yhat'|): 0.3122\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1473,  E(|Y-Yhat|): 0.3056,  E(|Yhat-Yhat'|): 0.3166\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1499,  E(|Y-Yhat|): 0.3037,  E(|Yhat-Yhat'|): 0.3077\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0214,  E(|Y-Yhat|): 0.0423,  E(|Yhat-Yhat'|): 0.0419\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4957,  E(|Y-Yhat|): 0.8125,  E(|Yhat-Yhat'|): 0.6336\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3243,  E(|Y-Yhat|): 0.6659,  E(|Yhat-Yhat'|): 0.6830\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3265,  E(|Y-Yhat|): 0.6404,  E(|Yhat-Yhat'|): 0.6279\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3232,  E(|Y-Yhat|): 0.6601,  E(|Yhat-Yhat'|): 0.6738\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0382,  E(|Y-Yhat|): 0.0796,  E(|Yhat-Yhat'|): 0.0829\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2568,  E(|Y-Yhat|): 0.4728,  E(|Yhat-Yhat'|): 0.4320\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1623,  E(|Y-Yhat|): 0.3308,  E(|Yhat-Yhat'|): 0.3370\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1640,  E(|Y-Yhat|): 0.3284,  E(|Yhat-Yhat'|): 0.3289\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1604,  E(|Y-Yhat|): 0.3200,  E(|Yhat-Yhat'|): 0.3193\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0322,  E(|Y-Yhat|): 0.0606,  E(|Yhat-Yhat'|): 0.0570\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7349,  E(|Y-Yhat|): 1.1563,  E(|Yhat-Yhat'|): 0.8430\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5081,  E(|Y-Yhat|): 0.9964,  E(|Yhat-Yhat'|): 0.9765\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5029,  E(|Y-Yhat|): 1.0157,  E(|Yhat-Yhat'|): 1.0256\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4984,  E(|Y-Yhat|): 0.9913,  E(|Yhat-Yhat'|): 0.9857\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 7.1620,  E(|Y-Yhat|): 13.7432,  E(|Yhat-Yhat'|): 13.1625\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7390,  E(|Y-Yhat|): 2.1218,  E(|Yhat-Yhat'|): 0.7657\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9017,  E(|Y-Yhat|): 100.0737,  E(|Yhat-Yhat'|): 198.3440\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4048,  E(|Y-Yhat|): 65.3742,  E(|Yhat-Yhat'|): 127.9388\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 3.1773,  E(|Y-Yhat|): 88.4332,  E(|Yhat-Yhat'|): 170.5118\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3330,  E(|Y-Yhat|): 615.2193,  E(|Yhat-Yhat'|): 1229.7726\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2225,  E(|Y-Yhat|): 0.3981,  E(|Yhat-Yhat'|): 0.3512\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1659,  E(|Y-Yhat|): 0.3378,  E(|Yhat-Yhat'|): 0.3438\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1632,  E(|Y-Yhat|): 0.3360,  E(|Yhat-Yhat'|): 0.3454\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1676,  E(|Y-Yhat|): 0.3313,  E(|Yhat-Yhat'|): 0.3274\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2924,  E(|Y-Yhat|): 0.5989,  E(|Yhat-Yhat'|): 0.6129\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4871,  E(|Y-Yhat|): 0.8189,  E(|Yhat-Yhat'|): 0.6636\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3366,  E(|Y-Yhat|): 0.6888,  E(|Yhat-Yhat'|): 0.7044\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3445,  E(|Y-Yhat|): 0.6790,  E(|Yhat-Yhat'|): 0.6692\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3481,  E(|Y-Yhat|): 0.6756,  E(|Yhat-Yhat'|): 0.6550\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7951,  E(|Y-Yhat|): 1.5638,  E(|Yhat-Yhat'|): 1.5374\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2560,  E(|Y-Yhat|): 0.4676,  E(|Yhat-Yhat'|): 0.4232\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1848,  E(|Y-Yhat|): 0.3721,  E(|Yhat-Yhat'|): 0.3747\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1793,  E(|Y-Yhat|): 0.3622,  E(|Yhat-Yhat'|): 0.3657\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1845,  E(|Y-Yhat|): 0.3664,  E(|Yhat-Yhat'|): 0.3640\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2946,  E(|Y-Yhat|): 0.5971,  E(|Yhat-Yhat'|): 0.6052\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7000,  E(|Y-Yhat|): 1.1719,  E(|Yhat-Yhat'|): 0.9438\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5135,  E(|Y-Yhat|): 1.0196,  E(|Yhat-Yhat'|): 1.0123\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5186,  E(|Y-Yhat|): 1.0286,  E(|Yhat-Yhat'|): 1.0200\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5139,  E(|Y-Yhat|): 1.0125,  E(|Yhat-Yhat'|): 0.9973\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4007,  E(|Y-Yhat|): 0.7651,  E(|Yhat-Yhat'|): 0.7287\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8762,  E(|Y-Yhat|): 2.2752,  E(|Yhat-Yhat'|): 0.7981\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1639,  E(|Y-Yhat|): 9.7463,  E(|Yhat-Yhat'|): 17.1647\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2407,  E(|Y-Yhat|): 3.9880,  E(|Yhat-Yhat'|): 5.4945\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.1836,  E(|Y-Yhat|): 7.6912,  E(|Yhat-Yhat'|): 13.0151\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3517,  E(|Y-Yhat|): 2.3472,  E(|Yhat-Yhat'|): 3.9911\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2336,  E(|Y-Yhat|): 0.4125,  E(|Yhat-Yhat'|): 0.3579\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1662,  E(|Y-Yhat|): 0.3357,  E(|Yhat-Yhat'|): 0.3390\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1671,  E(|Y-Yhat|): 0.3359,  E(|Yhat-Yhat'|): 0.3377\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1648,  E(|Y-Yhat|): 0.3305,  E(|Yhat-Yhat'|): 0.3313\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0891,  E(|Y-Yhat|): 0.1751,  E(|Yhat-Yhat'|): 0.1721\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5133,  E(|Y-Yhat|): 0.8433,  E(|Yhat-Yhat'|): 0.6599\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3461,  E(|Y-Yhat|): 0.7091,  E(|Yhat-Yhat'|): 0.7260\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3430,  E(|Y-Yhat|): 0.7037,  E(|Yhat-Yhat'|): 0.7213\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3447,  E(|Y-Yhat|): 0.7047,  E(|Yhat-Yhat'|): 0.7200\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1677,  E(|Y-Yhat|): 0.3421,  E(|Yhat-Yhat'|): 0.3487\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2402,  E(|Y-Yhat|): 0.4280,  E(|Yhat-Yhat'|): 0.3756\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1792,  E(|Y-Yhat|): 0.3574,  E(|Yhat-Yhat'|): 0.3563\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1785,  E(|Y-Yhat|): 0.3586,  E(|Yhat-Yhat'|): 0.3601\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1790,  E(|Y-Yhat|): 0.3606,  E(|Yhat-Yhat'|): 0.3632\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1088,  E(|Y-Yhat|): 0.2152,  E(|Yhat-Yhat'|): 0.2128\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6423,  E(|Y-Yhat|): 1.0287,  E(|Yhat-Yhat'|): 0.7729\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4057,  E(|Y-Yhat|): 0.8029,  E(|Yhat-Yhat'|): 0.7945\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3921,  E(|Y-Yhat|): 0.8039,  E(|Yhat-Yhat'|): 0.8237\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3974,  E(|Y-Yhat|): 0.8128,  E(|Yhat-Yhat'|): 0.8307\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 10.4885,  E(|Y-Yhat|): 20.7642,  E(|Yhat-Yhat'|): 20.5514\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9334,  E(|Y-Yhat|): 2.2890,  E(|Yhat-Yhat'|): 0.7112\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4905,  E(|Y-Yhat|): 16.2221,  E(|Yhat-Yhat'|): 29.4631\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2895,  E(|Y-Yhat|): 52.1844,  E(|Yhat-Yhat'|): 101.7898\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9626,  E(|Y-Yhat|): 22.9812,  E(|Yhat-Yhat'|): 44.0374\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 11.8271,  E(|Y-Yhat|): 205.1014,  E(|Yhat-Yhat'|): 386.5486\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1665,  E(|Y-Yhat|): 0.3129,  E(|Yhat-Yhat'|): 0.2929\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1190,  E(|Y-Yhat|): 0.2475,  E(|Yhat-Yhat'|): 0.2569\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1198,  E(|Y-Yhat|): 0.2378,  E(|Yhat-Yhat'|): 0.2359\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1174,  E(|Y-Yhat|): 0.2371,  E(|Yhat-Yhat'|): 0.2394\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2042,  E(|Y-Yhat|): 0.4348,  E(|Yhat-Yhat'|): 0.4612\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3683,  E(|Y-Yhat|): 0.6201,  E(|Yhat-Yhat'|): 0.5038\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2657,  E(|Y-Yhat|): 0.5370,  E(|Yhat-Yhat'|): 0.5427\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2674,  E(|Y-Yhat|): 0.5332,  E(|Yhat-Yhat'|): 0.5317\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2509,  E(|Y-Yhat|): 0.5258,  E(|Yhat-Yhat'|): 0.5498\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6173,  E(|Y-Yhat|): 1.2786,  E(|Yhat-Yhat'|): 1.3226\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1900,  E(|Y-Yhat|): 0.3636,  E(|Yhat-Yhat'|): 0.3472\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1221,  E(|Y-Yhat|): 0.2531,  E(|Yhat-Yhat'|): 0.2620\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1264,  E(|Y-Yhat|): 0.2534,  E(|Yhat-Yhat'|): 0.2540\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1219,  E(|Y-Yhat|): 0.2453,  E(|Yhat-Yhat'|): 0.2467\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2096,  E(|Y-Yhat|): 0.4292,  E(|Yhat-Yhat'|): 0.4393\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6276,  E(|Y-Yhat|): 1.0508,  E(|Yhat-Yhat'|): 0.8465\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4536,  E(|Y-Yhat|): 0.9152,  E(|Yhat-Yhat'|): 0.9232\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4473,  E(|Y-Yhat|): 0.9057,  E(|Yhat-Yhat'|): 0.9167\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4548,  E(|Y-Yhat|): 0.9211,  E(|Yhat-Yhat'|): 0.9327\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.4201,  E(|Y-Yhat|): 2.9028,  E(|Yhat-Yhat'|): 2.9655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8083,  E(|Y-Yhat|): 2.1036,  E(|Yhat-Yhat'|): 0.5905\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1079,  E(|Y-Yhat|): 6.4383,  E(|Yhat-Yhat'|): 10.6608\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3011,  E(|Y-Yhat|): 19.9038,  E(|Yhat-Yhat'|): 37.2054\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.7814,  E(|Y-Yhat|): 39.7444,  E(|Yhat-Yhat'|): 75.9259\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.7760,  E(|Y-Yhat|): 45.1655,  E(|Yhat-Yhat'|): 86.7789\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2156,  E(|Y-Yhat|): 0.3971,  E(|Yhat-Yhat'|): 0.3630\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1422,  E(|Y-Yhat|): 0.2940,  E(|Yhat-Yhat'|): 0.3035\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1392,  E(|Y-Yhat|): 0.2875,  E(|Yhat-Yhat'|): 0.2966\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1425,  E(|Y-Yhat|): 0.2885,  E(|Yhat-Yhat'|): 0.2919\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1581,  E(|Y-Yhat|): 0.3119,  E(|Yhat-Yhat'|): 0.3076\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4903,  E(|Y-Yhat|): 0.8399,  E(|Yhat-Yhat'|): 0.6992\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3046,  E(|Y-Yhat|): 0.6129,  E(|Yhat-Yhat'|): 0.6167\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3076,  E(|Y-Yhat|): 0.6179,  E(|Yhat-Yhat'|): 0.6206\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3032,  E(|Y-Yhat|): 0.6105,  E(|Yhat-Yhat'|): 0.6146\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3287,  E(|Y-Yhat|): 0.6750,  E(|Yhat-Yhat'|): 0.6925\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2217,  E(|Y-Yhat|): 0.4178,  E(|Yhat-Yhat'|): 0.3923\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1583,  E(|Y-Yhat|): 0.3290,  E(|Yhat-Yhat'|): 0.3413\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1595,  E(|Y-Yhat|): 0.3204,  E(|Yhat-Yhat'|): 0.3218\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1590,  E(|Y-Yhat|): 0.3174,  E(|Yhat-Yhat'|): 0.3167\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1696,  E(|Y-Yhat|): 0.3379,  E(|Yhat-Yhat'|): 0.3366\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6918,  E(|Y-Yhat|): 1.1187,  E(|Yhat-Yhat'|): 0.8537\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4486,  E(|Y-Yhat|): 0.8945,  E(|Yhat-Yhat'|): 0.8919\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4511,  E(|Y-Yhat|): 0.9137,  E(|Yhat-Yhat'|): 0.9252\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4643,  E(|Y-Yhat|): 0.8923,  E(|Yhat-Yhat'|): 0.8559\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1706,  E(|Y-Yhat|): 0.3087,  E(|Yhat-Yhat'|): 0.2763\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9380,  E(|Y-Yhat|): 2.3002,  E(|Yhat-Yhat'|): 0.7244\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.5855,  E(|Y-Yhat|): 7.8767,  E(|Yhat-Yhat'|): 12.5823\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5290,  E(|Y-Yhat|): 96.0821,  E(|Yhat-Yhat'|): 191.1061\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.9238,  E(|Y-Yhat|): 130.2904,  E(|Yhat-Yhat'|): 254.7332\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2529,  E(|Y-Yhat|): 17.0990,  E(|Yhat-Yhat'|): 33.6922\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2277,  E(|Y-Yhat|): 0.3976,  E(|Yhat-Yhat'|): 0.3399\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1335,  E(|Y-Yhat|): 0.2725,  E(|Yhat-Yhat'|): 0.2780\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1330,  E(|Y-Yhat|): 0.2663,  E(|Yhat-Yhat'|): 0.2665\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1324,  E(|Y-Yhat|): 0.2685,  E(|Yhat-Yhat'|): 0.2723\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0444,  E(|Y-Yhat|): 0.0901,  E(|Yhat-Yhat'|): 0.0914\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4200,  E(|Y-Yhat|): 0.7047,  E(|Yhat-Yhat'|): 0.5695\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3015,  E(|Y-Yhat|): 0.5935,  E(|Yhat-Yhat'|): 0.5842\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2910,  E(|Y-Yhat|): 0.5899,  E(|Yhat-Yhat'|): 0.5977\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2910,  E(|Y-Yhat|): 0.5785,  E(|Yhat-Yhat'|): 0.5749\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0851,  E(|Y-Yhat|): 0.1679,  E(|Yhat-Yhat'|): 0.1656\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2287,  E(|Y-Yhat|): 0.4364,  E(|Yhat-Yhat'|): 0.4154\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1444,  E(|Y-Yhat|): 0.2992,  E(|Yhat-Yhat'|): 0.3096\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1467,  E(|Y-Yhat|): 0.2985,  E(|Yhat-Yhat'|): 0.3036\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1473,  E(|Y-Yhat|): 0.2949,  E(|Yhat-Yhat'|): 0.2953\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0584,  E(|Y-Yhat|): 0.1211,  E(|Yhat-Yhat'|): 0.1256\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6632,  E(|Y-Yhat|): 1.0788,  E(|Yhat-Yhat'|): 0.8313\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3953,  E(|Y-Yhat|): 0.7944,  E(|Yhat-Yhat'|): 0.7984\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4014,  E(|Y-Yhat|): 0.7893,  E(|Yhat-Yhat'|): 0.7758\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3923,  E(|Y-Yhat|): 0.7858,  E(|Yhat-Yhat'|): 0.7871\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.4448,  E(|Y-Yhat|): 6.4519,  E(|Yhat-Yhat'|): 6.0140\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8871,  E(|Y-Yhat|): 2.1478,  E(|Yhat-Yhat'|): 0.5214\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4028,  E(|Y-Yhat|): 17.4437,  E(|Yhat-Yhat'|): 32.0819\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3360,  E(|Y-Yhat|): 5.7149,  E(|Yhat-Yhat'|): 8.7578\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3888,  E(|Y-Yhat|): 4.0659,  E(|Yhat-Yhat'|): 5.3543\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.2621,  E(|Y-Yhat|): 9.4197,  E(|Yhat-Yhat'|): 12.3152\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1794,  E(|Y-Yhat|): 0.3354,  E(|Yhat-Yhat'|): 0.3120\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1196,  E(|Y-Yhat|): 0.2437,  E(|Yhat-Yhat'|): 0.2483\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1163,  E(|Y-Yhat|): 0.2338,  E(|Yhat-Yhat'|): 0.2350\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1194,  E(|Y-Yhat|): 0.2405,  E(|Yhat-Yhat'|): 0.2422\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1539,  E(|Y-Yhat|): 0.3179,  E(|Yhat-Yhat'|): 0.3282\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4633,  E(|Y-Yhat|): 0.7790,  E(|Yhat-Yhat'|): 0.6314\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2673,  E(|Y-Yhat|): 0.5552,  E(|Yhat-Yhat'|): 0.5757\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2642,  E(|Y-Yhat|): 0.5358,  E(|Yhat-Yhat'|): 0.5432\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2682,  E(|Y-Yhat|): 0.5298,  E(|Yhat-Yhat'|): 0.5232\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3916,  E(|Y-Yhat|): 0.7791,  E(|Yhat-Yhat'|): 0.7751\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1719,  E(|Y-Yhat|): 0.3116,  E(|Yhat-Yhat'|): 0.2794\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1248,  E(|Y-Yhat|): 0.2500,  E(|Yhat-Yhat'|): 0.2504\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1232,  E(|Y-Yhat|): 0.2456,  E(|Yhat-Yhat'|): 0.2448\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1256,  E(|Y-Yhat|): 0.2493,  E(|Yhat-Yhat'|): 0.2473\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1553,  E(|Y-Yhat|): 0.3118,  E(|Yhat-Yhat'|): 0.3132\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0063),\n",
       " tensor(0.2031),\n",
       " tensor(0.0121),\n",
       " tensor(0.0075),\n",
       " tensor(0.0098))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"square\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = (F.relu(Z))**2 / 2.0\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b238491",
   "metadata": {},
   "source": [
    "## True function: log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "514a9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7654,  E(|Y-Yhat|): 1.3360,  E(|Yhat-Yhat'|): 1.1413\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4566,  E(|Y-Yhat|): 0.9243,  E(|Yhat-Yhat'|): 0.9353\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4562,  E(|Y-Yhat|): 0.9169,  E(|Yhat-Yhat'|): 0.9215\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4587,  E(|Y-Yhat|): 0.9133,  E(|Yhat-Yhat'|): 0.9093\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1372,  E(|Y-Yhat|): 0.2813,  E(|Yhat-Yhat'|): 0.2883\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5493,  E(|Y-Yhat|): 1.9875,  E(|Yhat-Yhat'|): 0.8763\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 3.2256,  E(|Y-Yhat|): 177.3384,  E(|Yhat-Yhat'|): 348.2256\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1097,  E(|Y-Yhat|): 90.6323,  E(|Yhat-Yhat'|): 181.0453\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8459,  E(|Y-Yhat|): 33.0586,  E(|Yhat-Yhat'|): 64.4255\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0289,  E(|Y-Yhat|): 6.6140,  E(|Yhat-Yhat'|): 13.1704\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2996,  E(|Y-Yhat|): 0.5236,  E(|Yhat-Yhat'|): 0.4478\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1705,  E(|Y-Yhat|): 0.3379,  E(|Yhat-Yhat'|): 0.3349\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1675,  E(|Y-Yhat|): 0.3413,  E(|Yhat-Yhat'|): 0.3476\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1670,  E(|Y-Yhat|): 0.3372,  E(|Yhat-Yhat'|): 0.3402\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0572,  E(|Y-Yhat|): 0.1160,  E(|Yhat-Yhat'|): 0.1177\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6174,  E(|Y-Yhat|): 1.0093,  E(|Yhat-Yhat'|): 0.7838\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3161,  E(|Y-Yhat|): 0.6401,  E(|Yhat-Yhat'|): 0.6479\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3163,  E(|Y-Yhat|): 0.6329,  E(|Yhat-Yhat'|): 0.6332\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3143,  E(|Y-Yhat|): 0.6316,  E(|Yhat-Yhat'|): 0.6346\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1043,  E(|Y-Yhat|): 0.2130,  E(|Yhat-Yhat'|): 0.2175\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3013,  E(|Y-Yhat|): 0.5510,  E(|Yhat-Yhat'|): 0.4993\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1907,  E(|Y-Yhat|): 0.3865,  E(|Yhat-Yhat'|): 0.3914\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1899,  E(|Y-Yhat|): 0.3855,  E(|Yhat-Yhat'|): 0.3912\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1934,  E(|Y-Yhat|): 0.3841,  E(|Yhat-Yhat'|): 0.3815\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0810,  E(|Y-Yhat|): 0.1643,  E(|Yhat-Yhat'|): 0.1665\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8486,  E(|Y-Yhat|): 1.4333,  E(|Yhat-Yhat'|): 1.1694\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4656,  E(|Y-Yhat|): 0.9656,  E(|Yhat-Yhat'|): 1.0000\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4719,  E(|Y-Yhat|): 0.9498,  E(|Yhat-Yhat'|): 0.9558\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4748,  E(|Y-Yhat|): 0.9446,  E(|Yhat-Yhat'|): 0.9396\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0280,  E(|Y-Yhat|): 0.0576,  E(|Yhat-Yhat'|): 0.0591\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7531,  E(|Y-Yhat|): 2.1001,  E(|Yhat-Yhat'|): 0.6940\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4817,  E(|Y-Yhat|): 55.9537,  E(|Yhat-Yhat'|): 108.9440\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6065,  E(|Y-Yhat|): 57.4516,  E(|Yhat-Yhat'|): 113.6902\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5628,  E(|Y-Yhat|): 58.2773,  E(|Yhat-Yhat'|): 115.4291\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0250,  E(|Y-Yhat|): 2.2126,  E(|Yhat-Yhat'|): 4.3753\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3145,  E(|Y-Yhat|): 0.5659,  E(|Yhat-Yhat'|): 0.5029\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1775,  E(|Y-Yhat|): 0.3618,  E(|Yhat-Yhat'|): 0.3684\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1736,  E(|Y-Yhat|): 0.3497,  E(|Yhat-Yhat'|): 0.3522\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1767,  E(|Y-Yhat|): 0.3532,  E(|Yhat-Yhat'|): 0.3531\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0132,  E(|Y-Yhat|): 0.0279,  E(|Yhat-Yhat'|): 0.0292\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6405,  E(|Y-Yhat|): 1.0626,  E(|Yhat-Yhat'|): 0.8442\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3262,  E(|Y-Yhat|): 0.6575,  E(|Yhat-Yhat'|): 0.6625\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3249,  E(|Y-Yhat|): 0.6631,  E(|Yhat-Yhat'|): 0.6766\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3269,  E(|Y-Yhat|): 0.6526,  E(|Yhat-Yhat'|): 0.6515\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0273,  E(|Y-Yhat|): 0.0529,  E(|Yhat-Yhat'|): 0.0512\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3210,  E(|Y-Yhat|): 0.5823,  E(|Yhat-Yhat'|): 0.5226\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2028,  E(|Y-Yhat|): 0.4053,  E(|Yhat-Yhat'|): 0.4051\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2014,  E(|Y-Yhat|): 0.4059,  E(|Yhat-Yhat'|): 0.4090\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2034,  E(|Y-Yhat|): 0.4080,  E(|Yhat-Yhat'|): 0.4091\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0245,  E(|Y-Yhat|): 0.0499,  E(|Yhat-Yhat'|): 0.0508\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8052,  E(|Y-Yhat|): 1.4278,  E(|Yhat-Yhat'|): 1.2453\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4929,  E(|Y-Yhat|): 1.0219,  E(|Yhat-Yhat'|): 1.0580\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4920,  E(|Y-Yhat|): 1.0101,  E(|Yhat-Yhat'|): 1.0362\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4976,  E(|Y-Yhat|): 0.9985,  E(|Yhat-Yhat'|): 1.0018\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0357,  E(|Y-Yhat|): 0.0720,  E(|Yhat-Yhat'|): 0.0724\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8329,  E(|Y-Yhat|): 2.1813,  E(|Yhat-Yhat'|): 0.6969\n",
      "[Epoch 100 (33%), batch 9] energy-loss: -1.4764,  E(|Y-Yhat|): 82.3437,  E(|Yhat-Yhat'|): 167.6403\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8702,  E(|Y-Yhat|): 74.5061,  E(|Yhat-Yhat'|): 145.2717\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4962,  E(|Y-Yhat|): 50.5572,  E(|Yhat-Yhat'|): 100.1221\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.0062,  E(|Y-Yhat|): 2.7665,  E(|Yhat-Yhat'|): 5.5453\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3250,  E(|Y-Yhat|): 0.5769,  E(|Yhat-Yhat'|): 0.5039\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1865,  E(|Y-Yhat|): 0.3821,  E(|Yhat-Yhat'|): 0.3913\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1878,  E(|Y-Yhat|): 0.3818,  E(|Yhat-Yhat'|): 0.3879\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1899,  E(|Y-Yhat|): 0.3817,  E(|Yhat-Yhat'|): 0.3836\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0164,  E(|Y-Yhat|): 0.0338,  E(|Yhat-Yhat'|): 0.0346\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6457,  E(|Y-Yhat|): 1.0668,  E(|Yhat-Yhat'|): 0.8423\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3439,  E(|Y-Yhat|): 0.6996,  E(|Yhat-Yhat'|): 0.7113\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3452,  E(|Y-Yhat|): 0.7007,  E(|Yhat-Yhat'|): 0.7110\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3466,  E(|Y-Yhat|): 0.6985,  E(|Yhat-Yhat'|): 0.7038\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0330,  E(|Y-Yhat|): 0.0668,  E(|Yhat-Yhat'|): 0.0677\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3311,  E(|Y-Yhat|): 0.5984,  E(|Yhat-Yhat'|): 0.5347\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2187,  E(|Y-Yhat|): 0.4382,  E(|Yhat-Yhat'|): 0.4389\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2167,  E(|Y-Yhat|): 0.4386,  E(|Yhat-Yhat'|): 0.4438\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2203,  E(|Y-Yhat|): 0.4396,  E(|Yhat-Yhat'|): 0.4388\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0309,  E(|Y-Yhat|): 0.0611,  E(|Yhat-Yhat'|): 0.0604\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8391,  E(|Y-Yhat|): 1.3953,  E(|Yhat-Yhat'|): 1.1125\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4656,  E(|Y-Yhat|): 0.9474,  E(|Yhat-Yhat'|): 0.9636\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4676,  E(|Y-Yhat|): 0.9506,  E(|Yhat-Yhat'|): 0.9661\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4726,  E(|Y-Yhat|): 0.9555,  E(|Yhat-Yhat'|): 0.9659\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0324,  E(|Y-Yhat|): 0.0661,  E(|Yhat-Yhat'|): 0.0673\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6821,  E(|Y-Yhat|): 2.0151,  E(|Yhat-Yhat'|): 0.6660\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7233,  E(|Y-Yhat|): 20.9117,  E(|Yhat-Yhat'|): 40.3768\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6626,  E(|Y-Yhat|): 29.8059,  E(|Yhat-Yhat'|): 58.2867\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7276,  E(|Y-Yhat|): 42.9213,  E(|Yhat-Yhat'|): 84.3874\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0111,  E(|Y-Yhat|): 2.0271,  E(|Yhat-Yhat'|): 4.0319\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3236,  E(|Y-Yhat|): 0.5457,  E(|Yhat-Yhat'|): 0.4442\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1763,  E(|Y-Yhat|): 0.3567,  E(|Yhat-Yhat'|): 0.3608\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1730,  E(|Y-Yhat|): 0.3545,  E(|Yhat-Yhat'|): 0.3630\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1767,  E(|Y-Yhat|): 0.3448,  E(|Yhat-Yhat'|): 0.3362\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0154,  E(|Y-Yhat|): 0.0307,  E(|Yhat-Yhat'|): 0.0306\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6271,  E(|Y-Yhat|): 1.0471,  E(|Yhat-Yhat'|): 0.8400\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3191,  E(|Y-Yhat|): 0.6484,  E(|Yhat-Yhat'|): 0.6586\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3275,  E(|Y-Yhat|): 0.6503,  E(|Yhat-Yhat'|): 0.6457\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3229,  E(|Y-Yhat|): 0.6421,  E(|Yhat-Yhat'|): 0.6384\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0292,  E(|Y-Yhat|): 0.0588,  E(|Yhat-Yhat'|): 0.0593\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3165,  E(|Y-Yhat|): 0.5829,  E(|Yhat-Yhat'|): 0.5328\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2008,  E(|Y-Yhat|): 0.4052,  E(|Yhat-Yhat'|): 0.4088\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2019,  E(|Y-Yhat|): 0.4073,  E(|Yhat-Yhat'|): 0.4108\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2008,  E(|Y-Yhat|): 0.4030,  E(|Yhat-Yhat'|): 0.4044\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0273,  E(|Y-Yhat|): 0.0567,  E(|Yhat-Yhat'|): 0.0589\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8445,  E(|Y-Yhat|): 1.3762,  E(|Yhat-Yhat'|): 1.0633\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4621,  E(|Y-Yhat|): 0.9406,  E(|Yhat-Yhat'|): 0.9570\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4627,  E(|Y-Yhat|): 0.9241,  E(|Yhat-Yhat'|): 0.9229\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4605,  E(|Y-Yhat|): 0.9342,  E(|Yhat-Yhat'|): 0.9474\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4897,  E(|Y-Yhat|): 0.9970,  E(|Yhat-Yhat'|): 1.0147\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6409,  E(|Y-Yhat|): 2.0091,  E(|Yhat-Yhat'|): 0.7364\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6699,  E(|Y-Yhat|): 20.9320,  E(|Yhat-Yhat'|): 40.5242\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 3.4420,  E(|Y-Yhat|): 1436.5485,  E(|Yhat-Yhat'|): 2866.2130\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 3.5562,  E(|Y-Yhat|): 231.5480,  E(|Yhat-Yhat'|): 455.9836\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0025,  E(|Y-Yhat|): 164.4777,  E(|Yhat-Yhat'|): 324.9503\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2999,  E(|Y-Yhat|): 0.5260,  E(|Yhat-Yhat'|): 0.4522\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1739,  E(|Y-Yhat|): 0.3491,  E(|Yhat-Yhat'|): 0.3505\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1682,  E(|Y-Yhat|): 0.3424,  E(|Yhat-Yhat'|): 0.3482\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1683,  E(|Y-Yhat|): 0.3427,  E(|Yhat-Yhat'|): 0.3488\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1420,  E(|Y-Yhat|): 0.2921,  E(|Yhat-Yhat'|): 0.3002\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6249,  E(|Y-Yhat|): 1.0287,  E(|Yhat-Yhat'|): 0.8076\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3166,  E(|Y-Yhat|): 0.6574,  E(|Yhat-Yhat'|): 0.6815\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3186,  E(|Y-Yhat|): 0.6363,  E(|Yhat-Yhat'|): 0.6355\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3231,  E(|Y-Yhat|): 0.6355,  E(|Yhat-Yhat'|): 0.6247\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2643,  E(|Y-Yhat|): 0.5187,  E(|Yhat-Yhat'|): 0.5088\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3230,  E(|Y-Yhat|): 0.5697,  E(|Yhat-Yhat'|): 0.4934\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1981,  E(|Y-Yhat|): 0.3951,  E(|Yhat-Yhat'|): 0.3941\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1910,  E(|Y-Yhat|): 0.3899,  E(|Yhat-Yhat'|): 0.3977\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1920,  E(|Y-Yhat|): 0.3880,  E(|Yhat-Yhat'|): 0.3920\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1701,  E(|Y-Yhat|): 0.3417,  E(|Yhat-Yhat'|): 0.3431\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8304,  E(|Y-Yhat|): 1.4484,  E(|Yhat-Yhat'|): 1.2359\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5148,  E(|Y-Yhat|): 1.0515,  E(|Yhat-Yhat'|): 1.0734\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5103,  E(|Y-Yhat|): 1.0357,  E(|Yhat-Yhat'|): 1.0507\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5140,  E(|Y-Yhat|): 1.0384,  E(|Yhat-Yhat'|): 1.0489\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1230,  E(|Y-Yhat|): 0.2493,  E(|Yhat-Yhat'|): 0.2527\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6973,  E(|Y-Yhat|): 2.0859,  E(|Yhat-Yhat'|): 0.7773\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8243,  E(|Y-Yhat|): 22.2573,  E(|Yhat-Yhat'|): 42.8660\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.4105,  E(|Y-Yhat|): 54.6761,  E(|Yhat-Yhat'|): 110.1731\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5464,  E(|Y-Yhat|): 42.6809,  E(|Yhat-Yhat'|): 84.2689\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3222,  E(|Y-Yhat|): 9.9176,  E(|Yhat-Yhat'|): 19.1909\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3425,  E(|Y-Yhat|): 0.5857,  E(|Yhat-Yhat'|): 0.4864\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2009,  E(|Y-Yhat|): 0.4059,  E(|Yhat-Yhat'|): 0.4100\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2005,  E(|Y-Yhat|): 0.4030,  E(|Yhat-Yhat'|): 0.4051\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1981,  E(|Y-Yhat|): 0.3969,  E(|Yhat-Yhat'|): 0.3977\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0542,  E(|Y-Yhat|): 0.1100,  E(|Yhat-Yhat'|): 0.1117\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6667,  E(|Y-Yhat|): 1.1097,  E(|Yhat-Yhat'|): 0.8858\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3558,  E(|Y-Yhat|): 0.7197,  E(|Yhat-Yhat'|): 0.7279\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3598,  E(|Y-Yhat|): 0.7265,  E(|Yhat-Yhat'|): 0.7335\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3607,  E(|Y-Yhat|): 0.7273,  E(|Yhat-Yhat'|): 0.7332\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0996,  E(|Y-Yhat|): 0.2035,  E(|Yhat-Yhat'|): 0.2078\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3454,  E(|Y-Yhat|): 0.6157,  E(|Yhat-Yhat'|): 0.5407\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2301,  E(|Y-Yhat|): 0.4580,  E(|Yhat-Yhat'|): 0.4559\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2293,  E(|Y-Yhat|): 0.4614,  E(|Yhat-Yhat'|): 0.4642\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2299,  E(|Y-Yhat|): 0.4635,  E(|Yhat-Yhat'|): 0.4671\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0834,  E(|Y-Yhat|): 0.1721,  E(|Yhat-Yhat'|): 0.1774\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8232,  E(|Y-Yhat|): 1.3548,  E(|Yhat-Yhat'|): 1.0631\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4571,  E(|Y-Yhat|): 0.9534,  E(|Yhat-Yhat'|): 0.9927\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4659,  E(|Y-Yhat|): 0.9312,  E(|Yhat-Yhat'|): 0.9307\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4622,  E(|Y-Yhat|): 0.9264,  E(|Yhat-Yhat'|): 0.9284\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5391,  E(|Y-Yhat|): 1.1141,  E(|Yhat-Yhat'|): 1.1501\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8095,  E(|Y-Yhat|): 2.1262,  E(|Yhat-Yhat'|): 0.6335\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7617,  E(|Y-Yhat|): 11.8309,  E(|Yhat-Yhat'|): 22.1383\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9335,  E(|Y-Yhat|): 14.6081,  E(|Yhat-Yhat'|): 27.3490\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0405,  E(|Y-Yhat|): 19.1055,  E(|Yhat-Yhat'|): 38.1300\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8866,  E(|Y-Yhat|): 17.7875,  E(|Yhat-Yhat'|): 33.8018\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2986,  E(|Y-Yhat|): 0.5307,  E(|Yhat-Yhat'|): 0.4641\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1727,  E(|Y-Yhat|): 0.3460,  E(|Yhat-Yhat'|): 0.3466\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1711,  E(|Y-Yhat|): 0.3504,  E(|Yhat-Yhat'|): 0.3588\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1753,  E(|Y-Yhat|): 0.3498,  E(|Yhat-Yhat'|): 0.3490\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1554,  E(|Y-Yhat|): 0.3062,  E(|Yhat-Yhat'|): 0.3016\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5865,  E(|Y-Yhat|): 1.0099,  E(|Yhat-Yhat'|): 0.8466\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3136,  E(|Y-Yhat|): 0.6443,  E(|Yhat-Yhat'|): 0.6615\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3198,  E(|Y-Yhat|): 0.6499,  E(|Yhat-Yhat'|): 0.6602\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3158,  E(|Y-Yhat|): 0.6549,  E(|Yhat-Yhat'|): 0.6781\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2769,  E(|Y-Yhat|): 0.5714,  E(|Yhat-Yhat'|): 0.5890\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3089,  E(|Y-Yhat|): 0.5709,  E(|Yhat-Yhat'|): 0.5240\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.4004,  E(|Yhat-Yhat'|): 0.4089\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1937,  E(|Y-Yhat|): 0.3993,  E(|Yhat-Yhat'|): 0.4112\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.3950,  E(|Yhat-Yhat'|): 0.3982\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1811,  E(|Y-Yhat|): 0.3623,  E(|Yhat-Yhat'|): 0.3625\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7446,  E(|Y-Yhat|): 1.3382,  E(|Yhat-Yhat'|): 1.1874\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4538,  E(|Y-Yhat|): 0.9351,  E(|Yhat-Yhat'|): 0.9626\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4652,  E(|Y-Yhat|): 0.9323,  E(|Yhat-Yhat'|): 0.9343\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4569,  E(|Y-Yhat|): 0.9311,  E(|Yhat-Yhat'|): 0.9483\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2843,  E(|Y-Yhat|): 0.5866,  E(|Yhat-Yhat'|): 0.6047\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6601,  E(|Y-Yhat|): 1.9422,  E(|Yhat-Yhat'|): 0.5641\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3669,  E(|Y-Yhat|): 23.6719,  E(|Yhat-Yhat'|): 46.6100\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 3.5198,  E(|Y-Yhat|): 1084.0794,  E(|Yhat-Yhat'|): 2161.1192\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 57.9870,  E(|Y-Yhat|): 4864.3174,  E(|Yhat-Yhat'|): 9612.6608\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -26.1160,  E(|Y-Yhat|): 2117.1003,  E(|Yhat-Yhat'|): 4286.4326\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2980,  E(|Y-Yhat|): 0.5441,  E(|Yhat-Yhat'|): 0.4922\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1753,  E(|Y-Yhat|): 0.3556,  E(|Yhat-Yhat'|): 0.3605\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1712,  E(|Y-Yhat|): 0.3521,  E(|Yhat-Yhat'|): 0.3618\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1766,  E(|Y-Yhat|): 0.3492,  E(|Yhat-Yhat'|): 0.3451\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1026,  E(|Y-Yhat|): 0.2037,  E(|Yhat-Yhat'|): 0.2022\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6511,  E(|Y-Yhat|): 1.0753,  E(|Yhat-Yhat'|): 0.8485\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3153,  E(|Y-Yhat|): 0.6488,  E(|Yhat-Yhat'|): 0.6670\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3242,  E(|Y-Yhat|): 0.6470,  E(|Yhat-Yhat'|): 0.6457\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3244,  E(|Y-Yhat|): 0.6518,  E(|Yhat-Yhat'|): 0.6548\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1864,  E(|Y-Yhat|): 0.3789,  E(|Yhat-Yhat'|): 0.3849\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3068,  E(|Y-Yhat|): 0.5572,  E(|Yhat-Yhat'|): 0.5007\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1973,  E(|Y-Yhat|): 0.3978,  E(|Yhat-Yhat'|): 0.4008\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1956,  E(|Y-Yhat|): 0.3900,  E(|Yhat-Yhat'|): 0.3889\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1929,  E(|Y-Yhat|): 0.4007,  E(|Yhat-Yhat'|): 0.4157\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1307,  E(|Y-Yhat|): 0.2667,  E(|Yhat-Yhat'|): 0.2720\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8292,  E(|Y-Yhat|): 1.3680,  E(|Yhat-Yhat'|): 1.0775\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4558,  E(|Y-Yhat|): 0.9480,  E(|Yhat-Yhat'|): 0.9844\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4562,  E(|Y-Yhat|): 0.9235,  E(|Yhat-Yhat'|): 0.9347\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4591,  E(|Y-Yhat|): 0.9379,  E(|Yhat-Yhat'|): 0.9575\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0161,  E(|Y-Yhat|): 0.0331,  E(|Yhat-Yhat'|): 0.0339\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7952,  E(|Y-Yhat|): 2.1521,  E(|Yhat-Yhat'|): 0.7137\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5979,  E(|Y-Yhat|): 8.6235,  E(|Yhat-Yhat'|): 16.0513\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9212,  E(|Y-Yhat|): 22.8629,  E(|Yhat-Yhat'|): 43.8834\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6334,  E(|Y-Yhat|): 11.0851,  E(|Yhat-Yhat'|): 20.9034\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0122,  E(|Y-Yhat|): 0.3270,  E(|Yhat-Yhat'|): 0.6295\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3120,  E(|Y-Yhat|): 0.5417,  E(|Yhat-Yhat'|): 0.4595\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1723,  E(|Y-Yhat|): 0.3445,  E(|Yhat-Yhat'|): 0.3443\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1689,  E(|Y-Yhat|): 0.3456,  E(|Yhat-Yhat'|): 0.3534\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1714,  E(|Y-Yhat|): 0.3474,  E(|Yhat-Yhat'|): 0.3521\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0080,  E(|Y-Yhat|): 0.0157,  E(|Yhat-Yhat'|): 0.0155\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5663,  E(|Y-Yhat|): 1.0011,  E(|Yhat-Yhat'|): 0.8696\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3170,  E(|Y-Yhat|): 0.6489,  E(|Yhat-Yhat'|): 0.6638\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3201,  E(|Y-Yhat|): 0.6437,  E(|Yhat-Yhat'|): 0.6473\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3122,  E(|Y-Yhat|): 0.6243,  E(|Yhat-Yhat'|): 0.6241\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0151,  E(|Y-Yhat|): 0.0307,  E(|Yhat-Yhat'|): 0.0312\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3208,  E(|Y-Yhat|): 0.5741,  E(|Yhat-Yhat'|): 0.5065\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1939,  E(|Y-Yhat|): 0.3934,  E(|Yhat-Yhat'|): 0.3990\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1933,  E(|Y-Yhat|): 0.3825,  E(|Yhat-Yhat'|): 0.3784\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1922,  E(|Y-Yhat|): 0.3911,  E(|Yhat-Yhat'|): 0.3977\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0146,  E(|Y-Yhat|): 0.0301,  E(|Yhat-Yhat'|): 0.0311\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7938,  E(|Y-Yhat|): 1.3457,  E(|Yhat-Yhat'|): 1.1039\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4564,  E(|Y-Yhat|): 0.9159,  E(|Yhat-Yhat'|): 0.9189\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4514,  E(|Y-Yhat|): 0.9023,  E(|Yhat-Yhat'|): 0.9019\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4478,  E(|Y-Yhat|): 0.9114,  E(|Yhat-Yhat'|): 0.9272\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1198,  E(|Y-Yhat|): 0.2408,  E(|Yhat-Yhat'|): 0.2421\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7923,  E(|Y-Yhat|): 2.0628,  E(|Yhat-Yhat'|): 0.5409\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7189,  E(|Y-Yhat|): 11.2827,  E(|Yhat-Yhat'|): 21.1276\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 6.3176,  E(|Y-Yhat|): 263.4437,  E(|Yhat-Yhat'|): 514.2522\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8094,  E(|Y-Yhat|): 39.4314,  E(|Yhat-Yhat'|): 77.2439\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0768,  E(|Y-Yhat|): 7.4485,  E(|Yhat-Yhat'|): 14.7434\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3129,  E(|Y-Yhat|): 0.5404,  E(|Yhat-Yhat'|): 0.4551\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1662,  E(|Y-Yhat|): 0.3374,  E(|Yhat-Yhat'|): 0.3422\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1622,  E(|Y-Yhat|): 0.3282,  E(|Yhat-Yhat'|): 0.3322\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1649,  E(|Y-Yhat|): 0.3270,  E(|Yhat-Yhat'|): 0.3241\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0502,  E(|Y-Yhat|): 0.1017,  E(|Yhat-Yhat'|): 0.1031\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5875,  E(|Y-Yhat|): 0.9941,  E(|Yhat-Yhat'|): 0.8132\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3099,  E(|Y-Yhat|): 0.6170,  E(|Yhat-Yhat'|): 0.6142\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3116,  E(|Y-Yhat|): 0.6315,  E(|Yhat-Yhat'|): 0.6398\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3092,  E(|Y-Yhat|): 0.6196,  E(|Yhat-Yhat'|): 0.6209\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0955,  E(|Y-Yhat|): 0.1909,  E(|Yhat-Yhat'|): 0.1907\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3009,  E(|Y-Yhat|): 0.5470,  E(|Yhat-Yhat'|): 0.4923\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1883,  E(|Y-Yhat|): 0.3806,  E(|Yhat-Yhat'|): 0.3846\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1827,  E(|Y-Yhat|): 0.3712,  E(|Yhat-Yhat'|): 0.3770\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1864,  E(|Y-Yhat|): 0.3722,  E(|Yhat-Yhat'|): 0.3716\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0739,  E(|Y-Yhat|): 0.1479,  E(|Yhat-Yhat'|): 0.1480\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0005),\n",
       " tensor(0.2635),\n",
       " tensor(0.0004),\n",
       " tensor(0.0004),\n",
       " tensor(0.0004))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"log\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 50)\n",
    "    x2 = torch.linspace(0, 5, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = log_lin(Z)\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb26ca1",
   "metadata": {},
   "source": [
    "# post-ANM, comparing 4 loss functions under different true functions ($X,Y \\in \\mathbb{R}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea678c9",
   "metadata": {},
   "source": [
    "## True function: softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cee49936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5688,  E(|Y-Yhat|): 1.0008,  E(|Yhat-Yhat'|): 0.8639\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4739,  E(|Y-Yhat|): 0.9768,  E(|Yhat-Yhat'|): 1.0058\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4844,  E(|Y-Yhat|): 0.9898,  E(|Yhat-Yhat'|): 1.0108\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4918,  E(|Y-Yhat|): 0.9864,  E(|Yhat-Yhat'|): 0.9892\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0399,  E(|Y-Yhat|): 2.0961,  E(|Yhat-Yhat'|): 2.1124\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8579,  E(|Y-Yhat|): 1.0825,  E(|Yhat-Yhat'|): 0.4493\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4870,  E(|Y-Yhat|): 45.2709,  E(|Yhat-Yhat'|): 89.5676\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0970,  E(|Y-Yhat|): 101.5014,  E(|Yhat-Yhat'|): 202.8089\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.6197,  E(|Y-Yhat|): 55.0863,  E(|Yhat-Yhat'|): 106.9332\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9178,  E(|Y-Yhat|): 67.6206,  E(|Yhat-Yhat'|): 133.4055\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2169,  E(|Y-Yhat|): 0.3819,  E(|Yhat-Yhat'|): 0.3300\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1811,  E(|Y-Yhat|): 0.3655,  E(|Yhat-Yhat'|): 0.3690\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1828,  E(|Y-Yhat|): 0.3622,  E(|Yhat-Yhat'|): 0.3589\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1835,  E(|Y-Yhat|): 0.3656,  E(|Yhat-Yhat'|): 0.3642\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2113,  E(|Y-Yhat|): 0.4214,  E(|Yhat-Yhat'|): 0.4200\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4157,  E(|Y-Yhat|): 0.6791,  E(|Yhat-Yhat'|): 0.5270\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3363,  E(|Y-Yhat|): 0.6741,  E(|Yhat-Yhat'|): 0.6756\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3315,  E(|Y-Yhat|): 0.6766,  E(|Yhat-Yhat'|): 0.6902\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3383,  E(|Y-Yhat|): 0.6798,  E(|Yhat-Yhat'|): 0.6830\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4084,  E(|Y-Yhat|): 0.8187,  E(|Yhat-Yhat'|): 0.8207\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2408,  E(|Y-Yhat|): 0.4158,  E(|Yhat-Yhat'|): 0.3499\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2050,  E(|Y-Yhat|): 0.4073,  E(|Yhat-Yhat'|): 0.4046\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2009,  E(|Y-Yhat|): 0.4074,  E(|Yhat-Yhat'|): 0.4130\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2018,  E(|Y-Yhat|): 0.4094,  E(|Yhat-Yhat'|): 0.4152\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2252,  E(|Y-Yhat|): 0.4570,  E(|Yhat-Yhat'|): 0.4636\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5789,  E(|Y-Yhat|): 1.0576,  E(|Yhat-Yhat'|): 0.9573\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5511,  E(|Y-Yhat|): 1.1018,  E(|Yhat-Yhat'|): 1.1014\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5489,  E(|Y-Yhat|): 1.1241,  E(|Yhat-Yhat'|): 1.1505\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5499,  E(|Y-Yhat|): 1.1178,  E(|Yhat-Yhat'|): 1.1359\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0361,  E(|Y-Yhat|): 2.1015,  E(|Yhat-Yhat'|): 2.1309\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9776,  E(|Y-Yhat|): 1.2185,  E(|Yhat-Yhat'|): 0.4818\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0287,  E(|Y-Yhat|): 3.8712,  E(|Yhat-Yhat'|): 5.6851\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9871,  E(|Y-Yhat|): 1.9972,  E(|Yhat-Yhat'|): 2.0202\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -0.0146,  E(|Y-Yhat|): 41.8182,  E(|Yhat-Yhat'|): 83.6655\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6059,  E(|Y-Yhat|): 48.5457,  E(|Yhat-Yhat'|): 95.8796\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2160,  E(|Y-Yhat|): 0.4050,  E(|Yhat-Yhat'|): 0.3780\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2064,  E(|Y-Yhat|): 0.4173,  E(|Yhat-Yhat'|): 0.4219\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2072,  E(|Y-Yhat|): 0.4169,  E(|Yhat-Yhat'|): 0.4194\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2094,  E(|Y-Yhat|): 0.4145,  E(|Yhat-Yhat'|): 0.4100\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2095,  E(|Y-Yhat|): 0.4239,  E(|Yhat-Yhat'|): 0.4289\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4191,  E(|Y-Yhat|): 0.7225,  E(|Yhat-Yhat'|): 0.6067\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3970,  E(|Y-Yhat|): 0.8050,  E(|Yhat-Yhat'|): 0.8160\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3941,  E(|Y-Yhat|): 0.7926,  E(|Yhat-Yhat'|): 0.7970\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4038,  E(|Y-Yhat|): 0.8019,  E(|Yhat-Yhat'|): 0.7963\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3962,  E(|Y-Yhat|): 0.7878,  E(|Yhat-Yhat'|): 0.7832\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2359,  E(|Y-Yhat|): 0.4415,  E(|Yhat-Yhat'|): 0.4112\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2228,  E(|Y-Yhat|): 0.4490,  E(|Yhat-Yhat'|): 0.4522\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2248,  E(|Y-Yhat|): 0.4464,  E(|Yhat-Yhat'|): 0.4430\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2245,  E(|Y-Yhat|): 0.4449,  E(|Yhat-Yhat'|): 0.4409\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2306,  E(|Y-Yhat|): 0.4624,  E(|Yhat-Yhat'|): 0.4636\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6114,  E(|Y-Yhat|): 0.9944,  E(|Yhat-Yhat'|): 0.7659\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5596,  E(|Y-Yhat|): 1.1325,  E(|Yhat-Yhat'|): 1.1458\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5667,  E(|Y-Yhat|): 1.1450,  E(|Yhat-Yhat'|): 1.1565\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5608,  E(|Y-Yhat|): 1.1309,  E(|Yhat-Yhat'|): 1.1401\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0099,  E(|Y-Yhat|): 2.0417,  E(|Yhat-Yhat'|): 2.0635\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0312,  E(|Y-Yhat|): 1.2393,  E(|Yhat-Yhat'|): 0.4162\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1109,  E(|Y-Yhat|): 22.4028,  E(|Yhat-Yhat'|): 42.5838\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6343,  E(|Y-Yhat|): 24.0779,  E(|Yhat-Yhat'|): 46.8872\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8066,  E(|Y-Yhat|): 29.7962,  E(|Yhat-Yhat'|): 57.9793\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2738,  E(|Y-Yhat|): 33.0311,  E(|Yhat-Yhat'|): 63.5145\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2263,  E(|Y-Yhat|): 0.3865,  E(|Yhat-Yhat'|): 0.3204\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2089,  E(|Y-Yhat|): 0.4264,  E(|Yhat-Yhat'|): 0.4351\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2118,  E(|Y-Yhat|): 0.4301,  E(|Yhat-Yhat'|): 0.4366\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2098,  E(|Y-Yhat|): 0.4244,  E(|Yhat-Yhat'|): 0.4291\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2139,  E(|Y-Yhat|): 0.4348,  E(|Yhat-Yhat'|): 0.4418\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4232,  E(|Y-Yhat|): 0.7367,  E(|Yhat-Yhat'|): 0.6269\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3960,  E(|Y-Yhat|): 0.8105,  E(|Yhat-Yhat'|): 0.8291\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4048,  E(|Y-Yhat|): 0.8145,  E(|Yhat-Yhat'|): 0.8193\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3984,  E(|Y-Yhat|): 0.8159,  E(|Yhat-Yhat'|): 0.8350\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4186,  E(|Y-Yhat|): 0.8384,  E(|Yhat-Yhat'|): 0.8397\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2412,  E(|Y-Yhat|): 0.4367,  E(|Yhat-Yhat'|): 0.3909\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2246,  E(|Y-Yhat|): 0.4557,  E(|Yhat-Yhat'|): 0.4623\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2264,  E(|Y-Yhat|): 0.4552,  E(|Yhat-Yhat'|): 0.4575\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2249,  E(|Y-Yhat|): 0.4556,  E(|Yhat-Yhat'|): 0.4614\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2304,  E(|Y-Yhat|): 0.4634,  E(|Yhat-Yhat'|): 0.4659\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5851,  E(|Y-Yhat|): 1.0435,  E(|Yhat-Yhat'|): 0.9166\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5570,  E(|Y-Yhat|): 1.1447,  E(|Yhat-Yhat'|): 1.1754\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5486,  E(|Y-Yhat|): 1.1388,  E(|Yhat-Yhat'|): 1.1804\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5547,  E(|Y-Yhat|): 1.1349,  E(|Yhat-Yhat'|): 1.1605\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0107,  E(|Y-Yhat|): 2.0652,  E(|Yhat-Yhat'|): 2.1089\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9782,  E(|Y-Yhat|): 1.2234,  E(|Yhat-Yhat'|): 0.4904\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9477,  E(|Y-Yhat|): 2.5500,  E(|Yhat-Yhat'|): 3.2046\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9814,  E(|Y-Yhat|): 3.5063,  E(|Yhat-Yhat'|): 5.0499\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0822,  E(|Y-Yhat|): 5.0311,  E(|Yhat-Yhat'|): 7.8978\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9505,  E(|Y-Yhat|): 4.5841,  E(|Yhat-Yhat'|): 7.2673\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2228,  E(|Y-Yhat|): 0.3947,  E(|Yhat-Yhat'|): 0.3438\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2062,  E(|Y-Yhat|): 0.4214,  E(|Yhat-Yhat'|): 0.4305\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2122,  E(|Y-Yhat|): 0.4243,  E(|Yhat-Yhat'|): 0.4240\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2117,  E(|Y-Yhat|): 0.4202,  E(|Yhat-Yhat'|): 0.4170\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2122,  E(|Y-Yhat|): 0.4231,  E(|Yhat-Yhat'|): 0.4219\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4191,  E(|Y-Yhat|): 0.7628,  E(|Yhat-Yhat'|): 0.6873\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4084,  E(|Y-Yhat|): 0.8022,  E(|Yhat-Yhat'|): 0.7875\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4060,  E(|Y-Yhat|): 0.8128,  E(|Yhat-Yhat'|): 0.8136\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3959,  E(|Y-Yhat|): 0.7948,  E(|Yhat-Yhat'|): 0.7978\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4162,  E(|Y-Yhat|): 0.8245,  E(|Yhat-Yhat'|): 0.8167\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2469,  E(|Y-Yhat|): 0.4315,  E(|Yhat-Yhat'|): 0.3692\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2271,  E(|Y-Yhat|): 0.4477,  E(|Yhat-Yhat'|): 0.4412\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2239,  E(|Y-Yhat|): 0.4528,  E(|Yhat-Yhat'|): 0.4578\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2240,  E(|Y-Yhat|): 0.4511,  E(|Yhat-Yhat'|): 0.4543\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2323,  E(|Y-Yhat|): 0.4574,  E(|Yhat-Yhat'|): 0.4503\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5891,  E(|Y-Yhat|): 1.0297,  E(|Yhat-Yhat'|): 0.8811\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5636,  E(|Y-Yhat|): 1.1589,  E(|Yhat-Yhat'|): 1.1905\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5621,  E(|Y-Yhat|): 1.1439,  E(|Yhat-Yhat'|): 1.1636\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5722,  E(|Y-Yhat|): 1.1349,  E(|Yhat-Yhat'|): 1.1254\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9522,  E(|Y-Yhat|): 1.9083,  E(|Yhat-Yhat'|): 1.9122\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9867,  E(|Y-Yhat|): 1.2531,  E(|Yhat-Yhat'|): 0.5327\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9783,  E(|Y-Yhat|): 1.7211,  E(|Yhat-Yhat'|): 1.4855\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0109,  E(|Y-Yhat|): 1.7749,  E(|Yhat-Yhat'|): 1.5278\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9829,  E(|Y-Yhat|): 1.7826,  E(|Yhat-Yhat'|): 1.5995\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9675,  E(|Y-Yhat|): 1.8139,  E(|Yhat-Yhat'|): 1.6927\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2357,  E(|Y-Yhat|): 0.4013,  E(|Yhat-Yhat'|): 0.3312\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2140,  E(|Y-Yhat|): 0.4305,  E(|Yhat-Yhat'|): 0.4330\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2123,  E(|Y-Yhat|): 0.4230,  E(|Yhat-Yhat'|): 0.4215\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2088,  E(|Y-Yhat|): 0.4256,  E(|Yhat-Yhat'|): 0.4336\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2093,  E(|Y-Yhat|): 0.4222,  E(|Yhat-Yhat'|): 0.4258\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4337,  E(|Y-Yhat|): 0.7336,  E(|Yhat-Yhat'|): 0.5997\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4010,  E(|Y-Yhat|): 0.8029,  E(|Yhat-Yhat'|): 0.8039\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4062,  E(|Y-Yhat|): 0.8258,  E(|Yhat-Yhat'|): 0.8392\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3951,  E(|Y-Yhat|): 0.8024,  E(|Yhat-Yhat'|): 0.8145\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4097,  E(|Y-Yhat|): 0.8192,  E(|Yhat-Yhat'|): 0.8191\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2435,  E(|Y-Yhat|): 0.4400,  E(|Yhat-Yhat'|): 0.3930\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2275,  E(|Y-Yhat|): 0.4537,  E(|Yhat-Yhat'|): 0.4524\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2306,  E(|Y-Yhat|): 0.4601,  E(|Yhat-Yhat'|): 0.4590\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2300,  E(|Y-Yhat|): 0.4589,  E(|Yhat-Yhat'|): 0.4578\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2257,  E(|Y-Yhat|): 0.4503,  E(|Yhat-Yhat'|): 0.4493\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5936,  E(|Y-Yhat|): 1.0115,  E(|Yhat-Yhat'|): 0.8357\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5639,  E(|Y-Yhat|): 1.1426,  E(|Yhat-Yhat'|): 1.1574\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5596,  E(|Y-Yhat|): 1.1355,  E(|Yhat-Yhat'|): 1.1518\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5649,  E(|Y-Yhat|): 1.1187,  E(|Yhat-Yhat'|): 1.1075\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9784,  E(|Y-Yhat|): 2.0113,  E(|Yhat-Yhat'|): 2.0658\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0097,  E(|Y-Yhat|): 1.1904,  E(|Yhat-Yhat'|): 0.3615\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9656,  E(|Y-Yhat|): 2.0251,  E(|Yhat-Yhat'|): 2.1191\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0475,  E(|Y-Yhat|): 5.4543,  E(|Yhat-Yhat'|): 8.8136\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.1461,  E(|Y-Yhat|): 16.4618,  E(|Yhat-Yhat'|): 30.6313\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9760,  E(|Y-Yhat|): 15.7923,  E(|Yhat-Yhat'|): 29.6325\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2302,  E(|Y-Yhat|): 0.3861,  E(|Yhat-Yhat'|): 0.3118\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2127,  E(|Y-Yhat|): 0.4290,  E(|Yhat-Yhat'|): 0.4326\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2111,  E(|Y-Yhat|): 0.4201,  E(|Yhat-Yhat'|): 0.4180\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2032,  E(|Y-Yhat|): 0.4203,  E(|Yhat-Yhat'|): 0.4342\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2107,  E(|Y-Yhat|): 0.4302,  E(|Yhat-Yhat'|): 0.4390\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4351,  E(|Y-Yhat|): 0.7268,  E(|Yhat-Yhat'|): 0.5834\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4107,  E(|Y-Yhat|): 0.8118,  E(|Yhat-Yhat'|): 0.8023\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4039,  E(|Y-Yhat|): 0.8173,  E(|Yhat-Yhat'|): 0.8267\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4002,  E(|Y-Yhat|): 0.8038,  E(|Yhat-Yhat'|): 0.8071\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4009,  E(|Y-Yhat|): 0.8142,  E(|Yhat-Yhat'|): 0.8265\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2410,  E(|Y-Yhat|): 0.4381,  E(|Yhat-Yhat'|): 0.3943\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2256,  E(|Y-Yhat|): 0.4595,  E(|Yhat-Yhat'|): 0.4678\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2230,  E(|Y-Yhat|): 0.4504,  E(|Yhat-Yhat'|): 0.4550\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2291,  E(|Y-Yhat|): 0.4526,  E(|Yhat-Yhat'|): 0.4470\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2240,  E(|Y-Yhat|): 0.4486,  E(|Yhat-Yhat'|): 0.4491\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5840,  E(|Y-Yhat|): 1.0669,  E(|Yhat-Yhat'|): 0.9658\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5594,  E(|Y-Yhat|): 1.1419,  E(|Yhat-Yhat'|): 1.1651\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5469,  E(|Y-Yhat|): 1.1502,  E(|Yhat-Yhat'|): 1.2067\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5609,  E(|Y-Yhat|): 1.1216,  E(|Yhat-Yhat'|): 1.1214\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9912,  E(|Y-Yhat|): 2.0350,  E(|Yhat-Yhat'|): 2.0877\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9926,  E(|Y-Yhat|): 1.2035,  E(|Yhat-Yhat'|): 0.4218\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9865,  E(|Y-Yhat|): 5.3635,  E(|Yhat-Yhat'|): 8.7539\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9994,  E(|Y-Yhat|): 2.7247,  E(|Yhat-Yhat'|): 3.4506\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0287,  E(|Y-Yhat|): 2.2021,  E(|Yhat-Yhat'|): 2.3468\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9888,  E(|Y-Yhat|): 2.0891,  E(|Yhat-Yhat'|): 2.2006\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2263,  E(|Y-Yhat|): 0.3963,  E(|Yhat-Yhat'|): 0.3401\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2111,  E(|Y-Yhat|): 0.4181,  E(|Yhat-Yhat'|): 0.4140\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2114,  E(|Y-Yhat|): 0.4245,  E(|Yhat-Yhat'|): 0.4264\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2089,  E(|Y-Yhat|): 0.4230,  E(|Yhat-Yhat'|): 0.4282\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2107,  E(|Y-Yhat|): 0.4252,  E(|Yhat-Yhat'|): 0.4291\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4328,  E(|Y-Yhat|): 0.7324,  E(|Yhat-Yhat'|): 0.5993\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4009,  E(|Y-Yhat|): 0.8132,  E(|Yhat-Yhat'|): 0.8245\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4018,  E(|Y-Yhat|): 0.7962,  E(|Yhat-Yhat'|): 0.7887\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4060,  E(|Y-Yhat|): 0.8179,  E(|Yhat-Yhat'|): 0.8238\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4091,  E(|Y-Yhat|): 0.8077,  E(|Yhat-Yhat'|): 0.7972\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2456,  E(|Y-Yhat|): 0.4377,  E(|Yhat-Yhat'|): 0.3842\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2247,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4607\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2215,  E(|Y-Yhat|): 0.4515,  E(|Yhat-Yhat'|): 0.4600\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2309,  E(|Y-Yhat|): 0.4578,  E(|Yhat-Yhat'|): 0.4538\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2327,  E(|Y-Yhat|): 0.4610,  E(|Yhat-Yhat'|): 0.4567\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5919,  E(|Y-Yhat|): 1.0237,  E(|Yhat-Yhat'|): 0.8636\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5546,  E(|Y-Yhat|): 1.1378,  E(|Yhat-Yhat'|): 1.1664\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5627,  E(|Y-Yhat|): 1.1413,  E(|Yhat-Yhat'|): 1.1573\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5595,  E(|Y-Yhat|): 1.1197,  E(|Yhat-Yhat'|): 1.1204\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0089,  E(|Y-Yhat|): 2.0932,  E(|Yhat-Yhat'|): 2.1686\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0529,  E(|Y-Yhat|): 1.2675,  E(|Yhat-Yhat'|): 0.4290\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0261,  E(|Y-Yhat|): 4.8348,  E(|Yhat-Yhat'|): 7.6174\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0046,  E(|Y-Yhat|): 2.0998,  E(|Yhat-Yhat'|): 2.1904\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9461,  E(|Y-Yhat|): 15.9339,  E(|Yhat-Yhat'|): 29.9755\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8664,  E(|Y-Yhat|): 15.8662,  E(|Yhat-Yhat'|): 29.9996\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2320,  E(|Y-Yhat|): 0.3884,  E(|Yhat-Yhat'|): 0.3128\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2107,  E(|Y-Yhat|): 0.4252,  E(|Yhat-Yhat'|): 0.4290\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2119,  E(|Y-Yhat|): 0.4206,  E(|Yhat-Yhat'|): 0.4172\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2137,  E(|Y-Yhat|): 0.4240,  E(|Yhat-Yhat'|): 0.4206\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2102,  E(|Y-Yhat|): 0.4257,  E(|Yhat-Yhat'|): 0.4310\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4374,  E(|Y-Yhat|): 0.7388,  E(|Yhat-Yhat'|): 0.6028\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4068,  E(|Y-Yhat|): 0.8141,  E(|Yhat-Yhat'|): 0.8146\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4077,  E(|Y-Yhat|): 0.8257,  E(|Yhat-Yhat'|): 0.8360\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4034,  E(|Y-Yhat|): 0.8106,  E(|Yhat-Yhat'|): 0.8144\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3960,  E(|Y-Yhat|): 0.8017,  E(|Yhat-Yhat'|): 0.8116\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2425,  E(|Y-Yhat|): 0.4366,  E(|Yhat-Yhat'|): 0.3883\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2256,  E(|Y-Yhat|): 0.4553,  E(|Yhat-Yhat'|): 0.4595\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2254,  E(|Y-Yhat|): 0.4568,  E(|Yhat-Yhat'|): 0.4628\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2310,  E(|Y-Yhat|): 0.4543,  E(|Yhat-Yhat'|): 0.4467\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2252,  E(|Y-Yhat|): 0.4487,  E(|Yhat-Yhat'|): 0.4471\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5790,  E(|Y-Yhat|): 0.9789,  E(|Yhat-Yhat'|): 0.7998\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5354,  E(|Y-Yhat|): 1.0616,  E(|Yhat-Yhat'|): 1.0523\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5364,  E(|Y-Yhat|): 1.0588,  E(|Yhat-Yhat'|): 1.0450\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5350,  E(|Y-Yhat|): 1.0758,  E(|Yhat-Yhat'|): 1.0815\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9794,  E(|Y-Yhat|): 1.9581,  E(|Yhat-Yhat'|): 1.9575\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0552,  E(|Y-Yhat|): 1.2390,  E(|Yhat-Yhat'|): 0.3676\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6908,  E(|Y-Yhat|): 46.4122,  E(|Yhat-Yhat'|): 89.4428\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9630,  E(|Y-Yhat|): 21.6060,  E(|Yhat-Yhat'|): 41.2860\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7746,  E(|Y-Yhat|): 8.8054,  E(|Yhat-Yhat'|): 16.0615\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1309,  E(|Y-Yhat|): 10.4217,  E(|Yhat-Yhat'|): 18.5816\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2206,  E(|Y-Yhat|): 0.3810,  E(|Yhat-Yhat'|): 0.3208\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2024,  E(|Y-Yhat|): 0.4029,  E(|Yhat-Yhat'|): 0.4010\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2015,  E(|Y-Yhat|): 0.4096,  E(|Yhat-Yhat'|): 0.4162\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2058,  E(|Y-Yhat|): 0.4080,  E(|Yhat-Yhat'|): 0.4045\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2092,  E(|Y-Yhat|): 0.4190,  E(|Yhat-Yhat'|): 0.4197\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4195,  E(|Y-Yhat|): 0.6949,  E(|Yhat-Yhat'|): 0.5508\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3788,  E(|Y-Yhat|): 0.7712,  E(|Yhat-Yhat'|): 0.7848\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3909,  E(|Y-Yhat|): 0.7691,  E(|Yhat-Yhat'|): 0.7564\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3750,  E(|Y-Yhat|): 0.7726,  E(|Yhat-Yhat'|): 0.7951\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4026,  E(|Y-Yhat|): 0.8261,  E(|Yhat-Yhat'|): 0.8470\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2365,  E(|Y-Yhat|): 0.4337,  E(|Yhat-Yhat'|): 0.3945\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2180,  E(|Y-Yhat|): 0.4367,  E(|Yhat-Yhat'|): 0.4374\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2182,  E(|Y-Yhat|): 0.4407,  E(|Yhat-Yhat'|): 0.4449\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2183,  E(|Y-Yhat|): 0.4408,  E(|Yhat-Yhat'|): 0.4451\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2288,  E(|Y-Yhat|): 0.4541,  E(|Yhat-Yhat'|): 0.4506\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5968,  E(|Y-Yhat|): 1.0285,  E(|Yhat-Yhat'|): 0.8634\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5595,  E(|Y-Yhat|): 1.1132,  E(|Yhat-Yhat'|): 1.1075\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5609,  E(|Y-Yhat|): 1.1419,  E(|Yhat-Yhat'|): 1.1620\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5625,  E(|Y-Yhat|): 1.1210,  E(|Yhat-Yhat'|): 1.1169\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9833,  E(|Y-Yhat|): 1.9756,  E(|Yhat-Yhat'|): 1.9846\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0074,  E(|Y-Yhat|): 1.3018,  E(|Yhat-Yhat'|): 0.5887\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2451,  E(|Y-Yhat|): 14.9016,  E(|Yhat-Yhat'|): 27.3130\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0797,  E(|Y-Yhat|): 32.3214,  E(|Yhat-Yhat'|): 62.4835\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 32.2255,  E(|Y-Yhat|): 1310.3784,  E(|Yhat-Yhat'|): 2556.3057\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -12.5543,  E(|Y-Yhat|): 1253.0886,  E(|Yhat-Yhat'|): 2531.2859\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2283,  E(|Y-Yhat|): 0.3901,  E(|Yhat-Yhat'|): 0.3237\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2142,  E(|Y-Yhat|): 0.4273,  E(|Yhat-Yhat'|): 0.4263\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2053,  E(|Y-Yhat|): 0.4232,  E(|Yhat-Yhat'|): 0.4359\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2075,  E(|Y-Yhat|): 0.4266,  E(|Yhat-Yhat'|): 0.4382\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2133,  E(|Y-Yhat|): 0.4332,  E(|Yhat-Yhat'|): 0.4397\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4403,  E(|Y-Yhat|): 0.7145,  E(|Yhat-Yhat'|): 0.5484\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4136,  E(|Y-Yhat|): 0.8090,  E(|Yhat-Yhat'|): 0.7908\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4018,  E(|Y-Yhat|): 0.8133,  E(|Yhat-Yhat'|): 0.8231\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3997,  E(|Y-Yhat|): 0.8199,  E(|Yhat-Yhat'|): 0.8405\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4027,  E(|Y-Yhat|): 0.8140,  E(|Yhat-Yhat'|): 0.8227\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2463,  E(|Y-Yhat|): 0.4387,  E(|Yhat-Yhat'|): 0.3848\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2286,  E(|Y-Yhat|): 0.4535,  E(|Yhat-Yhat'|): 0.4498\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2247,  E(|Y-Yhat|): 0.4573,  E(|Yhat-Yhat'|): 0.4652\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2232,  E(|Y-Yhat|): 0.4563,  E(|Yhat-Yhat'|): 0.4663\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2260,  E(|Y-Yhat|): 0.4559,  E(|Yhat-Yhat'|): 0.4597\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0029),\n",
       " tensor(0.2679),\n",
       " tensor(0.0029),\n",
       " tensor(0.0029),\n",
       " tensor(0.0031))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=1, dy=1, k=1, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(0, 5, 100)\n",
    "    y_eval = M0* F.softplus(A0 * x_eval)\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8195f",
   "metadata": {},
   "source": [
    "## True function: cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "622c93e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5457,  E(|Y-Yhat|): 0.9791,  E(|Yhat-Yhat'|): 0.8668\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3874,  E(|Y-Yhat|): 0.7792,  E(|Yhat-Yhat'|): 0.7837\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3817,  E(|Y-Yhat|): 0.7770,  E(|Yhat-Yhat'|): 0.7905\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3811,  E(|Y-Yhat|): 0.7802,  E(|Yhat-Yhat'|): 0.7981\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0456,  E(|Y-Yhat|): 2.2176,  E(|Yhat-Yhat'|): 2.3440\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8006,  E(|Y-Yhat|): 0.9999,  E(|Yhat-Yhat'|): 0.3987\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6382,  E(|Y-Yhat|): 63.5704,  E(|Yhat-Yhat'|): 125.8645\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6848,  E(|Y-Yhat|): 71.4714,  E(|Yhat-Yhat'|): 141.5732\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.5042,  E(|Y-Yhat|): 62.4240,  E(|Yhat-Yhat'|): 121.8395\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.4080,  E(|Y-Yhat|): 130.3737,  E(|Yhat-Yhat'|): 261.5634\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2115,  E(|Y-Yhat|): 0.3711,  E(|Yhat-Yhat'|): 0.3192\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1417,  E(|Y-Yhat|): 0.2882,  E(|Yhat-Yhat'|): 0.2930\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1383,  E(|Y-Yhat|): 0.2845,  E(|Yhat-Yhat'|): 0.2924\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1384,  E(|Y-Yhat|): 0.2815,  E(|Yhat-Yhat'|): 0.2861\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2154,  E(|Y-Yhat|): 0.4377,  E(|Yhat-Yhat'|): 0.4446\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3989,  E(|Y-Yhat|): 0.6528,  E(|Yhat-Yhat'|): 0.5080\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2496,  E(|Y-Yhat|): 0.5214,  E(|Yhat-Yhat'|): 0.5436\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2521,  E(|Y-Yhat|): 0.5030,  E(|Yhat-Yhat'|): 0.5017\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2555,  E(|Y-Yhat|): 0.5081,  E(|Yhat-Yhat'|): 0.5053\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4179,  E(|Y-Yhat|): 0.8304,  E(|Yhat-Yhat'|): 0.8249\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2325,  E(|Y-Yhat|): 0.4040,  E(|Yhat-Yhat'|): 0.3430\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1642,  E(|Y-Yhat|): 0.3406,  E(|Yhat-Yhat'|): 0.3529\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1694,  E(|Y-Yhat|): 0.3373,  E(|Yhat-Yhat'|): 0.3358\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1711,  E(|Y-Yhat|): 0.3364,  E(|Yhat-Yhat'|): 0.3306\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2263,  E(|Y-Yhat|): 0.4601,  E(|Yhat-Yhat'|): 0.4677\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5861,  E(|Y-Yhat|): 1.0646,  E(|Yhat-Yhat'|): 0.9570\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5595,  E(|Y-Yhat|): 1.1340,  E(|Yhat-Yhat'|): 1.1490\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5660,  E(|Y-Yhat|): 1.1274,  E(|Yhat-Yhat'|): 1.1229\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5607,  E(|Y-Yhat|): 1.1564,  E(|Yhat-Yhat'|): 1.1914\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0452,  E(|Y-Yhat|): 2.1468,  E(|Yhat-Yhat'|): 2.2034\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0041,  E(|Y-Yhat|): 1.2403,  E(|Yhat-Yhat'|): 0.4724\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0322,  E(|Y-Yhat|): 2.0909,  E(|Yhat-Yhat'|): 2.1175\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0183,  E(|Y-Yhat|): 1.5948,  E(|Yhat-Yhat'|): 1.1531\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0887,  E(|Y-Yhat|): 5.9901,  E(|Yhat-Yhat'|): 9.8028\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0129,  E(|Y-Yhat|): 5.2886,  E(|Yhat-Yhat'|): 8.5515\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2179,  E(|Y-Yhat|): 0.4078,  E(|Yhat-Yhat'|): 0.3796\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2126,  E(|Y-Yhat|): 0.4225,  E(|Yhat-Yhat'|): 0.4199\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2074,  E(|Y-Yhat|): 0.4224,  E(|Yhat-Yhat'|): 0.4301\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2112,  E(|Y-Yhat|): 0.4215,  E(|Yhat-Yhat'|): 0.4205\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2104,  E(|Y-Yhat|): 0.4273,  E(|Yhat-Yhat'|): 0.4338\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4265,  E(|Y-Yhat|): 0.7281,  E(|Yhat-Yhat'|): 0.6034\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4051,  E(|Y-Yhat|): 0.8090,  E(|Yhat-Yhat'|): 0.8079\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3919,  E(|Y-Yhat|): 0.8101,  E(|Yhat-Yhat'|): 0.8364\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4057,  E(|Y-Yhat|): 0.8061,  E(|Yhat-Yhat'|): 0.8009\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3960,  E(|Y-Yhat|): 0.8045,  E(|Yhat-Yhat'|): 0.8170\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2375,  E(|Y-Yhat|): 0.4428,  E(|Yhat-Yhat'|): 0.4108\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2260,  E(|Y-Yhat|): 0.4506,  E(|Yhat-Yhat'|): 0.4493\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2222,  E(|Y-Yhat|): 0.4517,  E(|Yhat-Yhat'|): 0.4590\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2232,  E(|Y-Yhat|): 0.4552,  E(|Yhat-Yhat'|): 0.4641\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2309,  E(|Y-Yhat|): 0.4672,  E(|Yhat-Yhat'|): 0.4726\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6123,  E(|Y-Yhat|): 0.9934,  E(|Yhat-Yhat'|): 0.7622\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5718,  E(|Y-Yhat|): 1.1343,  E(|Yhat-Yhat'|): 1.1249\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5557,  E(|Y-Yhat|): 1.1214,  E(|Yhat-Yhat'|): 1.1315\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5635,  E(|Y-Yhat|): 1.1333,  E(|Yhat-Yhat'|): 1.1395\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0081,  E(|Y-Yhat|): 2.0743,  E(|Yhat-Yhat'|): 2.1323\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0318,  E(|Y-Yhat|): 1.2425,  E(|Yhat-Yhat'|): 0.4216\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0854,  E(|Y-Yhat|): 35.9026,  E(|Yhat-Yhat'|): 69.6343\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.0530,  E(|Y-Yhat|): 122.7770,  E(|Yhat-Yhat'|): 245.6600\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2100,  E(|Y-Yhat|): 58.2936,  E(|Yhat-Yhat'|): 114.1671\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.3350,  E(|Y-Yhat|): 60.4434,  E(|Yhat-Yhat'|): 118.2167\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2266,  E(|Y-Yhat|): 0.3871,  E(|Yhat-Yhat'|): 0.3210\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2105,  E(|Y-Yhat|): 0.4294,  E(|Yhat-Yhat'|): 0.4378\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2131,  E(|Y-Yhat|): 0.4261,  E(|Yhat-Yhat'|): 0.4260\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2178,  E(|Y-Yhat|): 0.4306,  E(|Yhat-Yhat'|): 0.4255\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2158,  E(|Y-Yhat|): 0.4265,  E(|Yhat-Yhat'|): 0.4215\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4244,  E(|Y-Yhat|): 0.7373,  E(|Yhat-Yhat'|): 0.6259\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4072,  E(|Y-Yhat|): 0.8198,  E(|Yhat-Yhat'|): 0.8250\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4056,  E(|Y-Yhat|): 0.8065,  E(|Yhat-Yhat'|): 0.8018\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3975,  E(|Y-Yhat|): 0.8173,  E(|Yhat-Yhat'|): 0.8396\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4203,  E(|Y-Yhat|): 0.8325,  E(|Yhat-Yhat'|): 0.8243\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2421,  E(|Y-Yhat|): 0.4374,  E(|Yhat-Yhat'|): 0.3905\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2260,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4581\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2243,  E(|Y-Yhat|): 0.4560,  E(|Yhat-Yhat'|): 0.4634\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2280,  E(|Y-Yhat|): 0.4579,  E(|Yhat-Yhat'|): 0.4598\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2280,  E(|Y-Yhat|): 0.4636,  E(|Yhat-Yhat'|): 0.4712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5909,  E(|Y-Yhat|): 1.0476,  E(|Yhat-Yhat'|): 0.9134\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5705,  E(|Y-Yhat|): 1.1474,  E(|Yhat-Yhat'|): 1.1539\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5463,  E(|Y-Yhat|): 1.1237,  E(|Yhat-Yhat'|): 1.1548\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5551,  E(|Y-Yhat|): 1.1440,  E(|Yhat-Yhat'|): 1.1778\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0012,  E(|Y-Yhat|): 2.1316,  E(|Yhat-Yhat'|): 2.2607\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9917,  E(|Y-Yhat|): 1.2287,  E(|Yhat-Yhat'|): 0.4741\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9867,  E(|Y-Yhat|): 2.0113,  E(|Yhat-Yhat'|): 2.0491\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0025,  E(|Y-Yhat|): 1.8766,  E(|Yhat-Yhat'|): 1.7482\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0438,  E(|Y-Yhat|): 3.3670,  E(|Yhat-Yhat'|): 4.6465\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9737,  E(|Y-Yhat|): 3.0922,  E(|Yhat-Yhat'|): 4.2369\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2247,  E(|Y-Yhat|): 0.3965,  E(|Yhat-Yhat'|): 0.3436\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2102,  E(|Y-Yhat|): 0.4254,  E(|Yhat-Yhat'|): 0.4303\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2108,  E(|Y-Yhat|): 0.4220,  E(|Yhat-Yhat'|): 0.4225\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2155,  E(|Y-Yhat|): 0.4252,  E(|Yhat-Yhat'|): 0.4195\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2134,  E(|Y-Yhat|): 0.4265,  E(|Yhat-Yhat'|): 0.4263\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4218,  E(|Y-Yhat|): 0.7685,  E(|Yhat-Yhat'|): 0.6935\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4043,  E(|Y-Yhat|): 0.8212,  E(|Yhat-Yhat'|): 0.8337\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4064,  E(|Y-Yhat|): 0.8300,  E(|Yhat-Yhat'|): 0.8472\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3961,  E(|Y-Yhat|): 0.8084,  E(|Yhat-Yhat'|): 0.8246\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4078,  E(|Y-Yhat|): 0.8125,  E(|Yhat-Yhat'|): 0.8095\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2481,  E(|Y-Yhat|): 0.4320,  E(|Yhat-Yhat'|): 0.3678\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2274,  E(|Y-Yhat|): 0.4545,  E(|Yhat-Yhat'|): 0.4543\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2276,  E(|Y-Yhat|): 0.4553,  E(|Yhat-Yhat'|): 0.4554\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2297,  E(|Y-Yhat|): 0.4639,  E(|Yhat-Yhat'|): 0.4685\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2266,  E(|Y-Yhat|): 0.4552,  E(|Yhat-Yhat'|): 0.4572\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5445,  E(|Y-Yhat|): 0.9813,  E(|Yhat-Yhat'|): 0.8735\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4151,  E(|Y-Yhat|): 0.8482,  E(|Yhat-Yhat'|): 0.8662\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4068,  E(|Y-Yhat|): 0.8196,  E(|Yhat-Yhat'|): 0.8256\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4101,  E(|Y-Yhat|): 0.8226,  E(|Yhat-Yhat'|): 0.8248\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9866,  E(|Y-Yhat|): 2.0233,  E(|Yhat-Yhat'|): 2.0735\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7647,  E(|Y-Yhat|): 1.0785,  E(|Yhat-Yhat'|): 0.6275\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0941,  E(|Y-Yhat|): 32.9215,  E(|Yhat-Yhat'|): 63.6548\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5831,  E(|Y-Yhat|): 5.4724,  E(|Yhat-Yhat'|): 9.7787\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7704,  E(|Y-Yhat|): 23.1390,  E(|Yhat-Yhat'|): 44.7374\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1401,  E(|Y-Yhat|): 42.6447,  E(|Yhat-Yhat'|): 83.0092\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2224,  E(|Y-Yhat|): 0.3868,  E(|Yhat-Yhat'|): 0.3289\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1551,  E(|Y-Yhat|): 0.3016,  E(|Yhat-Yhat'|): 0.2929\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1456,  E(|Y-Yhat|): 0.3022,  E(|Yhat-Yhat'|): 0.3133\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1508,  E(|Y-Yhat|): 0.3090,  E(|Yhat-Yhat'|): 0.3165\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2077,  E(|Y-Yhat|): 0.4214,  E(|Yhat-Yhat'|): 0.4274\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3998,  E(|Y-Yhat|): 0.6822,  E(|Yhat-Yhat'|): 0.5646\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2747,  E(|Y-Yhat|): 0.5633,  E(|Yhat-Yhat'|): 0.5771\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2758,  E(|Y-Yhat|): 0.5527,  E(|Yhat-Yhat'|): 0.5537\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2753,  E(|Y-Yhat|): 0.5469,  E(|Yhat-Yhat'|): 0.5434\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4011,  E(|Y-Yhat|): 0.7944,  E(|Yhat-Yhat'|): 0.7866\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2279,  E(|Y-Yhat|): 0.4173,  E(|Yhat-Yhat'|): 0.3787\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1764,  E(|Y-Yhat|): 0.3608,  E(|Yhat-Yhat'|): 0.3689\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1764,  E(|Y-Yhat|): 0.3578,  E(|Yhat-Yhat'|): 0.3628\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1710,  E(|Y-Yhat|): 0.3512,  E(|Yhat-Yhat'|): 0.3605\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2248,  E(|Y-Yhat|): 0.4520,  E(|Yhat-Yhat'|): 0.4542\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5948,  E(|Y-Yhat|): 1.0162,  E(|Yhat-Yhat'|): 0.8428\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5704,  E(|Y-Yhat|): 1.1504,  E(|Yhat-Yhat'|): 1.1599\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5687,  E(|Y-Yhat|): 1.1227,  E(|Yhat-Yhat'|): 1.1080\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5577,  E(|Y-Yhat|): 1.1315,  E(|Yhat-Yhat'|): 1.1477\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9631,  E(|Y-Yhat|): 2.0237,  E(|Yhat-Yhat'|): 2.1214\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0148,  E(|Y-Yhat|): 1.1972,  E(|Yhat-Yhat'|): 0.3649\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9870,  E(|Y-Yhat|): 1.8274,  E(|Yhat-Yhat'|): 1.6809\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0122,  E(|Y-Yhat|): 4.3482,  E(|Yhat-Yhat'|): 6.6719\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9925,  E(|Y-Yhat|): 4.6606,  E(|Yhat-Yhat'|): 7.3362\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9805,  E(|Y-Yhat|): 4.4747,  E(|Yhat-Yhat'|): 6.9885\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2305,  E(|Y-Yhat|): 0.3865,  E(|Yhat-Yhat'|): 0.3120\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2133,  E(|Y-Yhat|): 0.4219,  E(|Yhat-Yhat'|): 0.4172\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2109,  E(|Y-Yhat|): 0.4235,  E(|Yhat-Yhat'|): 0.4252\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2081,  E(|Y-Yhat|): 0.4215,  E(|Yhat-Yhat'|): 0.4267\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2151,  E(|Y-Yhat|): 0.4278,  E(|Yhat-Yhat'|): 0.4254\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4364,  E(|Y-Yhat|): 0.7280,  E(|Yhat-Yhat'|): 0.5832\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4037,  E(|Y-Yhat|): 0.8105,  E(|Yhat-Yhat'|): 0.8136\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4095,  E(|Y-Yhat|): 0.8300,  E(|Yhat-Yhat'|): 0.8411\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4023,  E(|Y-Yhat|): 0.8206,  E(|Yhat-Yhat'|): 0.8366\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4034,  E(|Y-Yhat|): 0.8148,  E(|Yhat-Yhat'|): 0.8228\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2416,  E(|Y-Yhat|): 0.4390,  E(|Yhat-Yhat'|): 0.3947\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2300,  E(|Y-Yhat|): 0.4533,  E(|Yhat-Yhat'|): 0.4466\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2273,  E(|Y-Yhat|): 0.4577,  E(|Yhat-Yhat'|): 0.4608\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2253,  E(|Y-Yhat|): 0.4530,  E(|Yhat-Yhat'|): 0.4554\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2260,  E(|Y-Yhat|): 0.4519,  E(|Yhat-Yhat'|): 0.4518\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4250,  E(|Y-Yhat|): 0.7853,  E(|Yhat-Yhat'|): 0.7207\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1031,  E(|Y-Yhat|): 0.2227,  E(|Yhat-Yhat'|): 0.2392\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0969,  E(|Y-Yhat|): 0.2093,  E(|Yhat-Yhat'|): 0.2247\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0971,  E(|Y-Yhat|): 0.2024,  E(|Yhat-Yhat'|): 0.2107\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0704,  E(|Y-Yhat|): 2.7490,  E(|Yhat-Yhat'|): 3.3572\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6855,  E(|Y-Yhat|): 0.8892,  E(|Yhat-Yhat'|): 0.4074\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1634,  E(|Y-Yhat|): 4.4312,  E(|Yhat-Yhat'|): 8.5356\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2949,  E(|Y-Yhat|): 15.3381,  E(|Yhat-Yhat'|): 30.0866\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0659,  E(|Y-Yhat|): 1.0734,  E(|Yhat-Yhat'|): 2.0149\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9803,  E(|Y-Yhat|): 46.8295,  E(|Yhat-Yhat'|): 89.6984\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1883,  E(|Y-Yhat|): 0.3386,  E(|Yhat-Yhat'|): 0.3007\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0140,  E(|Y-Yhat|): 0.0378,  E(|Yhat-Yhat'|): 0.0476\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0135,  E(|Y-Yhat|): 0.0336,  E(|Yhat-Yhat'|): 0.0403\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0141,  E(|Y-Yhat|): 0.0342,  E(|Yhat-Yhat'|): 0.0403\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2407,  E(|Y-Yhat|): 0.4758,  E(|Yhat-Yhat'|): 0.4701\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3259,  E(|Y-Yhat|): 0.5408,  E(|Yhat-Yhat'|): 0.4299\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0278,  E(|Y-Yhat|): 0.0697,  E(|Yhat-Yhat'|): 0.0838\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0264,  E(|Y-Yhat|): 0.0695,  E(|Yhat-Yhat'|): 0.0862\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0255,  E(|Y-Yhat|): 0.0615,  E(|Yhat-Yhat'|): 0.0719\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4492,  E(|Y-Yhat|): 0.9594,  E(|Yhat-Yhat'|): 1.0206\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2178,  E(|Y-Yhat|): 0.3793,  E(|Yhat-Yhat'|): 0.3230\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0237,  E(|Y-Yhat|): 0.0551,  E(|Yhat-Yhat'|): 0.0629\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0257,  E(|Y-Yhat|): 0.0586,  E(|Yhat-Yhat'|): 0.0658\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0246,  E(|Y-Yhat|): 0.0535,  E(|Yhat-Yhat'|): 0.0578\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2419,  E(|Y-Yhat|): 0.4875,  E(|Yhat-Yhat'|): 0.4911\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5910,  E(|Y-Yhat|): 1.0197,  E(|Yhat-Yhat'|): 0.8574\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5662,  E(|Y-Yhat|): 1.1348,  E(|Yhat-Yhat'|): 1.1370\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5496,  E(|Y-Yhat|): 1.1398,  E(|Yhat-Yhat'|): 1.1803\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5530,  E(|Y-Yhat|): 1.1181,  E(|Yhat-Yhat'|): 1.1302\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9889,  E(|Y-Yhat|): 2.0923,  E(|Yhat-Yhat'|): 2.2067\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0574,  E(|Y-Yhat|): 1.2713,  E(|Yhat-Yhat'|): 0.4278\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0322,  E(|Y-Yhat|): 3.3565,  E(|Yhat-Yhat'|): 4.6487\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0001,  E(|Y-Yhat|): 1.7841,  E(|Yhat-Yhat'|): 1.5678\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9040,  E(|Y-Yhat|): 47.7866,  E(|Yhat-Yhat'|): 93.7654\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6701,  E(|Y-Yhat|): 48.9044,  E(|Yhat-Yhat'|): 96.4687\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2326,  E(|Y-Yhat|): 0.3888,  E(|Yhat-Yhat'|): 0.3123\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2134,  E(|Y-Yhat|): 0.4242,  E(|Yhat-Yhat'|): 0.4216\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2125,  E(|Y-Yhat|): 0.4278,  E(|Yhat-Yhat'|): 0.4306\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2178,  E(|Y-Yhat|): 0.4325,  E(|Yhat-Yhat'|): 0.4294\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2102,  E(|Y-Yhat|): 0.4272,  E(|Yhat-Yhat'|): 0.4341\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4380,  E(|Y-Yhat|): 0.7399,  E(|Yhat-Yhat'|): 0.6038\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4048,  E(|Y-Yhat|): 0.8261,  E(|Yhat-Yhat'|): 0.8425\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4000,  E(|Y-Yhat|): 0.8198,  E(|Yhat-Yhat'|): 0.8396\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4027,  E(|Y-Yhat|): 0.8016,  E(|Yhat-Yhat'|): 0.7978\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3948,  E(|Y-Yhat|): 0.8079,  E(|Yhat-Yhat'|): 0.8260\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2428,  E(|Y-Yhat|): 0.4371,  E(|Yhat-Yhat'|): 0.3885\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2240,  E(|Y-Yhat|): 0.4534,  E(|Yhat-Yhat'|): 0.4588\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2334,  E(|Y-Yhat|): 0.4683,  E(|Yhat-Yhat'|): 0.4698\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2303,  E(|Y-Yhat|): 0.4594,  E(|Yhat-Yhat'|): 0.4584\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2274,  E(|Y-Yhat|): 0.4551,  E(|Yhat-Yhat'|): 0.4554\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6017,  E(|Y-Yhat|): 1.0037,  E(|Yhat-Yhat'|): 0.8040\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5628,  E(|Y-Yhat|): 1.1155,  E(|Yhat-Yhat'|): 1.1054\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5673,  E(|Y-Yhat|): 1.1244,  E(|Yhat-Yhat'|): 1.1143\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5721,  E(|Y-Yhat|): 1.1553,  E(|Yhat-Yhat'|): 1.1664\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9797,  E(|Y-Yhat|): 2.0442,  E(|Yhat-Yhat'|): 2.1291\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.1290,  E(|Y-Yhat|): 1.3189,  E(|Yhat-Yhat'|): 0.3798\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4636,  E(|Y-Yhat|): 26.5127,  E(|Yhat-Yhat'|): 50.0982\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.1377,  E(|Y-Yhat|): 21.9928,  E(|Yhat-Yhat'|): 41.7101\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8259,  E(|Y-Yhat|): 9.1138,  E(|Yhat-Yhat'|): 16.5759\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0957,  E(|Y-Yhat|): 9.6919,  E(|Yhat-Yhat'|): 17.1923\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2277,  E(|Y-Yhat|): 0.3894,  E(|Yhat-Yhat'|): 0.3232\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2120,  E(|Y-Yhat|): 0.4200,  E(|Yhat-Yhat'|): 0.4160\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2114,  E(|Y-Yhat|): 0.4256,  E(|Yhat-Yhat'|): 0.4283\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2115,  E(|Y-Yhat|): 0.4272,  E(|Yhat-Yhat'|): 0.4313\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2117,  E(|Y-Yhat|): 0.4265,  E(|Yhat-Yhat'|): 0.4296\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4312,  E(|Y-Yhat|): 0.7121,  E(|Yhat-Yhat'|): 0.5619\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4028,  E(|Y-Yhat|): 0.8287,  E(|Yhat-Yhat'|): 0.8519\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4070,  E(|Y-Yhat|): 0.8181,  E(|Yhat-Yhat'|): 0.8222\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3991,  E(|Y-Yhat|): 0.8129,  E(|Yhat-Yhat'|): 0.8275\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4066,  E(|Y-Yhat|): 0.8158,  E(|Yhat-Yhat'|): 0.8184\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2427,  E(|Y-Yhat|): 0.4422,  E(|Yhat-Yhat'|): 0.3990\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2273,  E(|Y-Yhat|): 0.4511,  E(|Yhat-Yhat'|): 0.4477\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2265,  E(|Y-Yhat|): 0.4578,  E(|Yhat-Yhat'|): 0.4626\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2247,  E(|Y-Yhat|): 0.4569,  E(|Yhat-Yhat'|): 0.4643\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2283,  E(|Y-Yhat|): 0.4523,  E(|Yhat-Yhat'|): 0.4480\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5965,  E(|Y-Yhat|): 1.0281,  E(|Yhat-Yhat'|): 0.8631\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5620,  E(|Y-Yhat|): 1.1192,  E(|Yhat-Yhat'|): 1.1143\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5690,  E(|Y-Yhat|): 1.1420,  E(|Yhat-Yhat'|): 1.1460\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5631,  E(|Y-Yhat|): 1.1433,  E(|Yhat-Yhat'|): 1.1603\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0008,  E(|Y-Yhat|): 2.0945,  E(|Yhat-Yhat'|): 2.1874\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0096,  E(|Y-Yhat|): 1.3114,  E(|Yhat-Yhat'|): 0.6036\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2299,  E(|Y-Yhat|): 16.3891,  E(|Yhat-Yhat'|): 30.3185\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0224,  E(|Y-Yhat|): 47.1040,  E(|Yhat-Yhat'|): 92.1631\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 20.1313,  E(|Y-Yhat|): 1063.6517,  E(|Yhat-Yhat'|): 2087.0406\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -11.4889,  E(|Y-Yhat|): 1026.0251,  E(|Yhat-Yhat'|): 2075.0281\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2287,  E(|Y-Yhat|): 0.3905,  E(|Yhat-Yhat'|): 0.3236\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2148,  E(|Y-Yhat|): 0.4288,  E(|Yhat-Yhat'|): 0.4282\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2069,  E(|Y-Yhat|): 0.4171,  E(|Yhat-Yhat'|): 0.4204\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2112,  E(|Y-Yhat|): 0.4287,  E(|Yhat-Yhat'|): 0.4350\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2132,  E(|Y-Yhat|): 0.4213,  E(|Yhat-Yhat'|): 0.4161\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4409,  E(|Y-Yhat|): 0.7178,  E(|Yhat-Yhat'|): 0.5539\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4144,  E(|Y-Yhat|): 0.8096,  E(|Yhat-Yhat'|): 0.7904\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4097,  E(|Y-Yhat|): 0.8134,  E(|Yhat-Yhat'|): 0.8075\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3990,  E(|Y-Yhat|): 0.8102,  E(|Yhat-Yhat'|): 0.8224\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4037,  E(|Y-Yhat|): 0.8086,  E(|Yhat-Yhat'|): 0.8098\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2470,  E(|Y-Yhat|): 0.4392,  E(|Yhat-Yhat'|): 0.3845\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2294,  E(|Y-Yhat|): 0.4606,  E(|Yhat-Yhat'|): 0.4624\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2255,  E(|Y-Yhat|): 0.4583,  E(|Yhat-Yhat'|): 0.4655\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2273,  E(|Y-Yhat|): 0.4592,  E(|Yhat-Yhat'|): 0.4637\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2262,  E(|Y-Yhat|): 0.4560,  E(|Yhat-Yhat'|): 0.4596\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0131),\n",
       " tensor(0.6991),\n",
       " tensor(0.0134),\n",
       " tensor(0.0143),\n",
       " tensor(0.0136))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=1, dy=1, k=1, true_function =\"cubic\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(2, -2, 100)\n",
    "    y_eval = M0* (A0 * x_eval)**3 / 3.0\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b983e2a",
   "metadata": {},
   "source": [
    "## True function: square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78318581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5813,  E(|Y-Yhat|): 1.0212,  E(|Yhat-Yhat'|): 0.8798\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5240,  E(|Y-Yhat|): 1.0629,  E(|Yhat-Yhat'|): 1.0779\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5278,  E(|Y-Yhat|): 1.0832,  E(|Yhat-Yhat'|): 1.1107\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5341,  E(|Y-Yhat|): 1.0760,  E(|Yhat-Yhat'|): 1.0838\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0381,  E(|Y-Yhat|): 2.1034,  E(|Yhat-Yhat'|): 2.1305\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9477,  E(|Y-Yhat|): 1.1769,  E(|Yhat-Yhat'|): 0.4583\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0686,  E(|Y-Yhat|): 56.8635,  E(|Yhat-Yhat'|): 111.5897\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0674,  E(|Y-Yhat|): 81.6971,  E(|Yhat-Yhat'|): 163.2594\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.3110,  E(|Y-Yhat|): 86.7019,  E(|Yhat-Yhat'|): 168.7817\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.3909,  E(|Y-Yhat|): 91.3810,  E(|Yhat-Yhat'|): 179.9801\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2222,  E(|Y-Yhat|): 0.3878,  E(|Yhat-Yhat'|): 0.3311\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2019,  E(|Y-Yhat|): 0.4056,  E(|Yhat-Yhat'|): 0.4073\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2014,  E(|Y-Yhat|): 0.4016,  E(|Yhat-Yhat'|): 0.4006\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1988,  E(|Y-Yhat|): 0.4032,  E(|Yhat-Yhat'|): 0.4087\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2153,  E(|Y-Yhat|): 0.4362,  E(|Yhat-Yhat'|): 0.4419\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4278,  E(|Y-Yhat|): 0.6938,  E(|Yhat-Yhat'|): 0.5319\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3772,  E(|Y-Yhat|): 0.7418,  E(|Yhat-Yhat'|): 0.7291\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3700,  E(|Y-Yhat|): 0.7411,  E(|Yhat-Yhat'|): 0.7422\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3759,  E(|Y-Yhat|): 0.7507,  E(|Yhat-Yhat'|): 0.7497\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4049,  E(|Y-Yhat|): 0.8193,  E(|Yhat-Yhat'|): 0.8287\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2466,  E(|Y-Yhat|): 0.4200,  E(|Yhat-Yhat'|): 0.3468\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2154,  E(|Y-Yhat|): 0.4291,  E(|Yhat-Yhat'|): 0.4275\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2177,  E(|Y-Yhat|): 0.4404,  E(|Yhat-Yhat'|): 0.4454\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2192,  E(|Y-Yhat|): 0.4394,  E(|Yhat-Yhat'|): 0.4405\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2299,  E(|Y-Yhat|): 0.4627,  E(|Yhat-Yhat'|): 0.4656\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5860,  E(|Y-Yhat|): 1.0648,  E(|Yhat-Yhat'|): 0.9576\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5618,  E(|Y-Yhat|): 1.1466,  E(|Yhat-Yhat'|): 1.1697\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5713,  E(|Y-Yhat|): 1.1494,  E(|Yhat-Yhat'|): 1.1562\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5643,  E(|Y-Yhat|): 1.1566,  E(|Yhat-Yhat'|): 1.1846\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9863,  E(|Y-Yhat|): 2.0861,  E(|Yhat-Yhat'|): 2.1996\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0051,  E(|Y-Yhat|): 1.2434,  E(|Yhat-Yhat'|): 0.4765\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0441,  E(|Y-Yhat|): 2.4147,  E(|Yhat-Yhat'|): 2.7412\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0131,  E(|Y-Yhat|): 1.6643,  E(|Yhat-Yhat'|): 1.3025\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0312,  E(|Y-Yhat|): 2.6684,  E(|Yhat-Yhat'|): 3.2745\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9722,  E(|Y-Yhat|): 2.5279,  E(|Yhat-Yhat'|): 3.1114\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2179,  E(|Y-Yhat|): 0.4083,  E(|Yhat-Yhat'|): 0.3807\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2108,  E(|Y-Yhat|): 0.4217,  E(|Yhat-Yhat'|): 0.4218\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2135,  E(|Y-Yhat|): 0.4267,  E(|Yhat-Yhat'|): 0.4264\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2103,  E(|Y-Yhat|): 0.4281,  E(|Yhat-Yhat'|): 0.4356\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2121,  E(|Y-Yhat|): 0.4338,  E(|Yhat-Yhat'|): 0.4434\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4269,  E(|Y-Yhat|): 0.7289,  E(|Yhat-Yhat'|): 0.6040\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4007,  E(|Y-Yhat|): 0.8037,  E(|Yhat-Yhat'|): 0.8061\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4045,  E(|Y-Yhat|): 0.8142,  E(|Yhat-Yhat'|): 0.8194\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3969,  E(|Y-Yhat|): 0.8146,  E(|Yhat-Yhat'|): 0.8354\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4047,  E(|Y-Yhat|): 0.8300,  E(|Yhat-Yhat'|): 0.8505\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2376,  E(|Y-Yhat|): 0.4436,  E(|Yhat-Yhat'|): 0.4120\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2254,  E(|Y-Yhat|): 0.4498,  E(|Yhat-Yhat'|): 0.4487\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2273,  E(|Y-Yhat|): 0.4528,  E(|Yhat-Yhat'|): 0.4510\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2235,  E(|Y-Yhat|): 0.4520,  E(|Yhat-Yhat'|): 0.4569\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2278,  E(|Y-Yhat|): 0.4591,  E(|Yhat-Yhat'|): 0.4626\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6125,  E(|Y-Yhat|): 0.9949,  E(|Yhat-Yhat'|): 0.7648\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5715,  E(|Y-Yhat|): 1.1362,  E(|Yhat-Yhat'|): 1.1295\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5632,  E(|Y-Yhat|): 1.1246,  E(|Yhat-Yhat'|): 1.1228\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5665,  E(|Y-Yhat|): 1.1333,  E(|Yhat-Yhat'|): 1.1336\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0209,  E(|Y-Yhat|): 2.0529,  E(|Yhat-Yhat'|): 2.0640\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0319,  E(|Y-Yhat|): 1.2427,  E(|Yhat-Yhat'|): 0.4217\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2840,  E(|Y-Yhat|): 44.7760,  E(|Yhat-Yhat'|): 86.9840\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.0669,  E(|Y-Yhat|): 91.6973,  E(|Yhat-Yhat'|): 183.5283\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.1688,  E(|Y-Yhat|): 67.0838,  E(|Yhat-Yhat'|): 131.8300\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0293,  E(|Y-Yhat|): 71.0359,  E(|Yhat-Yhat'|): 140.0134\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2267,  E(|Y-Yhat|): 0.3872,  E(|Yhat-Yhat'|): 0.3210\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2106,  E(|Y-Yhat|): 0.4280,  E(|Yhat-Yhat'|): 0.4347\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2084,  E(|Y-Yhat|): 0.4190,  E(|Yhat-Yhat'|): 0.4212\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2094,  E(|Y-Yhat|): 0.4282,  E(|Yhat-Yhat'|): 0.4376\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2149,  E(|Y-Yhat|): 0.4332,  E(|Yhat-Yhat'|): 0.4365\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4244,  E(|Y-Yhat|): 0.7373,  E(|Yhat-Yhat'|): 0.6258\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4048,  E(|Y-Yhat|): 0.8121,  E(|Yhat-Yhat'|): 0.8145\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4139,  E(|Y-Yhat|): 0.8169,  E(|Yhat-Yhat'|): 0.8060\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3939,  E(|Y-Yhat|): 0.8100,  E(|Yhat-Yhat'|): 0.8322\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4122,  E(|Y-Yhat|): 0.8425,  E(|Yhat-Yhat'|): 0.8606\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2421,  E(|Y-Yhat|): 0.4374,  E(|Yhat-Yhat'|): 0.3905\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2261,  E(|Y-Yhat|): 0.4559,  E(|Yhat-Yhat'|): 0.4596\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2276,  E(|Y-Yhat|): 0.4584,  E(|Yhat-Yhat'|): 0.4616\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2270,  E(|Y-Yhat|): 0.4636,  E(|Yhat-Yhat'|): 0.4731\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2305,  E(|Y-Yhat|): 0.4661,  E(|Yhat-Yhat'|): 0.4711\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5937,  E(|Y-Yhat|): 1.0481,  E(|Yhat-Yhat'|): 0.9088\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5620,  E(|Y-Yhat|): 1.1549,  E(|Yhat-Yhat'|): 1.1856\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5601,  E(|Y-Yhat|): 1.1272,  E(|Yhat-Yhat'|): 1.1341\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5630,  E(|Y-Yhat|): 1.1572,  E(|Yhat-Yhat'|): 1.1884\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9821,  E(|Y-Yhat|): 2.1015,  E(|Yhat-Yhat'|): 2.2388\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9941,  E(|Y-Yhat|): 1.2331,  E(|Yhat-Yhat'|): 0.4780\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9927,  E(|Y-Yhat|): 1.8321,  E(|Yhat-Yhat'|): 1.6787\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9873,  E(|Y-Yhat|): 2.5860,  E(|Yhat-Yhat'|): 3.1974\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0299,  E(|Y-Yhat|): 8.2699,  E(|Yhat-Yhat'|): 14.4801\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9666,  E(|Y-Yhat|): 7.5538,  E(|Yhat-Yhat'|): 13.1744\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2251,  E(|Y-Yhat|): 0.3972,  E(|Yhat-Yhat'|): 0.3442\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2081,  E(|Y-Yhat|): 0.4249,  E(|Yhat-Yhat'|): 0.4336\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2119,  E(|Y-Yhat|): 0.4239,  E(|Yhat-Yhat'|): 0.4239\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2091,  E(|Y-Yhat|): 0.4279,  E(|Yhat-Yhat'|): 0.4377\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2100,  E(|Y-Yhat|): 0.4299,  E(|Yhat-Yhat'|): 0.4397\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4225,  E(|Y-Yhat|): 0.7692,  E(|Yhat-Yhat'|): 0.6934\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4091,  E(|Y-Yhat|): 0.8125,  E(|Yhat-Yhat'|): 0.8070\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4054,  E(|Y-Yhat|): 0.8177,  E(|Yhat-Yhat'|): 0.8247\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3905,  E(|Y-Yhat|): 0.7981,  E(|Yhat-Yhat'|): 0.8152\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4152,  E(|Y-Yhat|): 0.8272,  E(|Yhat-Yhat'|): 0.8240\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2481,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.3684\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2262,  E(|Y-Yhat|): 0.4538,  E(|Yhat-Yhat'|): 0.4552\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2276,  E(|Y-Yhat|): 0.4518,  E(|Yhat-Yhat'|): 0.4485\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2296,  E(|Y-Yhat|): 0.4657,  E(|Yhat-Yhat'|): 0.4721\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2250,  E(|Y-Yhat|): 0.4619,  E(|Yhat-Yhat'|): 0.4739\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5785,  E(|Y-Yhat|): 1.0199,  E(|Yhat-Yhat'|): 0.8828\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5403,  E(|Y-Yhat|): 1.0766,  E(|Yhat-Yhat'|): 1.0727\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5391,  E(|Y-Yhat|): 1.0800,  E(|Yhat-Yhat'|): 1.0817\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5432,  E(|Y-Yhat|): 1.0759,  E(|Yhat-Yhat'|): 1.0654\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9625,  E(|Y-Yhat|): 1.9150,  E(|Yhat-Yhat'|): 1.9050\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9448,  E(|Y-Yhat|): 1.2153,  E(|Yhat-Yhat'|): 0.5410\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9477,  E(|Y-Yhat|): 6.2476,  E(|Yhat-Yhat'|): 10.5999\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9458,  E(|Y-Yhat|): 2.2738,  E(|Yhat-Yhat'|): 2.6560\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9308,  E(|Y-Yhat|): 1.7995,  E(|Yhat-Yhat'|): 1.7375\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9779,  E(|Y-Yhat|): 1.9052,  E(|Yhat-Yhat'|): 1.8546\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2328,  E(|Y-Yhat|): 0.4001,  E(|Yhat-Yhat'|): 0.3345\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2039,  E(|Y-Yhat|): 0.4019,  E(|Yhat-Yhat'|): 0.3960\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2070,  E(|Y-Yhat|): 0.4038,  E(|Yhat-Yhat'|): 0.3935\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2059,  E(|Y-Yhat|): 0.4080,  E(|Yhat-Yhat'|): 0.4042\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2087,  E(|Y-Yhat|): 0.4226,  E(|Yhat-Yhat'|): 0.4279\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4284,  E(|Y-Yhat|): 0.7231,  E(|Yhat-Yhat'|): 0.5893\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3883,  E(|Y-Yhat|): 0.7745,  E(|Yhat-Yhat'|): 0.7725\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3834,  E(|Y-Yhat|): 0.7608,  E(|Yhat-Yhat'|): 0.7548\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3808,  E(|Y-Yhat|): 0.7666,  E(|Yhat-Yhat'|): 0.7715\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4015,  E(|Y-Yhat|): 0.7931,  E(|Yhat-Yhat'|): 0.7832\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2397,  E(|Y-Yhat|): 0.4340,  E(|Yhat-Yhat'|): 0.3887\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2212,  E(|Y-Yhat|): 0.4417,  E(|Yhat-Yhat'|): 0.4410\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2216,  E(|Y-Yhat|): 0.4421,  E(|Yhat-Yhat'|): 0.4411\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2235,  E(|Y-Yhat|): 0.4413,  E(|Yhat-Yhat'|): 0.4356\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2268,  E(|Y-Yhat|): 0.4489,  E(|Yhat-Yhat'|): 0.4441\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5956,  E(|Y-Yhat|): 1.0157,  E(|Yhat-Yhat'|): 0.8402\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5602,  E(|Y-Yhat|): 1.1328,  E(|Yhat-Yhat'|): 1.1453\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5573,  E(|Y-Yhat|): 1.1373,  E(|Yhat-Yhat'|): 1.1599\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5612,  E(|Y-Yhat|): 1.1418,  E(|Yhat-Yhat'|): 1.1613\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9667,  E(|Y-Yhat|): 2.0665,  E(|Yhat-Yhat'|): 2.1996\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0163,  E(|Y-Yhat|): 1.1968,  E(|Yhat-Yhat'|): 0.3609\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9765,  E(|Y-Yhat|): 2.0964,  E(|Yhat-Yhat'|): 2.2399\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0414,  E(|Y-Yhat|): 6.8111,  E(|Yhat-Yhat'|): 11.5393\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9299,  E(|Y-Yhat|): 17.2632,  E(|Yhat-Yhat'|): 32.6665\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0462,  E(|Y-Yhat|): 16.9326,  E(|Yhat-Yhat'|): 31.7727\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2306,  E(|Y-Yhat|): 0.3865,  E(|Yhat-Yhat'|): 0.3119\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2156,  E(|Y-Yhat|): 0.4253,  E(|Yhat-Yhat'|): 0.4194\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2079,  E(|Y-Yhat|): 0.4229,  E(|Yhat-Yhat'|): 0.4299\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2067,  E(|Y-Yhat|): 0.4149,  E(|Yhat-Yhat'|): 0.4165\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2094,  E(|Y-Yhat|): 0.4232,  E(|Yhat-Yhat'|): 0.4276\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4366,  E(|Y-Yhat|): 0.7281,  E(|Yhat-Yhat'|): 0.5830\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3974,  E(|Y-Yhat|): 0.8218,  E(|Yhat-Yhat'|): 0.8488\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3992,  E(|Y-Yhat|): 0.8186,  E(|Yhat-Yhat'|): 0.8389\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3936,  E(|Y-Yhat|): 0.8119,  E(|Yhat-Yhat'|): 0.8365\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4071,  E(|Y-Yhat|): 0.8212,  E(|Yhat-Yhat'|): 0.8280\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2414,  E(|Y-Yhat|): 0.4389,  E(|Yhat-Yhat'|): 0.3949\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2225,  E(|Y-Yhat|): 0.4579,  E(|Yhat-Yhat'|): 0.4708\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2267,  E(|Y-Yhat|): 0.4560,  E(|Yhat-Yhat'|): 0.4586\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2266,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4567\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2233,  E(|Y-Yhat|): 0.4540,  E(|Yhat-Yhat'|): 0.4615\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4904,  E(|Y-Yhat|): 0.9045,  E(|Yhat-Yhat'|): 0.8283\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2643,  E(|Y-Yhat|): 0.5425,  E(|Yhat-Yhat'|): 0.5565\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2632,  E(|Y-Yhat|): 0.5391,  E(|Yhat-Yhat'|): 0.5518\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2638,  E(|Y-Yhat|): 0.5423,  E(|Yhat-Yhat'|): 0.5569\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0462,  E(|Y-Yhat|): 2.2273,  E(|Yhat-Yhat'|): 2.3621\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7763,  E(|Y-Yhat|): 0.9708,  E(|Yhat-Yhat'|): 0.3890\n",
      "[Epoch 100 (33%), batch 9] energy-loss: -1.7399,  E(|Y-Yhat|): 165.5579,  E(|Yhat-Yhat'|): 334.5957\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.8832,  E(|Y-Yhat|): 348.5889,  E(|Yhat-Yhat'|): 691.4114\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5466,  E(|Y-Yhat|): 120.3580,  E(|Yhat-Yhat'|): 239.6228\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1784,  E(|Y-Yhat|): 555.2286,  E(|Yhat-Yhat'|): 1110.1003\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.1867,  E(|Y-Yhat|): 0.3391,  E(|Yhat-Yhat'|): 0.3047\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0841,  E(|Y-Yhat|): 0.1704,  E(|Yhat-Yhat'|): 0.1726\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0843,  E(|Y-Yhat|): 0.1671,  E(|Yhat-Yhat'|): 0.1656\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0861,  E(|Y-Yhat|): 0.1692,  E(|Yhat-Yhat'|): 0.1663\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2164,  E(|Y-Yhat|): 0.4216,  E(|Yhat-Yhat'|): 0.4104\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3750,  E(|Y-Yhat|): 0.5962,  E(|Yhat-Yhat'|): 0.4425\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1523,  E(|Y-Yhat|): 0.3136,  E(|Yhat-Yhat'|): 0.3225\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1537,  E(|Y-Yhat|): 0.3148,  E(|Yhat-Yhat'|): 0.3221\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1507,  E(|Y-Yhat|): 0.3111,  E(|Yhat-Yhat'|): 0.3207\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4058,  E(|Y-Yhat|): 0.8236,  E(|Yhat-Yhat'|): 0.8355\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2189,  E(|Y-Yhat|): 0.3870,  E(|Yhat-Yhat'|): 0.3360\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1125,  E(|Y-Yhat|): 0.2315,  E(|Yhat-Yhat'|): 0.2380\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1128,  E(|Y-Yhat|): 0.2340,  E(|Yhat-Yhat'|): 0.2423\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1131,  E(|Y-Yhat|): 0.2345,  E(|Yhat-Yhat'|): 0.2428\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2298,  E(|Y-Yhat|): 0.4637,  E(|Yhat-Yhat'|): 0.4679\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5910,  E(|Y-Yhat|): 1.0225,  E(|Yhat-Yhat'|): 0.8629\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5672,  E(|Y-Yhat|): 1.1220,  E(|Yhat-Yhat'|): 1.1096\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5559,  E(|Y-Yhat|): 1.1518,  E(|Yhat-Yhat'|): 1.1918\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5572,  E(|Y-Yhat|): 1.1322,  E(|Yhat-Yhat'|): 1.1501\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9889,  E(|Y-Yhat|): 2.0157,  E(|Yhat-Yhat'|): 2.0535\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0574,  E(|Y-Yhat|): 1.2703,  E(|Yhat-Yhat'|): 0.4258\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0085,  E(|Y-Yhat|): 7.5267,  E(|Yhat-Yhat'|): 13.0363\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0115,  E(|Y-Yhat|): 3.0347,  E(|Yhat-Yhat'|): 4.0463\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5461,  E(|Y-Yhat|): 93.9592,  E(|Yhat-Yhat'|): 186.8263\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0909,  E(|Y-Yhat|): 98.0339,  E(|Yhat-Yhat'|): 195.8862\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2326,  E(|Y-Yhat|): 0.3888,  E(|Yhat-Yhat'|): 0.3125\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2138,  E(|Y-Yhat|): 0.4277,  E(|Yhat-Yhat'|): 0.4278\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2125,  E(|Y-Yhat|): 0.4251,  E(|Yhat-Yhat'|): 0.4252\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2120,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.4407\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2128,  E(|Y-Yhat|): 0.4317,  E(|Yhat-Yhat'|): 0.4377\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4380,  E(|Y-Yhat|): 0.7400,  E(|Yhat-Yhat'|): 0.6038\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4056,  E(|Y-Yhat|): 0.8170,  E(|Yhat-Yhat'|): 0.8228\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3978,  E(|Y-Yhat|): 0.8106,  E(|Yhat-Yhat'|): 0.8256\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4113,  E(|Y-Yhat|): 0.8058,  E(|Yhat-Yhat'|): 0.7889\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4025,  E(|Y-Yhat|): 0.8101,  E(|Yhat-Yhat'|): 0.8152\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2428,  E(|Y-Yhat|): 0.4370,  E(|Yhat-Yhat'|): 0.3885\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2244,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4611\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2287,  E(|Y-Yhat|): 0.4620,  E(|Yhat-Yhat'|): 0.4665\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2297,  E(|Y-Yhat|): 0.4567,  E(|Yhat-Yhat'|): 0.4540\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2309,  E(|Y-Yhat|): 0.4591,  E(|Yhat-Yhat'|): 0.4564\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6011,  E(|Y-Yhat|): 1.0043,  E(|Yhat-Yhat'|): 0.8064\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5623,  E(|Y-Yhat|): 1.1296,  E(|Yhat-Yhat'|): 1.1347\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5666,  E(|Y-Yhat|): 1.1226,  E(|Yhat-Yhat'|): 1.1120\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5587,  E(|Y-Yhat|): 1.1421,  E(|Yhat-Yhat'|): 1.1669\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9494,  E(|Y-Yhat|): 1.9928,  E(|Yhat-Yhat'|): 2.0868\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.1276,  E(|Y-Yhat|): 1.3163,  E(|Yhat-Yhat'|): 0.3774\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2778,  E(|Y-Yhat|): 17.6512,  E(|Yhat-Yhat'|): 32.7469\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3514,  E(|Y-Yhat|): 30.3292,  E(|Yhat-Yhat'|): 57.9557\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7446,  E(|Y-Yhat|): 10.6958,  E(|Yhat-Yhat'|): 19.9023\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2324,  E(|Y-Yhat|): 11.3700,  E(|Yhat-Yhat'|): 20.2752\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2275,  E(|Y-Yhat|): 0.3891,  E(|Yhat-Yhat'|): 0.3232\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2111,  E(|Y-Yhat|): 0.4246,  E(|Yhat-Yhat'|): 0.4270\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2086,  E(|Y-Yhat|): 0.4235,  E(|Yhat-Yhat'|): 0.4298\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2092,  E(|Y-Yhat|): 0.4265,  E(|Yhat-Yhat'|): 0.4347\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2097,  E(|Y-Yhat|): 0.4226,  E(|Yhat-Yhat'|): 0.4258\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4308,  E(|Y-Yhat|): 0.7111,  E(|Yhat-Yhat'|): 0.5605\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4095,  E(|Y-Yhat|): 0.8175,  E(|Yhat-Yhat'|): 0.8159\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4005,  E(|Y-Yhat|): 0.8206,  E(|Yhat-Yhat'|): 0.8402\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3997,  E(|Y-Yhat|): 0.8230,  E(|Yhat-Yhat'|): 0.8467\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3983,  E(|Y-Yhat|): 0.8259,  E(|Yhat-Yhat'|): 0.8551\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2427,  E(|Y-Yhat|): 0.4418,  E(|Yhat-Yhat'|): 0.3983\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2299,  E(|Y-Yhat|): 0.4597,  E(|Yhat-Yhat'|): 0.4596\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2252,  E(|Y-Yhat|): 0.4578,  E(|Yhat-Yhat'|): 0.4652\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2259,  E(|Y-Yhat|): 0.4534,  E(|Yhat-Yhat'|): 0.4549\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2301,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4499\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5967,  E(|Y-Yhat|): 1.0289,  E(|Yhat-Yhat'|): 0.8645\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5689,  E(|Y-Yhat|): 1.1308,  E(|Yhat-Yhat'|): 1.1238\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5694,  E(|Y-Yhat|): 1.1187,  E(|Yhat-Yhat'|): 1.0986\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5646,  E(|Y-Yhat|): 1.1324,  E(|Yhat-Yhat'|): 1.1357\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9637,  E(|Y-Yhat|): 1.9801,  E(|Yhat-Yhat'|): 2.0329\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0094,  E(|Y-Yhat|): 1.3114,  E(|Yhat-Yhat'|): 0.6041\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3361,  E(|Y-Yhat|): 28.0760,  E(|Yhat-Yhat'|): 53.4798\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2488,  E(|Y-Yhat|): 139.0523,  E(|Yhat-Yhat'|): 275.6070\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 22.5157,  E(|Y-Yhat|): 1041.7300,  E(|Yhat-Yhat'|): 2038.4286\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -18.9914,  E(|Y-Yhat|): 994.8490,  E(|Yhat-Yhat'|): 2027.6808\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2286,  E(|Y-Yhat|): 0.3904,  E(|Yhat-Yhat'|): 0.3236\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2120,  E(|Y-Yhat|): 0.4209,  E(|Yhat-Yhat'|): 0.4179\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2061,  E(|Y-Yhat|): 0.4255,  E(|Yhat-Yhat'|): 0.4388\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2139,  E(|Y-Yhat|): 0.4266,  E(|Yhat-Yhat'|): 0.4255\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2121,  E(|Y-Yhat|): 0.4251,  E(|Yhat-Yhat'|): 0.4260\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4409,  E(|Y-Yhat|): 0.7178,  E(|Yhat-Yhat'|): 0.5538\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4120,  E(|Y-Yhat|): 0.8022,  E(|Yhat-Yhat'|): 0.7804\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4046,  E(|Y-Yhat|): 0.8156,  E(|Yhat-Yhat'|): 0.8221\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4093,  E(|Y-Yhat|): 0.8071,  E(|Yhat-Yhat'|): 0.7956\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4004,  E(|Y-Yhat|): 0.7968,  E(|Yhat-Yhat'|): 0.7929\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2470,  E(|Y-Yhat|): 0.4392,  E(|Yhat-Yhat'|): 0.3845\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2281,  E(|Y-Yhat|): 0.4647,  E(|Yhat-Yhat'|): 0.4733\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2294,  E(|Y-Yhat|): 0.4566,  E(|Yhat-Yhat'|): 0.4544\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2246,  E(|Y-Yhat|): 0.4531,  E(|Yhat-Yhat'|): 0.4570\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2279,  E(|Y-Yhat|): 0.4454,  E(|Yhat-Yhat'|): 0.4350\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0027),\n",
       " tensor(1.1665),\n",
       " tensor(0.0038),\n",
       " tensor(0.0028),\n",
       " tensor(0.0036))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=1, dy=1, k=1, true_function =\"square\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(2, -2, 100)\n",
    "    y_eval = M0* (F.relu(A0 * x_eval))**2 / 2.0\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0179b9",
   "metadata": {},
   "source": [
    "## True function: log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aaddd56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5907,  E(|Y-Yhat|): 1.0335,  E(|Yhat-Yhat'|): 0.8855\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5597,  E(|Y-Yhat|): 1.1389,  E(|Yhat-Yhat'|): 1.1584\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5594,  E(|Y-Yhat|): 1.1325,  E(|Yhat-Yhat'|): 1.1464\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5661,  E(|Y-Yhat|): 1.1500,  E(|Yhat-Yhat'|): 1.1678\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0414,  E(|Y-Yhat|): 2.1613,  E(|Yhat-Yhat'|): 2.2399\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9832,  E(|Y-Yhat|): 1.2038,  E(|Yhat-Yhat'|): 0.4412\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4269,  E(|Y-Yhat|): 96.8601,  E(|Yhat-Yhat'|): 190.8665\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -1.7146,  E(|Y-Yhat|): 291.6524,  E(|Yhat-Yhat'|): 586.7339\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 11.3817,  E(|Y-Yhat|): 480.9272,  E(|Yhat-Yhat'|): 939.0909\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7190,  E(|Y-Yhat|): 446.2999,  E(|Yhat-Yhat'|): 891.1618\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2259,  E(|Y-Yhat|): 0.3935,  E(|Yhat-Yhat'|): 0.3351\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2093,  E(|Y-Yhat|): 0.4191,  E(|Yhat-Yhat'|): 0.4195\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2128,  E(|Y-Yhat|): 0.4214,  E(|Yhat-Yhat'|): 0.4173\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2082,  E(|Y-Yhat|): 0.4134,  E(|Yhat-Yhat'|): 0.4104\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2065,  E(|Y-Yhat|): 0.4257,  E(|Yhat-Yhat'|): 0.4384\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4361,  E(|Y-Yhat|): 0.7016,  E(|Yhat-Yhat'|): 0.5310\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4021,  E(|Y-Yhat|): 0.8059,  E(|Yhat-Yhat'|): 0.8074\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3911,  E(|Y-Yhat|): 0.8077,  E(|Yhat-Yhat'|): 0.8332\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4014,  E(|Y-Yhat|): 0.8014,  E(|Yhat-Yhat'|): 0.8000\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4117,  E(|Y-Yhat|): 0.8185,  E(|Yhat-Yhat'|): 0.8136\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2489,  E(|Y-Yhat|): 0.4235,  E(|Yhat-Yhat'|): 0.3490\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2196,  E(|Y-Yhat|): 0.4546,  E(|Yhat-Yhat'|): 0.4699\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2260,  E(|Y-Yhat|): 0.4576,  E(|Yhat-Yhat'|): 0.4631\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2241,  E(|Y-Yhat|): 0.4521,  E(|Yhat-Yhat'|): 0.4561\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2280,  E(|Y-Yhat|): 0.4578,  E(|Yhat-Yhat'|): 0.4596\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5850,  E(|Y-Yhat|): 1.0674,  E(|Yhat-Yhat'|): 0.9648\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5578,  E(|Y-Yhat|): 1.1399,  E(|Yhat-Yhat'|): 1.1641\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5730,  E(|Y-Yhat|): 1.1542,  E(|Yhat-Yhat'|): 1.1624\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5641,  E(|Y-Yhat|): 1.1482,  E(|Yhat-Yhat'|): 1.1682\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0006,  E(|Y-Yhat|): 2.0715,  E(|Yhat-Yhat'|): 2.1419\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0029,  E(|Y-Yhat|): 1.2392,  E(|Yhat-Yhat'|): 0.4727\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0329,  E(|Y-Yhat|): 2.0794,  E(|Yhat-Yhat'|): 2.0932\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0006,  E(|Y-Yhat|): 1.4654,  E(|Yhat-Yhat'|): 0.9296\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3433,  E(|Y-Yhat|): 29.1484,  E(|Yhat-Yhat'|): 55.6102\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1800,  E(|Y-Yhat|): 27.0174,  E(|Yhat-Yhat'|): 51.6749\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2178,  E(|Y-Yhat|): 0.4076,  E(|Yhat-Yhat'|): 0.3797\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2080,  E(|Y-Yhat|): 0.4230,  E(|Yhat-Yhat'|): 0.4300\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2080,  E(|Y-Yhat|): 0.4207,  E(|Yhat-Yhat'|): 0.4253\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2116,  E(|Y-Yhat|): 0.4248,  E(|Yhat-Yhat'|): 0.4265\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2125,  E(|Y-Yhat|): 0.4239,  E(|Yhat-Yhat'|): 0.4230\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4258,  E(|Y-Yhat|): 0.7279,  E(|Yhat-Yhat'|): 0.6041\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3987,  E(|Y-Yhat|): 0.8026,  E(|Yhat-Yhat'|): 0.8078\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4002,  E(|Y-Yhat|): 0.8112,  E(|Yhat-Yhat'|): 0.8221\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4041,  E(|Y-Yhat|): 0.8093,  E(|Yhat-Yhat'|): 0.8104\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4054,  E(|Y-Yhat|): 0.8172,  E(|Yhat-Yhat'|): 0.8236\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2375,  E(|Y-Yhat|): 0.4432,  E(|Yhat-Yhat'|): 0.4113\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2293,  E(|Y-Yhat|): 0.4584,  E(|Yhat-Yhat'|): 0.4582\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2275,  E(|Y-Yhat|): 0.4555,  E(|Yhat-Yhat'|): 0.4559\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2280,  E(|Y-Yhat|): 0.4587,  E(|Yhat-Yhat'|): 0.4613\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2233,  E(|Y-Yhat|): 0.4587,  E(|Yhat-Yhat'|): 0.4707\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6131,  E(|Y-Yhat|): 0.9939,  E(|Yhat-Yhat'|): 0.7618\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5578,  E(|Y-Yhat|): 1.1384,  E(|Yhat-Yhat'|): 1.1613\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5590,  E(|Y-Yhat|): 1.1589,  E(|Yhat-Yhat'|): 1.1999\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5660,  E(|Y-Yhat|): 1.1327,  E(|Yhat-Yhat'|): 1.1334\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0256,  E(|Y-Yhat|): 2.1319,  E(|Yhat-Yhat'|): 2.2125\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0323,  E(|Y-Yhat|): 1.2422,  E(|Yhat-Yhat'|): 0.4198\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1052,  E(|Y-Yhat|): 29.5378,  E(|Yhat-Yhat'|): 56.8652\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8893,  E(|Y-Yhat|): 15.5359,  E(|Yhat-Yhat'|): 29.2931\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8999,  E(|Y-Yhat|): 19.8217,  E(|Yhat-Yhat'|): 37.8436\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9324,  E(|Y-Yhat|): 21.5379,  E(|Yhat-Yhat'|): 41.2110\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2267,  E(|Y-Yhat|): 0.3869,  E(|Yhat-Yhat'|): 0.3205\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2129,  E(|Y-Yhat|): 0.4176,  E(|Yhat-Yhat'|): 0.4095\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2121,  E(|Y-Yhat|): 0.4228,  E(|Yhat-Yhat'|): 0.4214\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2133,  E(|Y-Yhat|): 0.4266,  E(|Yhat-Yhat'|): 0.4266\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2158,  E(|Y-Yhat|): 0.4322,  E(|Yhat-Yhat'|): 0.4328\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4242,  E(|Y-Yhat|): 0.7373,  E(|Yhat-Yhat'|): 0.6263\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4042,  E(|Y-Yhat|): 0.8138,  E(|Yhat-Yhat'|): 0.8192\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4121,  E(|Y-Yhat|): 0.8073,  E(|Yhat-Yhat'|): 0.7904\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3956,  E(|Y-Yhat|): 0.8009,  E(|Yhat-Yhat'|): 0.8106\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4085,  E(|Y-Yhat|): 0.8285,  E(|Yhat-Yhat'|): 0.8399\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2418,  E(|Y-Yhat|): 0.4372,  E(|Yhat-Yhat'|): 0.3907\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2289,  E(|Y-Yhat|): 0.4573,  E(|Yhat-Yhat'|): 0.4568\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2271,  E(|Y-Yhat|): 0.4573,  E(|Yhat-Yhat'|): 0.4604\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2265,  E(|Y-Yhat|): 0.4560,  E(|Yhat-Yhat'|): 0.4591\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2305,  E(|Y-Yhat|): 0.4587,  E(|Yhat-Yhat'|): 0.4563\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5921,  E(|Y-Yhat|): 1.0460,  E(|Yhat-Yhat'|): 0.9079\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5651,  E(|Y-Yhat|): 1.1265,  E(|Yhat-Yhat'|): 1.1227\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5543,  E(|Y-Yhat|): 1.1446,  E(|Yhat-Yhat'|): 1.1806\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5583,  E(|Y-Yhat|): 1.1491,  E(|Yhat-Yhat'|): 1.1815\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9786,  E(|Y-Yhat|): 2.0573,  E(|Yhat-Yhat'|): 2.1573\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9934,  E(|Y-Yhat|): 1.2328,  E(|Yhat-Yhat'|): 0.4787\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9876,  E(|Y-Yhat|): 1.6843,  E(|Yhat-Yhat'|): 1.3934\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9985,  E(|Y-Yhat|): 2.1701,  E(|Yhat-Yhat'|): 2.3433\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9984,  E(|Y-Yhat|): 2.8088,  E(|Yhat-Yhat'|): 3.6207\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9883,  E(|Y-Yhat|): 2.7194,  E(|Yhat-Yhat'|): 3.4620\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2251,  E(|Y-Yhat|): 0.3965,  E(|Yhat-Yhat'|): 0.3429\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2096,  E(|Y-Yhat|): 0.4226,  E(|Yhat-Yhat'|): 0.4260\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2132,  E(|Y-Yhat|): 0.4244,  E(|Yhat-Yhat'|): 0.4224\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2134,  E(|Y-Yhat|): 0.4244,  E(|Yhat-Yhat'|): 0.4220\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2092,  E(|Y-Yhat|): 0.4227,  E(|Yhat-Yhat'|): 0.4269\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4226,  E(|Y-Yhat|): 0.7691,  E(|Yhat-Yhat'|): 0.6930\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4068,  E(|Y-Yhat|): 0.8144,  E(|Yhat-Yhat'|): 0.8152\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3994,  E(|Y-Yhat|): 0.8019,  E(|Yhat-Yhat'|): 0.8050\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3948,  E(|Y-Yhat|): 0.7975,  E(|Yhat-Yhat'|): 0.8053\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4134,  E(|Y-Yhat|): 0.8242,  E(|Yhat-Yhat'|): 0.8216\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2479,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.3688\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2275,  E(|Y-Yhat|): 0.4556,  E(|Yhat-Yhat'|): 0.4562\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2301,  E(|Y-Yhat|): 0.4616,  E(|Yhat-Yhat'|): 0.4630\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2307,  E(|Y-Yhat|): 0.4593,  E(|Yhat-Yhat'|): 0.4571\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2280,  E(|Y-Yhat|): 0.4582,  E(|Yhat-Yhat'|): 0.4604\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5825,  E(|Y-Yhat|): 1.0191,  E(|Yhat-Yhat'|): 0.8733\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5608,  E(|Y-Yhat|): 1.1192,  E(|Yhat-Yhat'|): 1.1168\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5506,  E(|Y-Yhat|): 1.1168,  E(|Yhat-Yhat'|): 1.1324\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5575,  E(|Y-Yhat|): 1.1036,  E(|Yhat-Yhat'|): 1.0922\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9737,  E(|Y-Yhat|): 1.8870,  E(|Yhat-Yhat'|): 1.8266\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9614,  E(|Y-Yhat|): 1.2342,  E(|Yhat-Yhat'|): 0.5455\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9443,  E(|Y-Yhat|): 1.7975,  E(|Yhat-Yhat'|): 1.7064\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9864,  E(|Y-Yhat|): 2.1308,  E(|Yhat-Yhat'|): 2.2887\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9844,  E(|Y-Yhat|): 3.2462,  E(|Yhat-Yhat'|): 4.5237\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9619,  E(|Y-Yhat|): 3.3984,  E(|Yhat-Yhat'|): 4.8731\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2334,  E(|Y-Yhat|): 0.4004,  E(|Yhat-Yhat'|): 0.3339\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2075,  E(|Y-Yhat|): 0.4173,  E(|Yhat-Yhat'|): 0.4197\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2082,  E(|Y-Yhat|): 0.4257,  E(|Yhat-Yhat'|): 0.4349\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2112,  E(|Y-Yhat|): 0.4223,  E(|Yhat-Yhat'|): 0.4222\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2081,  E(|Y-Yhat|): 0.4188,  E(|Yhat-Yhat'|): 0.4215\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4297,  E(|Y-Yhat|): 0.7267,  E(|Yhat-Yhat'|): 0.5941\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3920,  E(|Y-Yhat|): 0.8068,  E(|Yhat-Yhat'|): 0.8298\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4031,  E(|Y-Yhat|): 0.8165,  E(|Yhat-Yhat'|): 0.8269\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3907,  E(|Y-Yhat|): 0.7928,  E(|Yhat-Yhat'|): 0.8042\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4016,  E(|Y-Yhat|): 0.8087,  E(|Yhat-Yhat'|): 0.8142\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4375,  E(|Yhat-Yhat'|): 0.3906\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2256,  E(|Y-Yhat|): 0.4534,  E(|Yhat-Yhat'|): 0.4556\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2258,  E(|Y-Yhat|): 0.4569,  E(|Yhat-Yhat'|): 0.4622\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2260,  E(|Y-Yhat|): 0.4550,  E(|Yhat-Yhat'|): 0.4580\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2233,  E(|Y-Yhat|): 0.4512,  E(|Yhat-Yhat'|): 0.4558\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5921,  E(|Y-Yhat|): 1.0069,  E(|Yhat-Yhat'|): 0.8295\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5644,  E(|Y-Yhat|): 1.1263,  E(|Yhat-Yhat'|): 1.1238\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5704,  E(|Y-Yhat|): 1.1470,  E(|Yhat-Yhat'|): 1.1532\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5618,  E(|Y-Yhat|): 1.1280,  E(|Yhat-Yhat'|): 1.1324\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0028,  E(|Y-Yhat|): 2.0552,  E(|Yhat-Yhat'|): 2.1048\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0026,  E(|Y-Yhat|): 1.1837,  E(|Yhat-Yhat'|): 0.3623\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9627,  E(|Y-Yhat|): 1.7627,  E(|Yhat-Yhat'|): 1.6002\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9771,  E(|Y-Yhat|): 5.8370,  E(|Yhat-Yhat'|): 9.7197\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9158,  E(|Y-Yhat|): 11.3826,  E(|Yhat-Yhat'|): 20.9336\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9114,  E(|Y-Yhat|): 11.3662,  E(|Yhat-Yhat'|): 20.9095\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2296,  E(|Y-Yhat|): 0.3855,  E(|Yhat-Yhat'|): 0.3118\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2072,  E(|Y-Yhat|): 0.4143,  E(|Yhat-Yhat'|): 0.4142\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2070,  E(|Y-Yhat|): 0.4225,  E(|Yhat-Yhat'|): 0.4310\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2103,  E(|Y-Yhat|): 0.4183,  E(|Yhat-Yhat'|): 0.4159\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2118,  E(|Y-Yhat|): 0.4145,  E(|Yhat-Yhat'|): 0.4056\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4334,  E(|Y-Yhat|): 0.7253,  E(|Yhat-Yhat'|): 0.5838\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3998,  E(|Y-Yhat|): 0.7986,  E(|Yhat-Yhat'|): 0.7978\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3956,  E(|Y-Yhat|): 0.8067,  E(|Yhat-Yhat'|): 0.8223\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4019,  E(|Y-Yhat|): 0.7983,  E(|Yhat-Yhat'|): 0.7929\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3969,  E(|Y-Yhat|): 0.7961,  E(|Yhat-Yhat'|): 0.7986\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2404,  E(|Y-Yhat|): 0.4373,  E(|Yhat-Yhat'|): 0.3937\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2266,  E(|Y-Yhat|): 0.4567,  E(|Yhat-Yhat'|): 0.4601\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2301,  E(|Y-Yhat|): 0.4520,  E(|Yhat-Yhat'|): 0.4437\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2246,  E(|Y-Yhat|): 0.4518,  E(|Yhat-Yhat'|): 0.4544\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2270,  E(|Y-Yhat|): 0.4587,  E(|Yhat-Yhat'|): 0.4635\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5098,  E(|Y-Yhat|): 0.9757,  E(|Yhat-Yhat'|): 0.9319\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4137,  E(|Y-Yhat|): 0.8394,  E(|Yhat-Yhat'|): 0.8513\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4185,  E(|Y-Yhat|): 0.8454,  E(|Yhat-Yhat'|): 0.8539\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4163,  E(|Y-Yhat|): 0.8429,  E(|Yhat-Yhat'|): 0.8531\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0175,  E(|Y-Yhat|): 2.0103,  E(|Yhat-Yhat'|): 1.9855\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7946,  E(|Y-Yhat|): 1.0013,  E(|Yhat-Yhat'|): 0.4134\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5376,  E(|Y-Yhat|): 5.5426,  E(|Yhat-Yhat'|): 10.0099\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5613,  E(|Y-Yhat|): 2.6286,  E(|Yhat-Yhat'|): 4.1346\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5439,  E(|Y-Yhat|): 3.0719,  E(|Yhat-Yhat'|): 5.0559\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9633,  E(|Y-Yhat|): 5.5772,  E(|Yhat-Yhat'|): 9.2278\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2113,  E(|Y-Yhat|): 0.3822,  E(|Yhat-Yhat'|): 0.3418\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1567,  E(|Y-Yhat|): 0.3079,  E(|Yhat-Yhat'|): 0.3024\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1562,  E(|Y-Yhat|): 0.3105,  E(|Yhat-Yhat'|): 0.3087\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1587,  E(|Y-Yhat|): 0.3084,  E(|Yhat-Yhat'|): 0.2993\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2133,  E(|Y-Yhat|): 0.4219,  E(|Yhat-Yhat'|): 0.4171\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3880,  E(|Y-Yhat|): 0.6545,  E(|Yhat-Yhat'|): 0.5330\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2830,  E(|Y-Yhat|): 0.5744,  E(|Yhat-Yhat'|): 0.5827\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2900,  E(|Y-Yhat|): 0.5827,  E(|Yhat-Yhat'|): 0.5852\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2832,  E(|Y-Yhat|): 0.5622,  E(|Yhat-Yhat'|): 0.5580\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4031,  E(|Y-Yhat|): 0.8194,  E(|Yhat-Yhat'|): 0.8326\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2422,  E(|Y-Yhat|): 0.4330,  E(|Yhat-Yhat'|): 0.3816\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1782,  E(|Y-Yhat|): 0.3579,  E(|Yhat-Yhat'|): 0.3595\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1795,  E(|Y-Yhat|): 0.3631,  E(|Yhat-Yhat'|): 0.3672\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1810,  E(|Y-Yhat|): 0.3579,  E(|Yhat-Yhat'|): 0.3538\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2331,  E(|Y-Yhat|): 0.4616,  E(|Yhat-Yhat'|): 0.4571\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5927,  E(|Y-Yhat|): 1.0216,  E(|Yhat-Yhat'|): 0.8578\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5600,  E(|Y-Yhat|): 1.1387,  E(|Yhat-Yhat'|): 1.1572\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5601,  E(|Y-Yhat|): 1.1489,  E(|Yhat-Yhat'|): 1.1776\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5644,  E(|Y-Yhat|): 1.1315,  E(|Yhat-Yhat'|): 1.1342\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0161,  E(|Y-Yhat|): 1.9833,  E(|Yhat-Yhat'|): 1.9343\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0542,  E(|Y-Yhat|): 1.2683,  E(|Yhat-Yhat'|): 0.4282\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0691,  E(|Y-Yhat|): 6.0667,  E(|Yhat-Yhat'|): 9.9952\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0038,  E(|Y-Yhat|): 3.0172,  E(|Yhat-Yhat'|): 4.0268\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4129,  E(|Y-Yhat|): 98.9900,  E(|Yhat-Yhat'|): 197.1542\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0346,  E(|Y-Yhat|): 103.8010,  E(|Yhat-Yhat'|): 207.5328\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2322,  E(|Y-Yhat|): 0.3886,  E(|Yhat-Yhat'|): 0.3128\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2147,  E(|Y-Yhat|): 0.4362,  E(|Yhat-Yhat'|): 0.4429\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2137,  E(|Y-Yhat|): 0.4230,  E(|Yhat-Yhat'|): 0.4186\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2136,  E(|Y-Yhat|): 0.4285,  E(|Yhat-Yhat'|): 0.4299\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2153,  E(|Y-Yhat|): 0.4239,  E(|Yhat-Yhat'|): 0.4173\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4377,  E(|Y-Yhat|): 0.7391,  E(|Yhat-Yhat'|): 0.6028\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4087,  E(|Y-Yhat|): 0.8084,  E(|Yhat-Yhat'|): 0.7994\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3985,  E(|Y-Yhat|): 0.8179,  E(|Yhat-Yhat'|): 0.8389\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3942,  E(|Y-Yhat|): 0.8127,  E(|Yhat-Yhat'|): 0.8370\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3979,  E(|Y-Yhat|): 0.8206,  E(|Yhat-Yhat'|): 0.8455\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2426,  E(|Y-Yhat|): 0.4368,  E(|Yhat-Yhat'|): 0.3884\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2248,  E(|Y-Yhat|): 0.4604,  E(|Yhat-Yhat'|): 0.4712\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2293,  E(|Y-Yhat|): 0.4568,  E(|Yhat-Yhat'|): 0.4551\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2295,  E(|Y-Yhat|): 0.4587,  E(|Yhat-Yhat'|): 0.4585\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2278,  E(|Y-Yhat|): 0.4595,  E(|Yhat-Yhat'|): 0.4635\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5926,  E(|Y-Yhat|): 0.9981,  E(|Yhat-Yhat'|): 0.8110\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5651,  E(|Y-Yhat|): 1.1033,  E(|Yhat-Yhat'|): 1.0764\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5554,  E(|Y-Yhat|): 1.1149,  E(|Yhat-Yhat'|): 1.1189\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5638,  E(|Y-Yhat|): 1.1409,  E(|Yhat-Yhat'|): 1.1542\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9782,  E(|Y-Yhat|): 1.9996,  E(|Yhat-Yhat'|): 2.0428\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.1030,  E(|Y-Yhat|): 1.2895,  E(|Yhat-Yhat'|): 0.3730\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 3.0463,  E(|Y-Yhat|): 122.3295,  E(|Yhat-Yhat'|): 238.5664\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.3524,  E(|Y-Yhat|): 81.9233,  E(|Yhat-Yhat'|): 161.1417\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6883,  E(|Y-Yhat|): 39.1021,  E(|Yhat-Yhat'|): 76.8276\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.3603,  E(|Y-Yhat|): 41.1332,  E(|Yhat-Yhat'|): 79.5458\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2249,  E(|Y-Yhat|): 0.3863,  E(|Yhat-Yhat'|): 0.3229\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2092,  E(|Y-Yhat|): 0.4178,  E(|Yhat-Yhat'|): 0.4171\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2094,  E(|Y-Yhat|): 0.4253,  E(|Yhat-Yhat'|): 0.4317\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2071,  E(|Y-Yhat|): 0.4228,  E(|Yhat-Yhat'|): 0.4314\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2109,  E(|Y-Yhat|): 0.4282,  E(|Yhat-Yhat'|): 0.4345\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4264,  E(|Y-Yhat|): 0.7042,  E(|Yhat-Yhat'|): 0.5555\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3961,  E(|Y-Yhat|): 0.7960,  E(|Yhat-Yhat'|): 0.7999\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3979,  E(|Y-Yhat|): 0.7913,  E(|Yhat-Yhat'|): 0.7867\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3999,  E(|Y-Yhat|): 0.8044,  E(|Yhat-Yhat'|): 0.8090\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3955,  E(|Y-Yhat|): 0.8056,  E(|Yhat-Yhat'|): 0.8202\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2400,  E(|Y-Yhat|): 0.4382,  E(|Yhat-Yhat'|): 0.3965\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2291,  E(|Y-Yhat|): 0.4583,  E(|Yhat-Yhat'|): 0.4584\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2222,  E(|Y-Yhat|): 0.4527,  E(|Yhat-Yhat'|): 0.4609\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2281,  E(|Y-Yhat|): 0.4564,  E(|Yhat-Yhat'|): 0.4566\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2265,  E(|Y-Yhat|): 0.4542,  E(|Yhat-Yhat'|): 0.4554\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5965,  E(|Y-Yhat|): 1.0282,  E(|Yhat-Yhat'|): 0.8635\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5622,  E(|Y-Yhat|): 1.1396,  E(|Yhat-Yhat'|): 1.1547\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5576,  E(|Y-Yhat|): 1.1236,  E(|Yhat-Yhat'|): 1.1320\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5647,  E(|Y-Yhat|): 1.1167,  E(|Yhat-Yhat'|): 1.1040\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9778,  E(|Y-Yhat|): 1.9853,  E(|Yhat-Yhat'|): 2.0150\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.0084,  E(|Y-Yhat|): 1.3077,  E(|Yhat-Yhat'|): 0.5985\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4268,  E(|Y-Yhat|): 28.5670,  E(|Yhat-Yhat'|): 54.2804\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9314,  E(|Y-Yhat|): 22.7319,  E(|Yhat-Yhat'|): 43.6009\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 3.6840,  E(|Y-Yhat|): 117.9036,  E(|Yhat-Yhat'|): 228.4392\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -0.9166,  E(|Y-Yhat|): 112.8193,  E(|Yhat-Yhat'|): 227.4718\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2285,  E(|Y-Yhat|): 0.3904,  E(|Yhat-Yhat'|): 0.3238\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2113,  E(|Y-Yhat|): 0.4231,  E(|Yhat-Yhat'|): 0.4236\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2072,  E(|Y-Yhat|): 0.4195,  E(|Yhat-Yhat'|): 0.4246\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2096,  E(|Y-Yhat|): 0.4235,  E(|Yhat-Yhat'|): 0.4277\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2115,  E(|Y-Yhat|): 0.4208,  E(|Yhat-Yhat'|): 0.4187\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4407,  E(|Y-Yhat|): 0.7160,  E(|Yhat-Yhat'|): 0.5505\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4043,  E(|Y-Yhat|): 0.8044,  E(|Yhat-Yhat'|): 0.8002\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4045,  E(|Y-Yhat|): 0.8140,  E(|Yhat-Yhat'|): 0.8190\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3991,  E(|Y-Yhat|): 0.8136,  E(|Yhat-Yhat'|): 0.8290\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3995,  E(|Y-Yhat|): 0.8034,  E(|Yhat-Yhat'|): 0.8079\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2466,  E(|Y-Yhat|): 0.4389,  E(|Yhat-Yhat'|): 0.3846\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2288,  E(|Y-Yhat|): 0.4616,  E(|Yhat-Yhat'|): 0.4655\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2278,  E(|Y-Yhat|): 0.4585,  E(|Yhat-Yhat'|): 0.4613\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2252,  E(|Y-Yhat|): 0.4582,  E(|Yhat-Yhat'|): 0.4661\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.2271,  E(|Y-Yhat|): 0.4533,  E(|Yhat-Yhat'|): 0.4526\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0026),\n",
       " tensor(0.3022),\n",
       " tensor(0.0033),\n",
       " tensor(0.0018),\n",
       " tensor(0.0030))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=1, dy=1, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=1, dy=1, k=1, true_function =\"log\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x_eval = torch.linspace(0, 5, 100)\n",
    "    y_eval = M0* (log_lin(A0 * x_eval))\n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fad4bc",
   "metadata": {},
   "source": [
    "# post-ANM, comparing 4 loss functions under different true functions ($X,Y \\in \\mathbb{R}^2, k=1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3528e8",
   "metadata": {},
   "source": [
    "## True function: softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29dfa52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8328,  E(|Y-Yhat|): 1.4801,  E(|Yhat-Yhat'|): 1.2945\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5141,  E(|Y-Yhat|): 1.0631,  E(|Yhat-Yhat'|): 1.0980\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5194,  E(|Y-Yhat|): 1.0420,  E(|Yhat-Yhat'|): 1.0452\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5245,  E(|Y-Yhat|): 1.0441,  E(|Yhat-Yhat'|): 1.0392\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0010,  E(|Y-Yhat|): 4.2091,  E(|Yhat-Yhat'|): 4.4162\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5626,  E(|Y-Yhat|): 2.0130,  E(|Yhat-Yhat'|): 0.9008\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6811,  E(|Y-Yhat|): 9.7047,  E(|Yhat-Yhat'|): 18.0473\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7856,  E(|Y-Yhat|): 4.9901,  E(|Yhat-Yhat'|): 8.4091\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5775,  E(|Y-Yhat|): 23.0366,  E(|Yhat-Yhat'|): 44.9181\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.7730,  E(|Y-Yhat|): 155.5472,  E(|Yhat-Yhat'|): 309.5484\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3494,  E(|Y-Yhat|): 0.5906,  E(|Yhat-Yhat'|): 0.4825\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2017,  E(|Y-Yhat|): 0.4053,  E(|Yhat-Yhat'|): 0.4073\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1959,  E(|Y-Yhat|): 0.4018,  E(|Yhat-Yhat'|): 0.4119\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2011,  E(|Y-Yhat|): 0.3986,  E(|Yhat-Yhat'|): 0.3951\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3325,  E(|Y-Yhat|): 0.6663,  E(|Yhat-Yhat'|): 0.6675\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6285,  E(|Y-Yhat|): 1.0778,  E(|Yhat-Yhat'|): 0.8986\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3658,  E(|Y-Yhat|): 0.7347,  E(|Yhat-Yhat'|): 0.7378\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3650,  E(|Y-Yhat|): 0.7408,  E(|Yhat-Yhat'|): 0.7516\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3654,  E(|Y-Yhat|): 0.7412,  E(|Yhat-Yhat'|): 0.7516\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6642,  E(|Y-Yhat|): 1.3498,  E(|Yhat-Yhat'|): 1.3712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3423,  E(|Y-Yhat|): 0.6193,  E(|Yhat-Yhat'|): 0.5540\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2240,  E(|Y-Yhat|): 0.4545,  E(|Yhat-Yhat'|): 0.4611\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2279,  E(|Y-Yhat|): 0.4518,  E(|Yhat-Yhat'|): 0.4477\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2222,  E(|Y-Yhat|): 0.4524,  E(|Yhat-Yhat'|): 0.4604\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3330,  E(|Y-Yhat|): 0.6733,  E(|Yhat-Yhat'|): 0.6807\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9187,  E(|Y-Yhat|): 1.5546,  E(|Yhat-Yhat'|): 1.2717\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8641,  E(|Y-Yhat|): 1.7323,  E(|Yhat-Yhat'|): 1.7365\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8575,  E(|Y-Yhat|): 1.7493,  E(|Yhat-Yhat'|): 1.7836\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8587,  E(|Y-Yhat|): 1.7241,  E(|Yhat-Yhat'|): 1.7308\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0035,  E(|Y-Yhat|): 4.0991,  E(|Yhat-Yhat'|): 4.1913\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0275,  E(|Y-Yhat|): 2.3195,  E(|Yhat-Yhat'|): 0.5841\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.9447,  E(|Y-Yhat|): 144.2538,  E(|Yhat-Yhat'|): 282.6183\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.2148,  E(|Y-Yhat|): 71.1762,  E(|Yhat-Yhat'|): 137.9227\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0661,  E(|Y-Yhat|): 61.3895,  E(|Yhat-Yhat'|): 118.6468\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.2251,  E(|Y-Yhat|): 70.6074,  E(|Yhat-Yhat'|): 136.7646\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3552,  E(|Y-Yhat|): 0.6267,  E(|Yhat-Yhat'|): 0.5431\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3257,  E(|Y-Yhat|): 0.6551,  E(|Yhat-Yhat'|): 0.6588\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3231,  E(|Y-Yhat|): 0.6531,  E(|Yhat-Yhat'|): 0.6601\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3250,  E(|Y-Yhat|): 0.6555,  E(|Yhat-Yhat'|): 0.6610\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3347,  E(|Y-Yhat|): 0.6747,  E(|Yhat-Yhat'|): 0.6801\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7066,  E(|Y-Yhat|): 1.1519,  E(|Yhat-Yhat'|): 0.8905\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6492,  E(|Y-Yhat|): 1.3116,  E(|Yhat-Yhat'|): 1.3248\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6464,  E(|Y-Yhat|): 1.3010,  E(|Yhat-Yhat'|): 1.3091\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6498,  E(|Y-Yhat|): 1.2963,  E(|Yhat-Yhat'|): 1.2930\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6705,  E(|Y-Yhat|): 1.3503,  E(|Yhat-Yhat'|): 1.3596\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3559,  E(|Y-Yhat|): 0.6298,  E(|Yhat-Yhat'|): 0.5477\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3278,  E(|Y-Yhat|): 0.6579,  E(|Yhat-Yhat'|): 0.6600\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3323,  E(|Y-Yhat|): 0.6614,  E(|Yhat-Yhat'|): 0.6582\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3303,  E(|Y-Yhat|): 0.6587,  E(|Yhat-Yhat'|): 0.6568\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3320,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6659\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9088,  E(|Y-Yhat|): 1.5759,  E(|Yhat-Yhat'|): 1.3342\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8611,  E(|Y-Yhat|): 1.7554,  E(|Yhat-Yhat'|): 1.7887\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8647,  E(|Y-Yhat|): 1.7326,  E(|Yhat-Yhat'|): 1.7357\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8572,  E(|Y-Yhat|): 1.7275,  E(|Yhat-Yhat'|): 1.7406\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0177,  E(|Y-Yhat|): 4.1211,  E(|Yhat-Yhat'|): 4.2069\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1210,  E(|Y-Yhat|): 2.4594,  E(|Yhat-Yhat'|): 0.6769\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8848,  E(|Y-Yhat|): 4.1165,  E(|Yhat-Yhat'|): 4.4633\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9070,  E(|Y-Yhat|): 6.3621,  E(|Yhat-Yhat'|): 8.9102\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9096,  E(|Y-Yhat|): 8.5813,  E(|Yhat-Yhat'|): 13.3435\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0116,  E(|Y-Yhat|): 9.0284,  E(|Yhat-Yhat'|): 14.0337\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3574,  E(|Y-Yhat|): 0.6239,  E(|Yhat-Yhat'|): 0.5329\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3276,  E(|Y-Yhat|): 0.6543,  E(|Yhat-Yhat'|): 0.6534\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3301,  E(|Y-Yhat|): 0.6625,  E(|Yhat-Yhat'|): 0.6647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3306,  E(|Y-Yhat|): 0.6605,  E(|Yhat-Yhat'|): 0.6599\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3314,  E(|Y-Yhat|): 0.6702,  E(|Yhat-Yhat'|): 0.6776\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7190,  E(|Y-Yhat|): 1.1770,  E(|Yhat-Yhat'|): 0.9161\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6650,  E(|Y-Yhat|): 1.3240,  E(|Yhat-Yhat'|): 1.3178\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6585,  E(|Y-Yhat|): 1.3212,  E(|Yhat-Yhat'|): 1.3255\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6539,  E(|Y-Yhat|): 1.3153,  E(|Yhat-Yhat'|): 1.3227\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6662,  E(|Y-Yhat|): 1.3420,  E(|Yhat-Yhat'|): 1.3516\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3626,  E(|Y-Yhat|): 0.6297,  E(|Yhat-Yhat'|): 0.5342\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3293,  E(|Y-Yhat|): 0.6585,  E(|Yhat-Yhat'|): 0.6583\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3298,  E(|Y-Yhat|): 0.6589,  E(|Yhat-Yhat'|): 0.6581\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3243,  E(|Y-Yhat|): 0.6580,  E(|Yhat-Yhat'|): 0.6673\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3311,  E(|Y-Yhat|): 0.6641,  E(|Yhat-Yhat'|): 0.6660\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9273,  E(|Y-Yhat|): 1.5082,  E(|Yhat-Yhat'|): 1.1618\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8443,  E(|Y-Yhat|): 1.7083,  E(|Yhat-Yhat'|): 1.7280\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8463,  E(|Y-Yhat|): 1.7223,  E(|Yhat-Yhat'|): 1.7519\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8449,  E(|Y-Yhat|): 1.6964,  E(|Yhat-Yhat'|): 1.7030\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9551,  E(|Y-Yhat|): 4.0232,  E(|Yhat-Yhat'|): 4.1362\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9714,  E(|Y-Yhat|): 2.3175,  E(|Yhat-Yhat'|): 0.6922\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.7827,  E(|Y-Yhat|): 4.3483,  E(|Yhat-Yhat'|): 5.1312\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8590,  E(|Y-Yhat|): 13.8218,  E(|Yhat-Yhat'|): 23.9255\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8248,  E(|Y-Yhat|): 10.3138,  E(|Yhat-Yhat'|): 16.9782\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0950,  E(|Y-Yhat|): 12.2481,  E(|Yhat-Yhat'|): 20.3061\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3566,  E(|Y-Yhat|): 0.6091,  E(|Yhat-Yhat'|): 0.5050\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3254,  E(|Y-Yhat|): 0.6511,  E(|Yhat-Yhat'|): 0.6514\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3203,  E(|Y-Yhat|): 0.6493,  E(|Yhat-Yhat'|): 0.6579\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3221,  E(|Y-Yhat|): 0.6482,  E(|Yhat-Yhat'|): 0.6521\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3325,  E(|Y-Yhat|): 0.6641,  E(|Yhat-Yhat'|): 0.6634\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7011,  E(|Y-Yhat|): 1.1729,  E(|Yhat-Yhat'|): 0.9436\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6446,  E(|Y-Yhat|): 1.3110,  E(|Yhat-Yhat'|): 1.3328\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6478,  E(|Y-Yhat|): 1.2838,  E(|Yhat-Yhat'|): 1.2719\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6399,  E(|Y-Yhat|): 1.2884,  E(|Yhat-Yhat'|): 1.2971\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6702,  E(|Y-Yhat|): 1.3458,  E(|Yhat-Yhat'|): 1.3511\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3511,  E(|Y-Yhat|): 0.6399,  E(|Yhat-Yhat'|): 0.5776\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3216,  E(|Y-Yhat|): 0.6465,  E(|Yhat-Yhat'|): 0.6500\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3263,  E(|Y-Yhat|): 0.6529,  E(|Yhat-Yhat'|): 0.6532\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3240,  E(|Y-Yhat|): 0.6490,  E(|Yhat-Yhat'|): 0.6501\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3367,  E(|Y-Yhat|): 0.6708,  E(|Yhat-Yhat'|): 0.6684\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9183,  E(|Y-Yhat|): 1.5186,  E(|Yhat-Yhat'|): 1.2007\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8126,  E(|Y-Yhat|): 1.6604,  E(|Yhat-Yhat'|): 1.6956\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8240,  E(|Y-Yhat|): 1.6420,  E(|Yhat-Yhat'|): 1.6359\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8185,  E(|Y-Yhat|): 1.6632,  E(|Yhat-Yhat'|): 1.6895\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0055,  E(|Y-Yhat|): 4.1461,  E(|Yhat-Yhat'|): 4.2810\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9330,  E(|Y-Yhat|): 2.2853,  E(|Yhat-Yhat'|): 0.7046\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.1085,  E(|Y-Yhat|): 18.7338,  E(|Yhat-Yhat'|): 33.2506\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.7332,  E(|Y-Yhat|): 14.2826,  E(|Yhat-Yhat'|): 25.0989\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9580,  E(|Y-Yhat|): 25.0675,  E(|Yhat-Yhat'|): 46.2189\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9822,  E(|Y-Yhat|): 31.4333,  E(|Yhat-Yhat'|): 58.9022\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3564,  E(|Y-Yhat|): 0.6054,  E(|Yhat-Yhat'|): 0.4979\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3185,  E(|Y-Yhat|): 0.6366,  E(|Yhat-Yhat'|): 0.6363\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3109,  E(|Y-Yhat|): 0.6333,  E(|Yhat-Yhat'|): 0.6448\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3173,  E(|Y-Yhat|): 0.6312,  E(|Yhat-Yhat'|): 0.6277\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3322,  E(|Y-Yhat|): 0.6629,  E(|Yhat-Yhat'|): 0.6614\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7037,  E(|Y-Yhat|): 1.1626,  E(|Yhat-Yhat'|): 0.9179\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6223,  E(|Y-Yhat|): 1.2549,  E(|Yhat-Yhat'|): 1.2652\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6200,  E(|Y-Yhat|): 1.2490,  E(|Yhat-Yhat'|): 1.2580\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6172,  E(|Y-Yhat|): 1.2477,  E(|Yhat-Yhat'|): 1.2612\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6759,  E(|Y-Yhat|): 1.3661,  E(|Yhat-Yhat'|): 1.3804\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3555,  E(|Y-Yhat|): 0.6286,  E(|Yhat-Yhat'|): 0.5462\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3233,  E(|Y-Yhat|): 0.6424,  E(|Yhat-Yhat'|): 0.6383\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3184,  E(|Y-Yhat|): 0.6408,  E(|Yhat-Yhat'|): 0.6449\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3186,  E(|Y-Yhat|): 0.6398,  E(|Yhat-Yhat'|): 0.6423\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3327,  E(|Y-Yhat|): 0.6661,  E(|Yhat-Yhat'|): 0.6668\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9488,  E(|Y-Yhat|): 1.5630,  E(|Yhat-Yhat'|): 1.2284\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8870,  E(|Y-Yhat|): 1.7868,  E(|Yhat-Yhat'|): 1.7997\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8804,  E(|Y-Yhat|): 1.7698,  E(|Yhat-Yhat'|): 1.7788\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8808,  E(|Y-Yhat|): 1.7671,  E(|Yhat-Yhat'|): 1.7727\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0306,  E(|Y-Yhat|): 4.0900,  E(|Yhat-Yhat'|): 4.1187\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0883,  E(|Y-Yhat|): 2.4344,  E(|Yhat-Yhat'|): 0.6924\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.2803,  E(|Y-Yhat|): 15.4556,  E(|Yhat-Yhat'|): 26.3505\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.6384,  E(|Y-Yhat|): 20.3921,  E(|Yhat-Yhat'|): 37.5073\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0351,  E(|Y-Yhat|): 5.5176,  E(|Yhat-Yhat'|): 6.9649\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0743,  E(|Y-Yhat|): 5.6314,  E(|Yhat-Yhat'|): 7.1143\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3690,  E(|Y-Yhat|): 0.6111,  E(|Yhat-Yhat'|): 0.4842\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3357,  E(|Y-Yhat|): 0.6743,  E(|Yhat-Yhat'|): 0.6773\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3374,  E(|Y-Yhat|): 0.6705,  E(|Yhat-Yhat'|): 0.6663\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3365,  E(|Y-Yhat|): 0.6695,  E(|Yhat-Yhat'|): 0.6661\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3373,  E(|Y-Yhat|): 0.6752,  E(|Yhat-Yhat'|): 0.6758\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7309,  E(|Y-Yhat|): 1.1695,  E(|Yhat-Yhat'|): 0.8772\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6718,  E(|Y-Yhat|): 1.3425,  E(|Yhat-Yhat'|): 1.3414\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6685,  E(|Y-Yhat|): 1.3443,  E(|Yhat-Yhat'|): 1.3515\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6681,  E(|Y-Yhat|): 1.3449,  E(|Yhat-Yhat'|): 1.3535\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6758,  E(|Y-Yhat|): 1.3671,  E(|Yhat-Yhat'|): 1.3826\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3605,  E(|Y-Yhat|): 0.6329,  E(|Yhat-Yhat'|): 0.5449\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3297,  E(|Y-Yhat|): 0.6622,  E(|Yhat-Yhat'|): 0.6648\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3302,  E(|Y-Yhat|): 0.6646,  E(|Yhat-Yhat'|): 0.6689\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3285,  E(|Y-Yhat|): 0.6633,  E(|Yhat-Yhat'|): 0.6695\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3382,  E(|Y-Yhat|): 0.6738,  E(|Yhat-Yhat'|): 0.6712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9399,  E(|Y-Yhat|): 1.5572,  E(|Yhat-Yhat'|): 1.2347\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8967,  E(|Y-Yhat|): 1.7879,  E(|Yhat-Yhat'|): 1.7824\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8795,  E(|Y-Yhat|): 1.7825,  E(|Yhat-Yhat'|): 1.8060\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8863,  E(|Y-Yhat|): 1.7908,  E(|Yhat-Yhat'|): 1.8090\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9876,  E(|Y-Yhat|): 4.1214,  E(|Yhat-Yhat'|): 4.2675\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0857,  E(|Y-Yhat|): 2.3584,  E(|Yhat-Yhat'|): 0.5453\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.0010,  E(|Y-Yhat|): 8.3940,  E(|Yhat-Yhat'|): 12.7860\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0936,  E(|Y-Yhat|): 11.3794,  E(|Yhat-Yhat'|): 18.5715\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9661,  E(|Y-Yhat|): 4.5002,  E(|Yhat-Yhat'|): 5.0684\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0319,  E(|Y-Yhat|): 4.5789,  E(|Yhat-Yhat'|): 5.0940\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3683,  E(|Y-Yhat|): 0.6310,  E(|Yhat-Yhat'|): 0.5255\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6678,  E(|Yhat-Yhat'|): 0.6679\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3368,  E(|Y-Yhat|): 0.6717,  E(|Yhat-Yhat'|): 0.6699\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3328,  E(|Y-Yhat|): 0.6702,  E(|Yhat-Yhat'|): 0.6747\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3337,  E(|Y-Yhat|): 0.6710,  E(|Yhat-Yhat'|): 0.6747\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7135,  E(|Y-Yhat|): 1.1822,  E(|Yhat-Yhat'|): 0.9374\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6684,  E(|Y-Yhat|): 1.3630,  E(|Yhat-Yhat'|): 1.3892\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6692,  E(|Y-Yhat|): 1.3578,  E(|Yhat-Yhat'|): 1.3772\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6681,  E(|Y-Yhat|): 1.3438,  E(|Yhat-Yhat'|): 1.3515\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6657,  E(|Y-Yhat|): 1.3504,  E(|Yhat-Yhat'|): 1.3693\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3522,  E(|Y-Yhat|): 0.6419,  E(|Yhat-Yhat'|): 0.5795\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3317,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6665\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6646,  E(|Yhat-Yhat'|): 0.6616\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3362,  E(|Y-Yhat|): 0.6670,  E(|Yhat-Yhat'|): 0.6616\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3318,  E(|Y-Yhat|): 0.6679,  E(|Yhat-Yhat'|): 0.6723\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8456,  E(|Y-Yhat|): 1.5125,  E(|Yhat-Yhat'|): 1.3339\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7461,  E(|Y-Yhat|): 1.4974,  E(|Yhat-Yhat'|): 1.5026\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7476,  E(|Y-Yhat|): 1.4939,  E(|Yhat-Yhat'|): 1.4926\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7314,  E(|Y-Yhat|): 1.4996,  E(|Yhat-Yhat'|): 1.5363\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9959,  E(|Y-Yhat|): 4.1754,  E(|Yhat-Yhat'|): 4.3589\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8168,  E(|Y-Yhat|): 2.1554,  E(|Yhat-Yhat'|): 0.6773\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1929,  E(|Y-Yhat|): 88.7610,  E(|Yhat-Yhat'|): 175.1361\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.1393,  E(|Y-Yhat|): 39.8204,  E(|Yhat-Yhat'|): 77.3622\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.6343,  E(|Y-Yhat|): 69.4791,  E(|Yhat-Yhat'|): 135.6896\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0003,  E(|Y-Yhat|): 95.3167,  E(|Yhat-Yhat'|): 186.6329\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3419,  E(|Y-Yhat|): 0.6105,  E(|Yhat-Yhat'|): 0.5371\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2959,  E(|Y-Yhat|): 0.5958,  E(|Yhat-Yhat'|): 0.5997\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2927,  E(|Y-Yhat|): 0.5920,  E(|Yhat-Yhat'|): 0.5986\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2938,  E(|Y-Yhat|): 0.5875,  E(|Yhat-Yhat'|): 0.5874\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3314,  E(|Y-Yhat|): 0.6653,  E(|Yhat-Yhat'|): 0.6679\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7059,  E(|Y-Yhat|): 1.1608,  E(|Yhat-Yhat'|): 0.9097\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5539,  E(|Y-Yhat|): 1.1364,  E(|Yhat-Yhat'|): 1.1650\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5609,  E(|Y-Yhat|): 1.1319,  E(|Yhat-Yhat'|): 1.1420\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5546,  E(|Y-Yhat|): 1.1339,  E(|Yhat-Yhat'|): 1.1587\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6746,  E(|Y-Yhat|): 1.3638,  E(|Yhat-Yhat'|): 1.3783\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3518,  E(|Y-Yhat|): 0.6273,  E(|Yhat-Yhat'|): 0.5509\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2991,  E(|Y-Yhat|): 0.6055,  E(|Yhat-Yhat'|): 0.6127\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3024,  E(|Y-Yhat|): 0.6027,  E(|Yhat-Yhat'|): 0.6006\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3039,  E(|Y-Yhat|): 0.6123,  E(|Yhat-Yhat'|): 0.6169\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3323,  E(|Y-Yhat|): 0.6680,  E(|Yhat-Yhat'|): 0.6715\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9415,  E(|Y-Yhat|): 1.5363,  E(|Yhat-Yhat'|): 1.1895\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8864,  E(|Y-Yhat|): 1.7681,  E(|Yhat-Yhat'|): 1.7634\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8748,  E(|Y-Yhat|): 1.7756,  E(|Yhat-Yhat'|): 1.8016\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8767,  E(|Y-Yhat|): 1.7661,  E(|Yhat-Yhat'|): 1.7787\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0298,  E(|Y-Yhat|): 4.1149,  E(|Yhat-Yhat'|): 4.1702\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0682,  E(|Y-Yhat|): 2.4352,  E(|Yhat-Yhat'|): 0.7341\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9192,  E(|Y-Yhat|): 7.7509,  E(|Yhat-Yhat'|): 11.6635\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9612,  E(|Y-Yhat|): 4.5492,  E(|Yhat-Yhat'|): 5.1760\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9085,  E(|Y-Yhat|): 15.6606,  E(|Yhat-Yhat'|): 27.5044\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0015,  E(|Y-Yhat|): 16.7042,  E(|Yhat-Yhat'|): 29.4054\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3685,  E(|Y-Yhat|): 0.6150,  E(|Yhat-Yhat'|): 0.4930\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3303,  E(|Y-Yhat|): 0.6673,  E(|Yhat-Yhat'|): 0.6740\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3339,  E(|Y-Yhat|): 0.6644,  E(|Yhat-Yhat'|): 0.6611\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3328,  E(|Y-Yhat|): 0.6682,  E(|Yhat-Yhat'|): 0.6708\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3277,  E(|Y-Yhat|): 0.6612,  E(|Yhat-Yhat'|): 0.6669\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7161,  E(|Y-Yhat|): 1.1976,  E(|Yhat-Yhat'|): 0.9631\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6604,  E(|Y-Yhat|): 1.3475,  E(|Yhat-Yhat'|): 1.3742\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6699,  E(|Y-Yhat|): 1.3409,  E(|Yhat-Yhat'|): 1.3419\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6731,  E(|Y-Yhat|): 1.3395,  E(|Yhat-Yhat'|): 1.3327\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6660,  E(|Y-Yhat|): 1.3410,  E(|Yhat-Yhat'|): 1.3500\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3559,  E(|Y-Yhat|): 0.6340,  E(|Yhat-Yhat'|): 0.5562\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3286,  E(|Y-Yhat|): 0.6631,  E(|Yhat-Yhat'|): 0.6690\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3352,  E(|Y-Yhat|): 0.6678,  E(|Yhat-Yhat'|): 0.6651\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3299,  E(|Y-Yhat|): 0.6658,  E(|Yhat-Yhat'|): 0.6719\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3322,  E(|Y-Yhat|): 0.6658,  E(|Yhat-Yhat'|): 0.6671\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8893,  E(|Y-Yhat|): 1.5009,  E(|Yhat-Yhat'|): 1.2233\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7955,  E(|Y-Yhat|): 1.6008,  E(|Yhat-Yhat'|): 1.6105\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7854,  E(|Y-Yhat|): 1.5820,  E(|Yhat-Yhat'|): 1.5933\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7858,  E(|Y-Yhat|): 1.5759,  E(|Yhat-Yhat'|): 1.5802\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0383,  E(|Y-Yhat|): 4.1166,  E(|Yhat-Yhat'|): 4.1567\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9843,  E(|Y-Yhat|): 2.2714,  E(|Yhat-Yhat'|): 0.5741\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.5151,  E(|Y-Yhat|): 17.6415,  E(|Yhat-Yhat'|): 32.2529\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 11.3881,  E(|Y-Yhat|): 734.1940,  E(|Yhat-Yhat'|): 1445.6118\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8136,  E(|Y-Yhat|): 419.5060,  E(|Yhat-Yhat'|): 837.3848\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.8160,  E(|Y-Yhat|): 522.3315,  E(|Yhat-Yhat'|): 1035.0310\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3604,  E(|Y-Yhat|): 0.6158,  E(|Yhat-Yhat'|): 0.5108\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3052,  E(|Y-Yhat|): 0.6115,  E(|Yhat-Yhat'|): 0.6125\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3047,  E(|Y-Yhat|): 0.6141,  E(|Yhat-Yhat'|): 0.6187\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3044,  E(|Y-Yhat|): 0.6103,  E(|Yhat-Yhat'|): 0.6118\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3300,  E(|Y-Yhat|): 0.6663,  E(|Yhat-Yhat'|): 0.6725\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7176,  E(|Y-Yhat|): 1.1640,  E(|Yhat-Yhat'|): 0.8929\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5876,  E(|Y-Yhat|): 1.1750,  E(|Yhat-Yhat'|): 1.1750\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5950,  E(|Y-Yhat|): 1.1873,  E(|Yhat-Yhat'|): 1.1845\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5918,  E(|Y-Yhat|): 1.1891,  E(|Yhat-Yhat'|): 1.1946\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6693,  E(|Y-Yhat|): 1.3604,  E(|Yhat-Yhat'|): 1.3823\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3511,  E(|Y-Yhat|): 0.6234,  E(|Yhat-Yhat'|): 0.5445\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3089,  E(|Y-Yhat|): 0.6195,  E(|Yhat-Yhat'|): 0.6213\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3089,  E(|Y-Yhat|): 0.6228,  E(|Yhat-Yhat'|): 0.6278\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3102,  E(|Y-Yhat|): 0.6223,  E(|Yhat-Yhat'|): 0.6242\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3315,  E(|Y-Yhat|): 0.6654,  E(|Yhat-Yhat'|): 0.6679\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0031),\n",
       " tensor(0.4856),\n",
       " tensor(0.0042),\n",
       " tensor(0.0037),\n",
       " tensor(0.0045))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 50)\n",
    "    x2 = torch.linspace(0, 5, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = F.softplus(Z)   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ac0a4",
   "metadata": {},
   "source": [
    "## True function: cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70fba0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8041,  E(|Y-Yhat|): 1.3949,  E(|Yhat-Yhat'|): 1.1817\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5005,  E(|Y-Yhat|): 1.0155,  E(|Yhat-Yhat'|): 1.0298\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4999,  E(|Y-Yhat|): 1.0221,  E(|Yhat-Yhat'|): 1.0443\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5110,  E(|Y-Yhat|): 1.0250,  E(|Yhat-Yhat'|): 1.0282\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0290,  E(|Y-Yhat|): 4.3609,  E(|Yhat-Yhat'|): 4.6639\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5777,  E(|Y-Yhat|): 2.0354,  E(|Yhat-Yhat'|): 0.9153\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9836,  E(|Y-Yhat|): 6.7385,  E(|Yhat-Yhat'|): 11.5097\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6583,  E(|Y-Yhat|): 15.1722,  E(|Yhat-Yhat'|): 29.0278\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -0.6769,  E(|Y-Yhat|): 325.5118,  E(|Yhat-Yhat'|): 652.3774\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 8.3096,  E(|Y-Yhat|): 2397.3101,  E(|Yhat-Yhat'|): 4778.0010\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3154,  E(|Y-Yhat|): 0.5367,  E(|Yhat-Yhat'|): 0.4427\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1892,  E(|Y-Yhat|): 0.3832,  E(|Yhat-Yhat'|): 0.3881\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1886,  E(|Y-Yhat|): 0.3855,  E(|Yhat-Yhat'|): 0.3939\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1917,  E(|Y-Yhat|): 0.3810,  E(|Yhat-Yhat'|): 0.3786\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3385,  E(|Y-Yhat|): 0.6728,  E(|Yhat-Yhat'|): 0.6688\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5875,  E(|Y-Yhat|): 0.9985,  E(|Yhat-Yhat'|): 0.8220\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3422,  E(|Y-Yhat|): 0.7172,  E(|Yhat-Yhat'|): 0.7500\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3522,  E(|Y-Yhat|): 0.7133,  E(|Yhat-Yhat'|): 0.7223\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3546,  E(|Y-Yhat|): 0.7032,  E(|Yhat-Yhat'|): 0.6972\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6790,  E(|Y-Yhat|): 1.3639,  E(|Yhat-Yhat'|): 1.3697\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3134,  E(|Y-Yhat|): 0.5753,  E(|Yhat-Yhat'|): 0.5239\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2232,  E(|Y-Yhat|): 0.4505,  E(|Yhat-Yhat'|): 0.4546\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2177,  E(|Y-Yhat|): 0.4407,  E(|Yhat-Yhat'|): 0.4459\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2185,  E(|Y-Yhat|): 0.4394,  E(|Yhat-Yhat'|): 0.4418\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3352,  E(|Y-Yhat|): 0.6713,  E(|Yhat-Yhat'|): 0.6722\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9350,  E(|Y-Yhat|): 1.5720,  E(|Yhat-Yhat'|): 1.2741\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8852,  E(|Y-Yhat|): 1.7874,  E(|Yhat-Yhat'|): 1.8044\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8902,  E(|Y-Yhat|): 1.7942,  E(|Yhat-Yhat'|): 1.8081\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8895,  E(|Y-Yhat|): 1.7977,  E(|Yhat-Yhat'|): 1.8165\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0240,  E(|Y-Yhat|): 4.1562,  E(|Yhat-Yhat'|): 4.2645\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0734,  E(|Y-Yhat|): 2.3607,  E(|Yhat-Yhat'|): 0.5746\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.1337,  E(|Y-Yhat|): 27.4987,  E(|Yhat-Yhat'|): 50.7300\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0248,  E(|Y-Yhat|): 12.8583,  E(|Yhat-Yhat'|): 21.6669\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0454,  E(|Y-Yhat|): 13.8831,  E(|Yhat-Yhat'|): 23.6752\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0272,  E(|Y-Yhat|): 14.1413,  E(|Yhat-Yhat'|): 24.2283\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3589,  E(|Y-Yhat|): 0.6327,  E(|Yhat-Yhat'|): 0.5476\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6668,  E(|Yhat-Yhat'|): 0.6659\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3295,  E(|Y-Yhat|): 0.6655,  E(|Yhat-Yhat'|): 0.6720\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6729,  E(|Yhat-Yhat'|): 0.6781\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3348,  E(|Y-Yhat|): 0.6704,  E(|Yhat-Yhat'|): 0.6712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7244,  E(|Y-Yhat|): 1.1685,  E(|Yhat-Yhat'|): 0.8883\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6706,  E(|Y-Yhat|): 1.3567,  E(|Yhat-Yhat'|): 1.3723\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6617,  E(|Y-Yhat|): 1.3422,  E(|Yhat-Yhat'|): 1.3611\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6662,  E(|Y-Yhat|): 1.3435,  E(|Yhat-Yhat'|): 1.3547\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6706,  E(|Y-Yhat|): 1.3558,  E(|Yhat-Yhat'|): 1.3704\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3585,  E(|Y-Yhat|): 0.6339,  E(|Yhat-Yhat'|): 0.5508\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3316,  E(|Y-Yhat|): 0.6642,  E(|Yhat-Yhat'|): 0.6652\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3295,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6708\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3339,  E(|Y-Yhat|): 0.6680,  E(|Yhat-Yhat'|): 0.6683\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3309,  E(|Y-Yhat|): 0.6640,  E(|Yhat-Yhat'|): 0.6661\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9259,  E(|Y-Yhat|): 1.5929,  E(|Yhat-Yhat'|): 1.3339\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8900,  E(|Y-Yhat|): 1.8064,  E(|Yhat-Yhat'|): 1.8328\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8906,  E(|Y-Yhat|): 1.7902,  E(|Yhat-Yhat'|): 1.7993\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8815,  E(|Y-Yhat|): 1.7902,  E(|Yhat-Yhat'|): 1.8173\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9708,  E(|Y-Yhat|): 4.0979,  E(|Yhat-Yhat'|): 4.2541\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1723,  E(|Y-Yhat|): 2.5124,  E(|Yhat-Yhat'|): 0.6802\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9491,  E(|Y-Yhat|): 4.7452,  E(|Yhat-Yhat'|): 5.5922\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.7889,  E(|Y-Yhat|): 45.2162,  E(|Yhat-Yhat'|): 84.8546\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.6494,  E(|Y-Yhat|): 64.8367,  E(|Yhat-Yhat'|): 126.3745\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.2920,  E(|Y-Yhat|): 69.0523,  E(|Yhat-Yhat'|): 133.5206\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3590,  E(|Y-Yhat|): 0.6272,  E(|Yhat-Yhat'|): 0.5364\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3324,  E(|Y-Yhat|): 0.6667,  E(|Yhat-Yhat'|): 0.6687\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3372,  E(|Y-Yhat|): 0.6747,  E(|Yhat-Yhat'|): 0.6750\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3333,  E(|Y-Yhat|): 0.6643,  E(|Yhat-Yhat'|): 0.6620\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3331,  E(|Y-Yhat|): 0.6672,  E(|Yhat-Yhat'|): 0.6683\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7281,  E(|Y-Yhat|): 1.1881,  E(|Yhat-Yhat'|): 0.9200\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6725,  E(|Y-Yhat|): 1.3463,  E(|Yhat-Yhat'|): 1.3477\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6747,  E(|Y-Yhat|): 1.3548,  E(|Yhat-Yhat'|): 1.3602\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6759,  E(|Y-Yhat|): 1.3449,  E(|Yhat-Yhat'|): 1.3379\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6699,  E(|Y-Yhat|): 1.3485,  E(|Yhat-Yhat'|): 1.3570\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3641,  E(|Y-Yhat|): 0.6314,  E(|Yhat-Yhat'|): 0.5346\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3343,  E(|Y-Yhat|): 0.6695,  E(|Yhat-Yhat'|): 0.6704\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3356,  E(|Y-Yhat|): 0.6690,  E(|Yhat-Yhat'|): 0.6669\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3332,  E(|Y-Yhat|): 0.6675,  E(|Yhat-Yhat'|): 0.6685\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3338,  E(|Y-Yhat|): 0.6675,  E(|Yhat-Yhat'|): 0.6675\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9500,  E(|Y-Yhat|): 1.5277,  E(|Yhat-Yhat'|): 1.1553\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8783,  E(|Y-Yhat|): 1.7766,  E(|Yhat-Yhat'|): 1.7966\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8822,  E(|Y-Yhat|): 1.7920,  E(|Yhat-Yhat'|): 1.8196\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8842,  E(|Y-Yhat|): 1.7703,  E(|Yhat-Yhat'|): 1.7722\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9697,  E(|Y-Yhat|): 3.9867,  E(|Yhat-Yhat'|): 4.0340\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0169,  E(|Y-Yhat|): 2.3564,  E(|Yhat-Yhat'|): 0.6791\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9430,  E(|Y-Yhat|): 4.0424,  E(|Yhat-Yhat'|): 4.1987\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0174,  E(|Y-Yhat|): 2.9895,  E(|Yhat-Yhat'|): 1.9442\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9944,  E(|Y-Yhat|): 4.3203,  E(|Yhat-Yhat'|): 4.6518\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9355,  E(|Y-Yhat|): 4.3208,  E(|Yhat-Yhat'|): 4.7707\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3615,  E(|Y-Yhat|): 0.6146,  E(|Yhat-Yhat'|): 0.5063\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3370,  E(|Y-Yhat|): 0.6732,  E(|Yhat-Yhat'|): 0.6724\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3258,  E(|Y-Yhat|): 0.6647,  E(|Yhat-Yhat'|): 0.6778\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3322,  E(|Y-Yhat|): 0.6674,  E(|Yhat-Yhat'|): 0.6703\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3358,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6636\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7153,  E(|Y-Yhat|): 1.1900,  E(|Yhat-Yhat'|): 0.9494\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6651,  E(|Y-Yhat|): 1.3457,  E(|Yhat-Yhat'|): 1.3613\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6789,  E(|Y-Yhat|): 1.3511,  E(|Yhat-Yhat'|): 1.3446\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6707,  E(|Y-Yhat|): 1.3497,  E(|Yhat-Yhat'|): 1.3579\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6662,  E(|Y-Yhat|): 1.3441,  E(|Yhat-Yhat'|): 1.3560\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3549,  E(|Y-Yhat|): 0.6431,  E(|Yhat-Yhat'|): 0.5764\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3318,  E(|Y-Yhat|): 0.6656,  E(|Yhat-Yhat'|): 0.6675\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3310,  E(|Y-Yhat|): 0.6656,  E(|Yhat-Yhat'|): 0.6692\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3345,  E(|Y-Yhat|): 0.6667,  E(|Yhat-Yhat'|): 0.6643\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3306,  E(|Y-Yhat|): 0.6683,  E(|Yhat-Yhat'|): 0.6755\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7620,  E(|Y-Yhat|): 1.3122,  E(|Yhat-Yhat'|): 1.1002\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4079,  E(|Y-Yhat|): 0.8363,  E(|Yhat-Yhat'|): 0.8568\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4077,  E(|Y-Yhat|): 0.8328,  E(|Yhat-Yhat'|): 0.8502\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4043,  E(|Y-Yhat|): 0.8274,  E(|Yhat-Yhat'|): 0.8462\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0683,  E(|Y-Yhat|): 4.6486,  E(|Yhat-Yhat'|): 5.1606\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6705,  E(|Y-Yhat|): 2.0352,  E(|Yhat-Yhat'|): 0.7294\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6505,  E(|Y-Yhat|): 11.4379,  E(|Yhat-Yhat'|): 21.5748\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.9244,  E(|Y-Yhat|): 20.9456,  E(|Yhat-Yhat'|): 40.0423\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8565,  E(|Y-Yhat|): 13.9473,  E(|Yhat-Yhat'|): 26.1816\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.0821,  E(|Y-Yhat|): 120.1460,  E(|Yhat-Yhat'|): 232.1279\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2859,  E(|Y-Yhat|): 0.5008,  E(|Yhat-Yhat'|): 0.4299\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1498,  E(|Y-Yhat|): 0.3073,  E(|Yhat-Yhat'|): 0.3151\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1468,  E(|Y-Yhat|): 0.2984,  E(|Yhat-Yhat'|): 0.3033\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1483,  E(|Y-Yhat|): 0.3015,  E(|Yhat-Yhat'|): 0.3064\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3395,  E(|Y-Yhat|): 0.6803,  E(|Yhat-Yhat'|): 0.6817\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5883,  E(|Y-Yhat|): 0.9903,  E(|Yhat-Yhat'|): 0.8040\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2736,  E(|Y-Yhat|): 0.5570,  E(|Yhat-Yhat'|): 0.5668\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2725,  E(|Y-Yhat|): 0.5596,  E(|Yhat-Yhat'|): 0.5743\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2643,  E(|Y-Yhat|): 0.5519,  E(|Yhat-Yhat'|): 0.5752\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6827,  E(|Y-Yhat|): 1.3982,  E(|Yhat-Yhat'|): 1.4309\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3048,  E(|Y-Yhat|): 0.5529,  E(|Yhat-Yhat'|): 0.4962\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1847,  E(|Y-Yhat|): 0.3727,  E(|Yhat-Yhat'|): 0.3760\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1801,  E(|Y-Yhat|): 0.3663,  E(|Yhat-Yhat'|): 0.3723\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1830,  E(|Y-Yhat|): 0.3661,  E(|Yhat-Yhat'|): 0.3662\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3350,  E(|Y-Yhat|): 0.6758,  E(|Yhat-Yhat'|): 0.6815\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9458,  E(|Y-Yhat|): 1.5589,  E(|Yhat-Yhat'|): 1.2263\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8860,  E(|Y-Yhat|): 1.7759,  E(|Yhat-Yhat'|): 1.7798\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8755,  E(|Y-Yhat|): 1.7890,  E(|Yhat-Yhat'|): 1.8271\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8730,  E(|Y-Yhat|): 1.7726,  E(|Yhat-Yhat'|): 1.7993\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0704,  E(|Y-Yhat|): 4.2657,  E(|Yhat-Yhat'|): 4.3906\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0756,  E(|Y-Yhat|): 2.4223,  E(|Yhat-Yhat'|): 0.6933\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.1182,  E(|Y-Yhat|): 9.0992,  E(|Yhat-Yhat'|): 13.9620\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8989,  E(|Y-Yhat|): 11.3941,  E(|Yhat-Yhat'|): 18.9904\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.2977,  E(|Y-Yhat|): 22.9795,  E(|Yhat-Yhat'|): 41.3634\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.2497,  E(|Y-Yhat|): 22.7170,  E(|Yhat-Yhat'|): 40.9347\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3676,  E(|Y-Yhat|): 0.6102,  E(|Yhat-Yhat'|): 0.4852\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3340,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6649\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3356,  E(|Y-Yhat|): 0.6717,  E(|Yhat-Yhat'|): 0.6721\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3323,  E(|Y-Yhat|): 0.6622,  E(|Yhat-Yhat'|): 0.6598\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3366,  E(|Y-Yhat|): 0.6726,  E(|Yhat-Yhat'|): 0.6721\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7280,  E(|Y-Yhat|): 1.1659,  E(|Yhat-Yhat'|): 0.8760\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6644,  E(|Y-Yhat|): 1.3409,  E(|Yhat-Yhat'|): 1.3531\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6667,  E(|Y-Yhat|): 1.3479,  E(|Yhat-Yhat'|): 1.3624\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6690,  E(|Y-Yhat|): 1.3413,  E(|Yhat-Yhat'|): 1.3445\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6880,  E(|Y-Yhat|): 1.3806,  E(|Yhat-Yhat'|): 1.3852\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3596,  E(|Y-Yhat|): 0.6322,  E(|Yhat-Yhat'|): 0.5451\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3291,  E(|Y-Yhat|): 0.6666,  E(|Yhat-Yhat'|): 0.6751\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3315,  E(|Y-Yhat|): 0.6662,  E(|Yhat-Yhat'|): 0.6692\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3281,  E(|Y-Yhat|): 0.6626,  E(|Yhat-Yhat'|): 0.6690\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3349,  E(|Y-Yhat|): 0.6712,  E(|Yhat-Yhat'|): 0.6726\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6870,  E(|Y-Yhat|): 1.1416,  E(|Yhat-Yhat'|): 0.9091\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2771,  E(|Y-Yhat|): 0.5744,  E(|Yhat-Yhat'|): 0.5948\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2781,  E(|Y-Yhat|): 0.5765,  E(|Yhat-Yhat'|): 0.5966\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2772,  E(|Y-Yhat|): 0.5693,  E(|Yhat-Yhat'|): 0.5840\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0112,  E(|Y-Yhat|): 4.4520,  E(|Yhat-Yhat'|): 4.8815\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6228,  E(|Y-Yhat|): 1.9135,  E(|Yhat-Yhat'|): 0.5814\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5020,  E(|Y-Yhat|): 22.8180,  E(|Yhat-Yhat'|): 44.6320\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4239,  E(|Y-Yhat|): 5.4954,  E(|Yhat-Yhat'|): 10.1430\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2127,  E(|Y-Yhat|): 12.9672,  E(|Yhat-Yhat'|): 25.5091\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.3719,  E(|Y-Yhat|): 152.1371,  E(|Yhat-Yhat'|): 299.5305\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2320,  E(|Y-Yhat|): 0.4224,  E(|Yhat-Yhat'|): 0.3808\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0819,  E(|Y-Yhat|): 0.1688,  E(|Yhat-Yhat'|): 0.1737\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0823,  E(|Y-Yhat|): 0.1689,  E(|Yhat-Yhat'|): 0.1731\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0813,  E(|Y-Yhat|): 0.1677,  E(|Yhat-Yhat'|): 0.1726\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3437,  E(|Y-Yhat|): 0.6741,  E(|Yhat-Yhat'|): 0.6607\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5025,  E(|Y-Yhat|): 0.8468,  E(|Yhat-Yhat'|): 0.6887\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1533,  E(|Y-Yhat|): 0.3206,  E(|Yhat-Yhat'|): 0.3347\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1513,  E(|Y-Yhat|): 0.3126,  E(|Yhat-Yhat'|): 0.3227\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1493,  E(|Y-Yhat|): 0.3072,  E(|Yhat-Yhat'|): 0.3158\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6842,  E(|Y-Yhat|): 1.3994,  E(|Yhat-Yhat'|): 1.4304\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2499,  E(|Y-Yhat|): 0.4692,  E(|Yhat-Yhat'|): 0.4387\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1197,  E(|Y-Yhat|): 0.2458,  E(|Yhat-Yhat'|): 0.2522\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1165,  E(|Y-Yhat|): 0.2403,  E(|Yhat-Yhat'|): 0.2477\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1194,  E(|Y-Yhat|): 0.2447,  E(|Yhat-Yhat'|): 0.2506\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3318,  E(|Y-Yhat|): 0.6717,  E(|Yhat-Yhat'|): 0.6798\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9052,  E(|Y-Yhat|): 1.5892,  E(|Yhat-Yhat'|): 1.3682\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8694,  E(|Y-Yhat|): 1.7548,  E(|Yhat-Yhat'|): 1.7707\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8622,  E(|Y-Yhat|): 1.7565,  E(|Yhat-Yhat'|): 1.7886\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8502,  E(|Y-Yhat|): 1.7351,  E(|Yhat-Yhat'|): 1.7697\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0098,  E(|Y-Yhat|): 4.1253,  E(|Yhat-Yhat'|): 4.2309\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9552,  E(|Y-Yhat|): 2.2976,  E(|Yhat-Yhat'|): 0.6848\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9361,  E(|Y-Yhat|): 6.3213,  E(|Yhat-Yhat'|): 8.7704\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9097,  E(|Y-Yhat|): 8.8171,  E(|Yhat-Yhat'|): 13.8148\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9094,  E(|Y-Yhat|): 10.3443,  E(|Yhat-Yhat'|): 16.8698\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0411,  E(|Y-Yhat|): 10.5465,  E(|Yhat-Yhat'|): 17.0109\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3540,  E(|Y-Yhat|): 0.6210,  E(|Yhat-Yhat'|): 0.5341\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3332,  E(|Y-Yhat|): 0.6578,  E(|Yhat-Yhat'|): 0.6493\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3290,  E(|Y-Yhat|): 0.6624,  E(|Yhat-Yhat'|): 0.6667\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3293,  E(|Y-Yhat|): 0.6574,  E(|Yhat-Yhat'|): 0.6562\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3339,  E(|Y-Yhat|): 0.6692,  E(|Yhat-Yhat'|): 0.6708\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7251,  E(|Y-Yhat|): 1.1850,  E(|Yhat-Yhat'|): 0.9197\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6459,  E(|Y-Yhat|): 1.3192,  E(|Yhat-Yhat'|): 1.3466\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6617,  E(|Y-Yhat|): 1.3198,  E(|Yhat-Yhat'|): 1.3161\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6620,  E(|Y-Yhat|): 1.3288,  E(|Yhat-Yhat'|): 1.3337\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6671,  E(|Y-Yhat|): 1.3598,  E(|Yhat-Yhat'|): 1.3854\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3590,  E(|Y-Yhat|): 0.6356,  E(|Yhat-Yhat'|): 0.5533\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3295,  E(|Y-Yhat|): 0.6613,  E(|Yhat-Yhat'|): 0.6637\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3294,  E(|Y-Yhat|): 0.6603,  E(|Yhat-Yhat'|): 0.6618\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3272,  E(|Y-Yhat|): 0.6586,  E(|Yhat-Yhat'|): 0.6627\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3318,  E(|Y-Yhat|): 0.6687,  E(|Yhat-Yhat'|): 0.6738\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9115,  E(|Y-Yhat|): 1.5094,  E(|Yhat-Yhat'|): 1.1958\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8402,  E(|Y-Yhat|): 1.6691,  E(|Yhat-Yhat'|): 1.6577\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8238,  E(|Y-Yhat|): 1.6904,  E(|Yhat-Yhat'|): 1.7332\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8175,  E(|Y-Yhat|): 1.6378,  E(|Yhat-Yhat'|): 1.6407\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0158,  E(|Y-Yhat|): 3.9847,  E(|Yhat-Yhat'|): 3.9378\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9804,  E(|Y-Yhat|): 2.3636,  E(|Yhat-Yhat'|): 0.7664\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8282,  E(|Y-Yhat|): 5.1644,  E(|Yhat-Yhat'|): 6.6725\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.7776,  E(|Y-Yhat|): 7.7527,  E(|Yhat-Yhat'|): 11.9503\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9028,  E(|Y-Yhat|): 41.4440,  E(|Yhat-Yhat'|): 79.0826\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.5219,  E(|Y-Yhat|): 47.0089,  E(|Yhat-Yhat'|): 90.9740\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3624,  E(|Y-Yhat|): 0.6060,  E(|Yhat-Yhat'|): 0.4874\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3202,  E(|Y-Yhat|): 0.6461,  E(|Yhat-Yhat'|): 0.6517\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3198,  E(|Y-Yhat|): 0.6430,  E(|Yhat-Yhat'|): 0.6463\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3173,  E(|Y-Yhat|): 0.6318,  E(|Yhat-Yhat'|): 0.6289\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3347,  E(|Y-Yhat|): 0.6684,  E(|Yhat-Yhat'|): 0.6675\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7015,  E(|Y-Yhat|): 1.1770,  E(|Yhat-Yhat'|): 0.9509\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6209,  E(|Y-Yhat|): 1.2661,  E(|Yhat-Yhat'|): 1.2905\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6216,  E(|Y-Yhat|): 1.2724,  E(|Yhat-Yhat'|): 1.3018\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6241,  E(|Y-Yhat|): 1.2574,  E(|Yhat-Yhat'|): 1.2665\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6677,  E(|Y-Yhat|): 1.3472,  E(|Yhat-Yhat'|): 1.3590\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3517,  E(|Y-Yhat|): 0.6275,  E(|Yhat-Yhat'|): 0.5514\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3241,  E(|Y-Yhat|): 0.6492,  E(|Yhat-Yhat'|): 0.6504\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3225,  E(|Y-Yhat|): 0.6459,  E(|Yhat-Yhat'|): 0.6469\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3199,  E(|Y-Yhat|): 0.6414,  E(|Yhat-Yhat'|): 0.6432\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3317,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6698\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8741,  E(|Y-Yhat|): 1.4784,  E(|Yhat-Yhat'|): 1.2085\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7471,  E(|Y-Yhat|): 1.4868,  E(|Yhat-Yhat'|): 1.4794\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7125,  E(|Y-Yhat|): 1.4537,  E(|Yhat-Yhat'|): 1.4825\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7177,  E(|Y-Yhat|): 1.4447,  E(|Yhat-Yhat'|): 1.4539\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0461,  E(|Y-Yhat|): 4.1715,  E(|Yhat-Yhat'|): 4.2509\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9711,  E(|Y-Yhat|): 2.2537,  E(|Yhat-Yhat'|): 0.5652\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.7290,  E(|Y-Yhat|): 63.0850,  E(|Yhat-Yhat'|): 122.7121\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 10.2877,  E(|Y-Yhat|): 1114.9970,  E(|Yhat-Yhat'|): 2209.4186\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8817,  E(|Y-Yhat|): 354.5928,  E(|Yhat-Yhat'|): 707.4223\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5670,  E(|Y-Yhat|): 528.0507,  E(|Yhat-Yhat'|): 1054.9674\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3540,  E(|Y-Yhat|): 0.6046,  E(|Yhat-Yhat'|): 0.5012\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2906,  E(|Y-Yhat|): 0.5830,  E(|Yhat-Yhat'|): 0.5847\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2855,  E(|Y-Yhat|): 0.5726,  E(|Yhat-Yhat'|): 0.5742\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2836,  E(|Y-Yhat|): 0.5651,  E(|Yhat-Yhat'|): 0.5630\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3371,  E(|Y-Yhat|): 0.6741,  E(|Yhat-Yhat'|): 0.6740\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7037,  E(|Y-Yhat|): 1.1446,  E(|Yhat-Yhat'|): 0.8818\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5403,  E(|Y-Yhat|): 1.0852,  E(|Yhat-Yhat'|): 1.0898\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5473,  E(|Y-Yhat|): 1.1003,  E(|Yhat-Yhat'|): 1.1059\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5421,  E(|Y-Yhat|): 1.0893,  E(|Yhat-Yhat'|): 1.0944\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6671,  E(|Y-Yhat|): 1.3550,  E(|Yhat-Yhat'|): 1.3758\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3467,  E(|Y-Yhat|): 0.6135,  E(|Yhat-Yhat'|): 0.5334\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2960,  E(|Y-Yhat|): 0.5939,  E(|Yhat-Yhat'|): 0.5959\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2947,  E(|Y-Yhat|): 0.5868,  E(|Yhat-Yhat'|): 0.5843\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2966,  E(|Y-Yhat|): 0.5901,  E(|Yhat-Yhat'|): 0.5869\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3342,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6668\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0100),\n",
       " tensor(0.8455),\n",
       " tensor(0.0106),\n",
       " tensor(0.0104),\n",
       " tensor(0.0091))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"cubic\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = Z ** 3 / 3.0   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15aa50",
   "metadata": {},
   "source": [
    "## True function: square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "452998c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8602,  E(|Y-Yhat|): 1.5045,  E(|Yhat-Yhat'|): 1.2885\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6757,  E(|Y-Yhat|): 1.3869,  E(|Yhat-Yhat'|): 1.4224\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6746,  E(|Y-Yhat|): 1.3580,  E(|Yhat-Yhat'|): 1.3668\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6723,  E(|Y-Yhat|): 1.3639,  E(|Yhat-Yhat'|): 1.3832\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0141,  E(|Y-Yhat|): 4.1601,  E(|Yhat-Yhat'|): 4.2920\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7726,  E(|Y-Yhat|): 2.1853,  E(|Yhat-Yhat'|): 0.8253\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.2330,  E(|Y-Yhat|): 5.6371,  E(|Yhat-Yhat'|): 8.8082\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2536,  E(|Y-Yhat|): 5.1179,  E(|Yhat-Yhat'|): 7.7285\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2042,  E(|Y-Yhat|): 4.2145,  E(|Yhat-Yhat'|): 6.0205\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9444,  E(|Y-Yhat|): 9.8630,  E(|Yhat-Yhat'|): 15.8373\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3521,  E(|Y-Yhat|): 0.5789,  E(|Yhat-Yhat'|): 0.4536\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2635,  E(|Y-Yhat|): 0.5302,  E(|Yhat-Yhat'|): 0.5335\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2627,  E(|Y-Yhat|): 0.5307,  E(|Yhat-Yhat'|): 0.5360\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2655,  E(|Y-Yhat|): 0.5342,  E(|Yhat-Yhat'|): 0.5375\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3382,  E(|Y-Yhat|): 0.6717,  E(|Yhat-Yhat'|): 0.6671\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6643,  E(|Y-Yhat|): 1.0910,  E(|Yhat-Yhat'|): 0.8534\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4943,  E(|Y-Yhat|): 1.0238,  E(|Yhat-Yhat'|): 1.0590\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5030,  E(|Y-Yhat|): 1.0023,  E(|Yhat-Yhat'|): 0.9985\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5029,  E(|Y-Yhat|): 1.0084,  E(|Yhat-Yhat'|): 1.0110\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6703,  E(|Y-Yhat|): 1.3425,  E(|Yhat-Yhat'|): 1.3445\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3379,  E(|Y-Yhat|): 0.6079,  E(|Yhat-Yhat'|): 0.5401\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2818,  E(|Y-Yhat|): 0.5668,  E(|Yhat-Yhat'|): 0.5698\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2817,  E(|Y-Yhat|): 0.5649,  E(|Yhat-Yhat'|): 0.5662\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2780,  E(|Y-Yhat|): 0.5600,  E(|Yhat-Yhat'|): 0.5641\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3302,  E(|Y-Yhat|): 0.6695,  E(|Yhat-Yhat'|): 0.6786\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9355,  E(|Y-Yhat|): 1.5727,  E(|Yhat-Yhat'|): 1.2745\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8949,  E(|Y-Yhat|): 1.7753,  E(|Yhat-Yhat'|): 1.7608\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8912,  E(|Y-Yhat|): 1.7820,  E(|Yhat-Yhat'|): 1.7816\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8965,  E(|Y-Yhat|): 1.7909,  E(|Yhat-Yhat'|): 1.7887\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9850,  E(|Y-Yhat|): 4.0835,  E(|Yhat-Yhat'|): 4.1971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0753,  E(|Y-Yhat|): 2.3635,  E(|Yhat-Yhat'|): 0.5765\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.4330,  E(|Y-Yhat|): 81.0314,  E(|Yhat-Yhat'|): 157.1969\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0765,  E(|Y-Yhat|): 41.5204,  E(|Yhat-Yhat'|): 78.8879\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.5422,  E(|Y-Yhat|): 71.5252,  E(|Yhat-Yhat'|): 137.9660\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0550,  E(|Y-Yhat|): 72.3596,  E(|Yhat-Yhat'|): 140.6092\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3590,  E(|Y-Yhat|): 0.6330,  E(|Yhat-Yhat'|): 0.5479\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3340,  E(|Y-Yhat|): 0.6731,  E(|Yhat-Yhat'|): 0.6782\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3274,  E(|Y-Yhat|): 0.6629,  E(|Yhat-Yhat'|): 0.6711\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3323,  E(|Y-Yhat|): 0.6693,  E(|Yhat-Yhat'|): 0.6740\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3337,  E(|Y-Yhat|): 0.6734,  E(|Yhat-Yhat'|): 0.6794\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7248,  E(|Y-Yhat|): 1.1697,  E(|Yhat-Yhat'|): 0.8898\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6716,  E(|Y-Yhat|): 1.3530,  E(|Yhat-Yhat'|): 1.3628\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6666,  E(|Y-Yhat|): 1.3439,  E(|Yhat-Yhat'|): 1.3546\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6784,  E(|Y-Yhat|): 1.3475,  E(|Yhat-Yhat'|): 1.3382\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6690,  E(|Y-Yhat|): 1.3446,  E(|Yhat-Yhat'|): 1.3512\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3587,  E(|Y-Yhat|): 0.6343,  E(|Yhat-Yhat'|): 0.5511\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3320,  E(|Y-Yhat|): 0.6707,  E(|Yhat-Yhat'|): 0.6775\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3362,  E(|Y-Yhat|): 0.6692,  E(|Yhat-Yhat'|): 0.6660\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3334,  E(|Y-Yhat|): 0.6716,  E(|Yhat-Yhat'|): 0.6764\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3327,  E(|Y-Yhat|): 0.6705,  E(|Yhat-Yhat'|): 0.6755\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9252,  E(|Y-Yhat|): 1.5932,  E(|Yhat-Yhat'|): 1.3360\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8881,  E(|Y-Yhat|): 1.7880,  E(|Yhat-Yhat'|): 1.7998\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8884,  E(|Y-Yhat|): 1.7911,  E(|Yhat-Yhat'|): 1.8055\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8793,  E(|Y-Yhat|): 1.7779,  E(|Yhat-Yhat'|): 1.7972\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9843,  E(|Y-Yhat|): 4.1489,  E(|Yhat-Yhat'|): 4.3291\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1710,  E(|Y-Yhat|): 2.5107,  E(|Yhat-Yhat'|): 0.6794\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9580,  E(|Y-Yhat|): 3.7764,  E(|Yhat-Yhat'|): 3.6368\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9887,  E(|Y-Yhat|): 6.8834,  E(|Yhat-Yhat'|): 9.7893\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0025,  E(|Y-Yhat|): 4.7183,  E(|Yhat-Yhat'|): 5.4318\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9802,  E(|Y-Yhat|): 4.7052,  E(|Yhat-Yhat'|): 5.4499\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3591,  E(|Y-Yhat|): 0.6274,  E(|Yhat-Yhat'|): 0.5366\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3303,  E(|Y-Yhat|): 0.6691,  E(|Yhat-Yhat'|): 0.6776\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3342,  E(|Y-Yhat|): 0.6726,  E(|Yhat-Yhat'|): 0.6768\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3341,  E(|Y-Yhat|): 0.6687,  E(|Yhat-Yhat'|): 0.6691\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3288,  E(|Y-Yhat|): 0.6654,  E(|Yhat-Yhat'|): 0.6733\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7278,  E(|Y-Yhat|): 1.1877,  E(|Yhat-Yhat'|): 0.9199\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6639,  E(|Y-Yhat|): 1.3485,  E(|Yhat-Yhat'|): 1.3692\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6679,  E(|Y-Yhat|): 1.3488,  E(|Yhat-Yhat'|): 1.3619\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6829,  E(|Y-Yhat|): 1.3391,  E(|Yhat-Yhat'|): 1.3125\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6670,  E(|Y-Yhat|): 1.3349,  E(|Yhat-Yhat'|): 1.3357\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3640,  E(|Y-Yhat|): 0.6314,  E(|Yhat-Yhat'|): 0.5348\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3334,  E(|Y-Yhat|): 0.6684,  E(|Yhat-Yhat'|): 0.6700\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3314,  E(|Y-Yhat|): 0.6653,  E(|Yhat-Yhat'|): 0.6678\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3334,  E(|Y-Yhat|): 0.6692,  E(|Yhat-Yhat'|): 0.6715\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6655,  E(|Yhat-Yhat'|): 0.6644\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9511,  E(|Y-Yhat|): 1.5306,  E(|Yhat-Yhat'|): 1.1591\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8860,  E(|Y-Yhat|): 1.7859,  E(|Yhat-Yhat'|): 1.7996\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8838,  E(|Y-Yhat|): 1.8044,  E(|Yhat-Yhat'|): 1.8412\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8896,  E(|Y-Yhat|): 1.7748,  E(|Yhat-Yhat'|): 1.7705\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9595,  E(|Y-Yhat|): 3.9381,  E(|Yhat-Yhat'|): 3.9571\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0209,  E(|Y-Yhat|): 2.3604,  E(|Yhat-Yhat'|): 0.6790\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9435,  E(|Y-Yhat|): 4.2706,  E(|Yhat-Yhat'|): 4.6543\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9667,  E(|Y-Yhat|): 6.4603,  E(|Yhat-Yhat'|): 8.9873\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9563,  E(|Y-Yhat|): 8.3381,  E(|Yhat-Yhat'|): 12.7635\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9712,  E(|Y-Yhat|): 8.4137,  E(|Yhat-Yhat'|): 12.8851\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3619,  E(|Y-Yhat|): 0.6146,  E(|Yhat-Yhat'|): 0.5054\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3360,  E(|Y-Yhat|): 0.6682,  E(|Yhat-Yhat'|): 0.6643\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3363,  E(|Y-Yhat|): 0.6686,  E(|Yhat-Yhat'|): 0.6646\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3317,  E(|Y-Yhat|): 0.6653,  E(|Yhat-Yhat'|): 0.6671\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3316,  E(|Y-Yhat|): 0.6680,  E(|Yhat-Yhat'|): 0.6727\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7161,  E(|Y-Yhat|): 1.1918,  E(|Yhat-Yhat'|): 0.9514\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6709,  E(|Y-Yhat|): 1.3474,  E(|Yhat-Yhat'|): 1.3532\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6763,  E(|Y-Yhat|): 1.3557,  E(|Yhat-Yhat'|): 1.3590\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6645,  E(|Y-Yhat|): 1.3468,  E(|Yhat-Yhat'|): 1.3645\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6709,  E(|Y-Yhat|): 1.3517,  E(|Yhat-Yhat'|): 1.3616\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3550,  E(|Y-Yhat|): 0.6429,  E(|Yhat-Yhat'|): 0.5758\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3308,  E(|Y-Yhat|): 0.6656,  E(|Yhat-Yhat'|): 0.6698\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3310,  E(|Y-Yhat|): 0.6669,  E(|Yhat-Yhat'|): 0.6718\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3296,  E(|Y-Yhat|): 0.6641,  E(|Yhat-Yhat'|): 0.6691\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3325,  E(|Y-Yhat|): 0.6685,  E(|Yhat-Yhat'|): 0.6721\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8663,  E(|Y-Yhat|): 1.4416,  E(|Yhat-Yhat'|): 1.1505\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6266,  E(|Y-Yhat|): 1.2658,  E(|Yhat-Yhat'|): 1.2785\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6278,  E(|Y-Yhat|): 1.2602,  E(|Yhat-Yhat'|): 1.2647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6244,  E(|Y-Yhat|): 1.2491,  E(|Yhat-Yhat'|): 1.2495\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0185,  E(|Y-Yhat|): 4.1155,  E(|Yhat-Yhat'|): 4.1940\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8134,  E(|Y-Yhat|): 2.1733,  E(|Yhat-Yhat'|): 0.7198\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0250,  E(|Y-Yhat|): 8.4763,  E(|Yhat-Yhat'|): 14.9028\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0962,  E(|Y-Yhat|): 7.2406,  E(|Yhat-Yhat'|): 12.2889\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4527,  E(|Y-Yhat|): 63.8364,  E(|Yhat-Yhat'|): 126.7674\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -1.2028,  E(|Y-Yhat|): 230.1272,  E(|Yhat-Yhat'|): 462.6599\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3276,  E(|Y-Yhat|): 0.5609,  E(|Yhat-Yhat'|): 0.4667\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2473,  E(|Y-Yhat|): 0.5019,  E(|Yhat-Yhat'|): 0.5093\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2447,  E(|Y-Yhat|): 0.4972,  E(|Yhat-Yhat'|): 0.5049\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2452,  E(|Y-Yhat|): 0.4941,  E(|Yhat-Yhat'|): 0.4977\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3316,  E(|Y-Yhat|): 0.6685,  E(|Yhat-Yhat'|): 0.6737\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6577,  E(|Y-Yhat|): 1.0803,  E(|Yhat-Yhat'|): 0.8452\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4565,  E(|Y-Yhat|): 0.9224,  E(|Yhat-Yhat'|): 0.9317\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4583,  E(|Y-Yhat|): 0.9187,  E(|Yhat-Yhat'|): 0.9208\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4507,  E(|Y-Yhat|): 0.9177,  E(|Yhat-Yhat'|): 0.9341\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6712,  E(|Y-Yhat|): 1.3618,  E(|Yhat-Yhat'|): 1.3812\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3431,  E(|Y-Yhat|): 0.5996,  E(|Yhat-Yhat'|): 0.5131\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2676,  E(|Y-Yhat|): 0.5333,  E(|Yhat-Yhat'|): 0.5313\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2655,  E(|Y-Yhat|): 0.5337,  E(|Yhat-Yhat'|): 0.5364\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2664,  E(|Y-Yhat|): 0.5317,  E(|Yhat-Yhat'|): 0.5305\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3313,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6706\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9475,  E(|Y-Yhat|): 1.5623,  E(|Yhat-Yhat'|): 1.2297\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8881,  E(|Y-Yhat|): 1.7888,  E(|Yhat-Yhat'|): 1.8015\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8718,  E(|Y-Yhat|): 1.7701,  E(|Yhat-Yhat'|): 1.7965\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8831,  E(|Y-Yhat|): 1.7838,  E(|Yhat-Yhat'|): 1.8015\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0852,  E(|Y-Yhat|): 4.2313,  E(|Yhat-Yhat'|): 4.2922\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0837,  E(|Y-Yhat|): 2.4272,  E(|Yhat-Yhat'|): 0.6870\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.1135,  E(|Y-Yhat|): 9.5983,  E(|Yhat-Yhat'|): 14.9695\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8943,  E(|Y-Yhat|): 8.8371,  E(|Yhat-Yhat'|): 13.8856\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.1538,  E(|Y-Yhat|): 20.5036,  E(|Yhat-Yhat'|): 36.6997\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.2730,  E(|Y-Yhat|): 20.7348,  E(|Yhat-Yhat'|): 36.9237\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3684,  E(|Y-Yhat|): 0.6107,  E(|Yhat-Yhat'|): 0.4846\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3333,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6685\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3329,  E(|Y-Yhat|): 0.6684,  E(|Yhat-Yhat'|): 0.6710\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3298,  E(|Y-Yhat|): 0.6648,  E(|Yhat-Yhat'|): 0.6699\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3377,  E(|Y-Yhat|): 0.6777,  E(|Yhat-Yhat'|): 0.6800\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7298,  E(|Y-Yhat|): 1.1678,  E(|Yhat-Yhat'|): 0.8760\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6645,  E(|Y-Yhat|): 1.3353,  E(|Yhat-Yhat'|): 1.3416\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6713,  E(|Y-Yhat|): 1.3537,  E(|Yhat-Yhat'|): 1.3647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6677,  E(|Y-Yhat|): 1.3434,  E(|Yhat-Yhat'|): 1.3514\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6784,  E(|Y-Yhat|): 1.3593,  E(|Yhat-Yhat'|): 1.3618\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3600,  E(|Y-Yhat|): 0.6325,  E(|Yhat-Yhat'|): 0.5450\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3287,  E(|Y-Yhat|): 0.6621,  E(|Yhat-Yhat'|): 0.6667\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3302,  E(|Y-Yhat|): 0.6634,  E(|Yhat-Yhat'|): 0.6664\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3358,  E(|Y-Yhat|): 0.6685,  E(|Yhat-Yhat'|): 0.6654\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3348,  E(|Y-Yhat|): 0.6731,  E(|Yhat-Yhat'|): 0.6765\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8409,  E(|Y-Yhat|): 1.4238,  E(|Yhat-Yhat'|): 1.1659\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6358,  E(|Y-Yhat|): 1.2737,  E(|Yhat-Yhat'|): 1.2758\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6253,  E(|Y-Yhat|): 1.2609,  E(|Yhat-Yhat'|): 1.2713\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6245,  E(|Y-Yhat|): 1.2775,  E(|Yhat-Yhat'|): 1.3061\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0349,  E(|Y-Yhat|): 4.1415,  E(|Yhat-Yhat'|): 4.2134\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8664,  E(|Y-Yhat|): 2.1399,  E(|Yhat-Yhat'|): 0.5470\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 3.0714,  E(|Y-Yhat|): 106.6112,  E(|Yhat-Yhat'|): 207.0796\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8448,  E(|Y-Yhat|): 10.9385,  E(|Yhat-Yhat'|): 20.1873\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8418,  E(|Y-Yhat|): 8.9087,  E(|Yhat-Yhat'|): 16.1337\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8659,  E(|Y-Yhat|): 18.8254,  E(|Yhat-Yhat'|): 33.9190\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3314,  E(|Y-Yhat|): 0.5638,  E(|Yhat-Yhat'|): 0.4648\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2505,  E(|Y-Yhat|): 0.5022,  E(|Yhat-Yhat'|): 0.5033\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2527,  E(|Y-Yhat|): 0.5121,  E(|Yhat-Yhat'|): 0.5187\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2481,  E(|Y-Yhat|): 0.5059,  E(|Yhat-Yhat'|): 0.5154\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3384,  E(|Y-Yhat|): 0.6764,  E(|Yhat-Yhat'|): 0.6760\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6574,  E(|Y-Yhat|): 1.0747,  E(|Yhat-Yhat'|): 0.8346\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4589,  E(|Y-Yhat|): 0.9271,  E(|Yhat-Yhat'|): 0.9364\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4665,  E(|Y-Yhat|): 0.9370,  E(|Yhat-Yhat'|): 0.9410\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4614,  E(|Y-Yhat|): 0.9389,  E(|Yhat-Yhat'|): 0.9551\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6724,  E(|Y-Yhat|): 1.3737,  E(|Yhat-Yhat'|): 1.4028\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3221,  E(|Y-Yhat|): 0.5897,  E(|Yhat-Yhat'|): 0.5353\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2699,  E(|Y-Yhat|): 0.5483,  E(|Yhat-Yhat'|): 0.5568\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2720,  E(|Y-Yhat|): 0.5398,  E(|Yhat-Yhat'|): 0.5357\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2718,  E(|Y-Yhat|): 0.5417,  E(|Yhat-Yhat'|): 0.5398\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3327,  E(|Y-Yhat|): 0.6670,  E(|Yhat-Yhat'|): 0.6685\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9105,  E(|Y-Yhat|): 1.5961,  E(|Yhat-Yhat'|): 1.3712\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8603,  E(|Y-Yhat|): 1.7562,  E(|Yhat-Yhat'|): 1.7917\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8613,  E(|Y-Yhat|): 1.7539,  E(|Yhat-Yhat'|): 1.7852\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8641,  E(|Y-Yhat|): 1.7507,  E(|Yhat-Yhat'|): 1.7732\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0311,  E(|Y-Yhat|): 4.0619,  E(|Yhat-Yhat'|): 4.0615\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9668,  E(|Y-Yhat|): 2.3079,  E(|Yhat-Yhat'|): 0.6823\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9274,  E(|Y-Yhat|): 11.8656,  E(|Yhat-Yhat'|): 19.8764\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9468,  E(|Y-Yhat|): 7.4229,  E(|Yhat-Yhat'|): 10.9521\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9310,  E(|Y-Yhat|): 6.1726,  E(|Yhat-Yhat'|): 8.4833\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0064,  E(|Y-Yhat|): 6.2450,  E(|Yhat-Yhat'|): 8.4771\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3538,  E(|Y-Yhat|): 0.6214,  E(|Yhat-Yhat'|): 0.5352\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3302,  E(|Y-Yhat|): 0.6650,  E(|Yhat-Yhat'|): 0.6697\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3265,  E(|Y-Yhat|): 0.6589,  E(|Yhat-Yhat'|): 0.6647\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3277,  E(|Y-Yhat|): 0.6604,  E(|Yhat-Yhat'|): 0.6655\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3346,  E(|Y-Yhat|): 0.6708,  E(|Yhat-Yhat'|): 0.6725\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7279,  E(|Y-Yhat|): 1.1879,  E(|Yhat-Yhat'|): 0.9200\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6486,  E(|Y-Yhat|): 1.3239,  E(|Yhat-Yhat'|): 1.3506\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6591,  E(|Y-Yhat|): 1.3219,  E(|Yhat-Yhat'|): 1.3255\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6622,  E(|Y-Yhat|): 1.3294,  E(|Yhat-Yhat'|): 1.3344\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6667,  E(|Y-Yhat|): 1.3540,  E(|Yhat-Yhat'|): 1.3746\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3592,  E(|Y-Yhat|): 0.6359,  E(|Yhat-Yhat'|): 0.5534\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3305,  E(|Y-Yhat|): 0.6630,  E(|Yhat-Yhat'|): 0.6651\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3292,  E(|Y-Yhat|): 0.6595,  E(|Yhat-Yhat'|): 0.6607\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3287,  E(|Y-Yhat|): 0.6595,  E(|Yhat-Yhat'|): 0.6615\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3307,  E(|Y-Yhat|): 0.6659,  E(|Yhat-Yhat'|): 0.6704\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9345,  E(|Y-Yhat|): 1.5311,  E(|Yhat-Yhat'|): 1.1932\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8671,  E(|Y-Yhat|): 1.7376,  E(|Yhat-Yhat'|): 1.7410\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8605,  E(|Y-Yhat|): 1.7557,  E(|Yhat-Yhat'|): 1.7903\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8603,  E(|Y-Yhat|): 1.7159,  E(|Yhat-Yhat'|): 1.7112\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9961,  E(|Y-Yhat|): 4.0725,  E(|Yhat-Yhat'|): 4.1528\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0471,  E(|Y-Yhat|): 2.4181,  E(|Yhat-Yhat'|): 0.7419\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9175,  E(|Y-Yhat|): 14.2607,  E(|Yhat-Yhat'|): 24.6863\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8695,  E(|Y-Yhat|): 6.6430,  E(|Yhat-Yhat'|): 9.5470\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9731,  E(|Y-Yhat|): 9.9223,  E(|Yhat-Yhat'|): 15.8983\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8694,  E(|Y-Yhat|): 10.3469,  E(|Yhat-Yhat'|): 16.9549\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3667,  E(|Y-Yhat|): 0.6133,  E(|Yhat-Yhat'|): 0.4931\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3269,  E(|Y-Yhat|): 0.6610,  E(|Yhat-Yhat'|): 0.6682\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3288,  E(|Y-Yhat|): 0.6615,  E(|Yhat-Yhat'|): 0.6654\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3258,  E(|Y-Yhat|): 0.6600,  E(|Yhat-Yhat'|): 0.6683\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3316,  E(|Y-Yhat|): 0.6659,  E(|Yhat-Yhat'|): 0.6685\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7127,  E(|Y-Yhat|): 1.1974,  E(|Yhat-Yhat'|): 0.9694\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6561,  E(|Y-Yhat|): 1.3197,  E(|Yhat-Yhat'|): 1.3272\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6629,  E(|Y-Yhat|): 1.3239,  E(|Yhat-Yhat'|): 1.3220\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6573,  E(|Y-Yhat|): 1.3167,  E(|Yhat-Yhat'|): 1.3187\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6674,  E(|Y-Yhat|): 1.3444,  E(|Yhat-Yhat'|): 1.3541\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3550,  E(|Y-Yhat|): 0.6323,  E(|Yhat-Yhat'|): 0.5545\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3297,  E(|Y-Yhat|): 0.6638,  E(|Yhat-Yhat'|): 0.6682\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3247,  E(|Y-Yhat|): 0.6603,  E(|Yhat-Yhat'|): 0.6712\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3272,  E(|Y-Yhat|): 0.6604,  E(|Yhat-Yhat'|): 0.6664\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3304,  E(|Y-Yhat|): 0.6625,  E(|Yhat-Yhat'|): 0.6642\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9191,  E(|Y-Yhat|): 1.5371,  E(|Yhat-Yhat'|): 1.2359\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8568,  E(|Y-Yhat|): 1.7214,  E(|Yhat-Yhat'|): 1.7294\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8539,  E(|Y-Yhat|): 1.7252,  E(|Yhat-Yhat'|): 1.7426\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8431,  E(|Y-Yhat|): 1.7009,  E(|Yhat-Yhat'|): 1.7156\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0568,  E(|Y-Yhat|): 4.0942,  E(|Yhat-Yhat'|): 4.0748\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0261,  E(|Y-Yhat|): 2.3171,  E(|Yhat-Yhat'|): 0.5821\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8894,  E(|Y-Yhat|): 44.5177,  E(|Yhat-Yhat'|): 85.2565\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 5.0193,  E(|Y-Yhat|): 756.8387,  E(|Yhat-Yhat'|): 1503.6388\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 5.3895,  E(|Y-Yhat|): 454.5675,  E(|Yhat-Yhat'|): 898.3560\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.6358,  E(|Y-Yhat|): 495.3832,  E(|Yhat-Yhat'|): 985.4948\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3684,  E(|Y-Yhat|): 0.6210,  E(|Yhat-Yhat'|): 0.5052\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3271,  E(|Y-Yhat|): 0.6525,  E(|Yhat-Yhat'|): 0.6509\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3208,  E(|Y-Yhat|): 0.6483,  E(|Yhat-Yhat'|): 0.6550\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3240,  E(|Y-Yhat|): 0.6504,  E(|Yhat-Yhat'|): 0.6529\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3352,  E(|Y-Yhat|): 0.6741,  E(|Yhat-Yhat'|): 0.6780\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7261,  E(|Y-Yhat|): 1.1786,  E(|Yhat-Yhat'|): 0.9050\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6470,  E(|Y-Yhat|): 1.2835,  E(|Yhat-Yhat'|): 1.2729\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6588,  E(|Y-Yhat|): 1.3088,  E(|Yhat-Yhat'|): 1.3000\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6354,  E(|Y-Yhat|): 1.2825,  E(|Yhat-Yhat'|): 1.2941\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6763,  E(|Y-Yhat|): 1.3550,  E(|Yhat-Yhat'|): 1.3573\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3569,  E(|Y-Yhat|): 0.6316,  E(|Yhat-Yhat'|): 0.5493\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3265,  E(|Y-Yhat|): 0.6520,  E(|Yhat-Yhat'|): 0.6508\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3244,  E(|Y-Yhat|): 0.6531,  E(|Yhat-Yhat'|): 0.6574\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3249,  E(|Y-Yhat|): 0.6511,  E(|Yhat-Yhat'|): 0.6522\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3312,  E(|Y-Yhat|): 0.6664,  E(|Yhat-Yhat'|): 0.6704\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0038),\n",
       " tensor(0.6292),\n",
       " tensor(0.0048),\n",
       " tensor(0.0037),\n",
       " tensor(0.0048))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"square\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = (F.relu(Z))**2 / 2.0\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb6308",
   "metadata": {},
   "source": [
    "## True function: log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0d8186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8852,  E(|Y-Yhat|): 1.5405,  E(|Yhat-Yhat'|): 1.3106\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7509,  E(|Y-Yhat|): 1.5202,  E(|Yhat-Yhat'|): 1.5387\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7406,  E(|Y-Yhat|): 1.5293,  E(|Yhat-Yhat'|): 1.5774\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7599,  E(|Y-Yhat|): 1.5124,  E(|Yhat-Yhat'|): 1.5050\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0352,  E(|Y-Yhat|): 4.1411,  E(|Yhat-Yhat'|): 4.2117\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7612,  E(|Y-Yhat|): 2.2161,  E(|Yhat-Yhat'|): 0.9098\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4821,  E(|Y-Yhat|): 9.5094,  E(|Yhat-Yhat'|): 16.0545\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4347,  E(|Y-Yhat|): 3.4695,  E(|Yhat-Yhat'|): 4.0696\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4231,  E(|Y-Yhat|): 2.9634,  E(|Yhat-Yhat'|): 3.0805\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9847,  E(|Y-Yhat|): 4.6802,  E(|Yhat-Yhat'|): 5.3910\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3577,  E(|Y-Yhat|): 0.6056,  E(|Yhat-Yhat'|): 0.4959\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2902,  E(|Y-Yhat|): 0.5847,  E(|Yhat-Yhat'|): 0.5888\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2892,  E(|Y-Yhat|): 0.5850,  E(|Yhat-Yhat'|): 0.5916\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2916,  E(|Y-Yhat|): 0.5832,  E(|Yhat-Yhat'|): 0.5833\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3325,  E(|Y-Yhat|): 0.6685,  E(|Yhat-Yhat'|): 0.6720\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6756,  E(|Y-Yhat|): 1.1245,  E(|Yhat-Yhat'|): 0.8978\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5558,  E(|Y-Yhat|): 1.1355,  E(|Yhat-Yhat'|): 1.1594\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5669,  E(|Y-Yhat|): 1.1384,  E(|Yhat-Yhat'|): 1.1430\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5607,  E(|Y-Yhat|): 1.1261,  E(|Yhat-Yhat'|): 1.1308\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6672,  E(|Y-Yhat|): 1.3539,  E(|Yhat-Yhat'|): 1.3733\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3492,  E(|Y-Yhat|): 0.6307,  E(|Yhat-Yhat'|): 0.5631\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3026,  E(|Y-Yhat|): 0.6079,  E(|Yhat-Yhat'|): 0.6104\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3031,  E(|Y-Yhat|): 0.6084,  E(|Yhat-Yhat'|): 0.6108\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2998,  E(|Y-Yhat|): 0.6033,  E(|Yhat-Yhat'|): 0.6070\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3334,  E(|Y-Yhat|): 0.6707,  E(|Yhat-Yhat'|): 0.6746\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9327,  E(|Y-Yhat|): 1.5701,  E(|Yhat-Yhat'|): 1.2748\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8924,  E(|Y-Yhat|): 1.7466,  E(|Yhat-Yhat'|): 1.7084\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8815,  E(|Y-Yhat|): 1.8018,  E(|Yhat-Yhat'|): 1.8406\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8829,  E(|Y-Yhat|): 1.7717,  E(|Yhat-Yhat'|): 1.7777\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9974,  E(|Y-Yhat|): 4.1181,  E(|Yhat-Yhat'|): 4.2412\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0664,  E(|Y-Yhat|): 2.3545,  E(|Yhat-Yhat'|): 0.5761\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.8249,  E(|Y-Yhat|): 212.9417,  E(|Yhat-Yhat'|): 420.2336\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.6378,  E(|Y-Yhat|): 129.2884,  E(|Yhat-Yhat'|): 253.3013\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.1530,  E(|Y-Yhat|): 63.6945,  E(|Yhat-Yhat'|): 123.0831\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.3282,  E(|Y-Yhat|): 66.3547,  E(|Yhat-Yhat'|): 128.0531\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3583,  E(|Y-Yhat|): 0.6316,  E(|Yhat-Yhat'|): 0.5466\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3318,  E(|Y-Yhat|): 0.6680,  E(|Yhat-Yhat'|): 0.6724\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3355,  E(|Y-Yhat|): 0.6689,  E(|Yhat-Yhat'|): 0.6667\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3337,  E(|Y-Yhat|): 0.6698,  E(|Yhat-Yhat'|): 0.6722\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3314,  E(|Y-Yhat|): 0.6703,  E(|Yhat-Yhat'|): 0.6778\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7214,  E(|Y-Yhat|): 1.1657,  E(|Yhat-Yhat'|): 0.8885\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6686,  E(|Y-Yhat|): 1.3502,  E(|Yhat-Yhat'|): 1.3633\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6625,  E(|Y-Yhat|): 1.3415,  E(|Yhat-Yhat'|): 1.3580\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6661,  E(|Y-Yhat|): 1.3407,  E(|Yhat-Yhat'|): 1.3492\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6746,  E(|Y-Yhat|): 1.3573,  E(|Yhat-Yhat'|): 1.3655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3582,  E(|Y-Yhat|): 0.6334,  E(|Yhat-Yhat'|): 0.5503\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3283,  E(|Y-Yhat|): 0.6638,  E(|Yhat-Yhat'|): 0.6711\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3303,  E(|Y-Yhat|): 0.6662,  E(|Yhat-Yhat'|): 0.6717\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6668,  E(|Yhat-Yhat'|): 0.6661\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3337,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9184,  E(|Y-Yhat|): 1.5869,  E(|Yhat-Yhat'|): 1.3369\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8787,  E(|Y-Yhat|): 1.7862,  E(|Yhat-Yhat'|): 1.8150\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8733,  E(|Y-Yhat|): 1.7698,  E(|Yhat-Yhat'|): 1.7929\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8703,  E(|Y-Yhat|): 1.7544,  E(|Yhat-Yhat'|): 1.7683\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9931,  E(|Y-Yhat|): 3.9997,  E(|Yhat-Yhat'|): 4.0132\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1496,  E(|Y-Yhat|): 2.4889,  E(|Yhat-Yhat'|): 0.6788\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9432,  E(|Y-Yhat|): 3.5698,  E(|Yhat-Yhat'|): 3.2532\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9897,  E(|Y-Yhat|): 6.3270,  E(|Yhat-Yhat'|): 8.6747\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9402,  E(|Y-Yhat|): 11.5123,  E(|Yhat-Yhat'|): 19.1442\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0816,  E(|Y-Yhat|): 12.0728,  E(|Yhat-Yhat'|): 19.9823\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3582,  E(|Y-Yhat|): 0.6261,  E(|Yhat-Yhat'|): 0.5358\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3268,  E(|Y-Yhat|): 0.6640,  E(|Yhat-Yhat'|): 0.6744\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3309,  E(|Y-Yhat|): 0.6655,  E(|Yhat-Yhat'|): 0.6693\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3358,  E(|Y-Yhat|): 0.6722,  E(|Yhat-Yhat'|): 0.6727\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3301,  E(|Y-Yhat|): 0.6684,  E(|Yhat-Yhat'|): 0.6767\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7241,  E(|Y-Yhat|): 1.1836,  E(|Yhat-Yhat'|): 0.9188\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6644,  E(|Y-Yhat|): 1.3443,  E(|Yhat-Yhat'|): 1.3597\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6734,  E(|Y-Yhat|): 1.3328,  E(|Yhat-Yhat'|): 1.3187\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6660,  E(|Y-Yhat|): 1.3395,  E(|Yhat-Yhat'|): 1.3470\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6783,  E(|Y-Yhat|): 1.3546,  E(|Yhat-Yhat'|): 1.3525\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3635,  E(|Y-Yhat|): 0.6307,  E(|Yhat-Yhat'|): 0.5343\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3293,  E(|Y-Yhat|): 0.6621,  E(|Yhat-Yhat'|): 0.6655\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6672,  E(|Yhat-Yhat'|): 0.6668\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3324,  E(|Y-Yhat|): 0.6639,  E(|Yhat-Yhat'|): 0.6630\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3287,  E(|Y-Yhat|): 0.6613,  E(|Yhat-Yhat'|): 0.6652\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9475,  E(|Y-Yhat|): 1.5249,  E(|Yhat-Yhat'|): 1.1549\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8792,  E(|Y-Yhat|): 1.7848,  E(|Yhat-Yhat'|): 1.8113\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8782,  E(|Y-Yhat|): 1.7905,  E(|Yhat-Yhat'|): 1.8247\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8811,  E(|Y-Yhat|): 1.7652,  E(|Yhat-Yhat'|): 1.7682\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9790,  E(|Y-Yhat|): 4.0214,  E(|Yhat-Yhat'|): 4.0848\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0119,  E(|Y-Yhat|): 2.3511,  E(|Yhat-Yhat'|): 0.6786\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9313,  E(|Y-Yhat|): 3.6542,  E(|Yhat-Yhat'|): 3.4459\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9788,  E(|Y-Yhat|): 3.3354,  E(|Yhat-Yhat'|): 2.7132\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9410,  E(|Y-Yhat|): 4.2594,  E(|Yhat-Yhat'|): 4.6369\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9528,  E(|Y-Yhat|): 4.4043,  E(|Yhat-Yhat'|): 4.9030\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3611,  E(|Y-Yhat|): 0.6135,  E(|Yhat-Yhat'|): 0.5049\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3353,  E(|Y-Yhat|): 0.6695,  E(|Yhat-Yhat'|): 0.6683\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3325,  E(|Y-Yhat|): 0.6678,  E(|Yhat-Yhat'|): 0.6705\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3366,  E(|Y-Yhat|): 0.6677,  E(|Yhat-Yhat'|): 0.6622\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3345,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6608\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7134,  E(|Y-Yhat|): 1.1884,  E(|Yhat-Yhat'|): 0.9501\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6664,  E(|Y-Yhat|): 1.3409,  E(|Yhat-Yhat'|): 1.3491\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6749,  E(|Y-Yhat|): 1.3434,  E(|Yhat-Yhat'|): 1.3370\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6635,  E(|Y-Yhat|): 1.3349,  E(|Yhat-Yhat'|): 1.3428\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6716,  E(|Y-Yhat|): 1.3507,  E(|Yhat-Yhat'|): 1.3582\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3540,  E(|Y-Yhat|): 0.6423,  E(|Yhat-Yhat'|): 0.5765\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3317,  E(|Y-Yhat|): 0.6614,  E(|Yhat-Yhat'|): 0.6594\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3334,  E(|Y-Yhat|): 0.6644,  E(|Yhat-Yhat'|): 0.6621\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3280,  E(|Y-Yhat|): 0.6591,  E(|Yhat-Yhat'|): 0.6623\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3348,  E(|Y-Yhat|): 0.6655,  E(|Yhat-Yhat'|): 0.6615\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8705,  E(|Y-Yhat|): 1.4781,  E(|Yhat-Yhat'|): 1.2151\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6828,  E(|Y-Yhat|): 1.4025,  E(|Yhat-Yhat'|): 1.4394\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6854,  E(|Y-Yhat|): 1.3889,  E(|Yhat-Yhat'|): 1.4069\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6871,  E(|Y-Yhat|): 1.3955,  E(|Yhat-Yhat'|): 1.4169\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0312,  E(|Y-Yhat|): 4.1646,  E(|Yhat-Yhat'|): 4.2668\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7895,  E(|Y-Yhat|): 2.1490,  E(|Yhat-Yhat'|): 0.7191\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3538,  E(|Y-Yhat|): 48.0140,  E(|Yhat-Yhat'|): 93.3202\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.1799,  E(|Y-Yhat|): 35.8455,  E(|Yhat-Yhat'|): 69.3311\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.9802,  E(|Y-Yhat|): 86.7926,  E(|Yhat-Yhat'|): 167.6248\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.8580,  E(|Y-Yhat|): 179.8331,  E(|Yhat-Yhat'|): 349.9502\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3439,  E(|Y-Yhat|): 0.5899,  E(|Yhat-Yhat'|): 0.4919\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2707,  E(|Y-Yhat|): 0.5417,  E(|Yhat-Yhat'|): 0.5419\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2677,  E(|Y-Yhat|): 0.5429,  E(|Yhat-Yhat'|): 0.5503\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2709,  E(|Y-Yhat|): 0.5428,  E(|Yhat-Yhat'|): 0.5437\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3341,  E(|Y-Yhat|): 0.6661,  E(|Yhat-Yhat'|): 0.6641\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6705,  E(|Y-Yhat|): 1.1258,  E(|Yhat-Yhat'|): 0.9107\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5066,  E(|Y-Yhat|): 1.0243,  E(|Yhat-Yhat'|): 1.0354\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5167,  E(|Y-Yhat|): 1.0340,  E(|Yhat-Yhat'|): 1.0346\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5136,  E(|Y-Yhat|): 1.0268,  E(|Yhat-Yhat'|): 1.0265\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6867,  E(|Y-Yhat|): 1.3669,  E(|Yhat-Yhat'|): 1.3605\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3490,  E(|Y-Yhat|): 0.6221,  E(|Yhat-Yhat'|): 0.5462\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2825,  E(|Y-Yhat|): 0.5717,  E(|Yhat-Yhat'|): 0.5783\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2820,  E(|Y-Yhat|): 0.5715,  E(|Yhat-Yhat'|): 0.5790\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2836,  E(|Y-Yhat|): 0.5710,  E(|Yhat-Yhat'|): 0.5747\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3297,  E(|Y-Yhat|): 0.6653,  E(|Yhat-Yhat'|): 0.6711\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9424,  E(|Y-Yhat|): 1.5559,  E(|Yhat-Yhat'|): 1.2271\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8791,  E(|Y-Yhat|): 1.7858,  E(|Yhat-Yhat'|): 1.8134\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8691,  E(|Y-Yhat|): 1.7452,  E(|Yhat-Yhat'|): 1.7523\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8775,  E(|Y-Yhat|): 1.7677,  E(|Yhat-Yhat'|): 1.7805\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0521,  E(|Y-Yhat|): 4.1623,  E(|Yhat-Yhat'|): 4.2204\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0614,  E(|Y-Yhat|): 2.4111,  E(|Yhat-Yhat'|): 0.6994\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.0747,  E(|Y-Yhat|): 9.9209,  E(|Yhat-Yhat'|): 15.6922\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9184,  E(|Y-Yhat|): 10.8863,  E(|Yhat-Yhat'|): 17.9358\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9771,  E(|Y-Yhat|): 14.0198,  E(|Yhat-Yhat'|): 24.0853\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.1476,  E(|Y-Yhat|): 14.7628,  E(|Yhat-Yhat'|): 25.2304\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3672,  E(|Y-Yhat|): 0.6095,  E(|Yhat-Yhat'|): 0.4848\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3335,  E(|Y-Yhat|): 0.6693,  E(|Yhat-Yhat'|): 0.6716\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3327,  E(|Y-Yhat|): 0.6687,  E(|Yhat-Yhat'|): 0.6721\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3283,  E(|Y-Yhat|): 0.6625,  E(|Yhat-Yhat'|): 0.6684\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3360,  E(|Y-Yhat|): 0.6718,  E(|Yhat-Yhat'|): 0.6717\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7262,  E(|Y-Yhat|): 1.1641,  E(|Yhat-Yhat'|): 0.8758\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6614,  E(|Y-Yhat|): 1.3271,  E(|Yhat-Yhat'|): 1.3315\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6586,  E(|Y-Yhat|): 1.3430,  E(|Yhat-Yhat'|): 1.3687\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6625,  E(|Y-Yhat|): 1.3376,  E(|Yhat-Yhat'|): 1.3502\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6762,  E(|Y-Yhat|): 1.3684,  E(|Yhat-Yhat'|): 1.3843\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3591,  E(|Y-Yhat|): 0.6316,  E(|Yhat-Yhat'|): 0.5449\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3279,  E(|Y-Yhat|): 0.6638,  E(|Yhat-Yhat'|): 0.6718\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3294,  E(|Y-Yhat|): 0.6613,  E(|Yhat-Yhat'|): 0.6639\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3308,  E(|Y-Yhat|): 0.6636,  E(|Yhat-Yhat'|): 0.6656\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3345,  E(|Y-Yhat|): 0.6694,  E(|Yhat-Yhat'|): 0.6700\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8914,  E(|Y-Yhat|): 1.5055,  E(|Yhat-Yhat'|): 1.2283\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7911,  E(|Y-Yhat|): 1.5953,  E(|Yhat-Yhat'|): 1.6084\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7891,  E(|Y-Yhat|): 1.5948,  E(|Yhat-Yhat'|): 1.6115\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7970,  E(|Y-Yhat|): 1.6038,  E(|Yhat-Yhat'|): 1.6135\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0099,  E(|Y-Yhat|): 4.1031,  E(|Yhat-Yhat'|): 4.1864\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8879,  E(|Y-Yhat|): 2.1708,  E(|Yhat-Yhat'|): 0.5659\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6581,  E(|Y-Yhat|): 11.9024,  E(|Yhat-Yhat'|): 20.4886\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5419,  E(|Y-Yhat|): 14.2072,  E(|Yhat-Yhat'|): 25.3306\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.5778,  E(|Y-Yhat|): 18.7241,  E(|Yhat-Yhat'|): 34.2927\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.1257,  E(|Y-Yhat|): 24.3665,  E(|Yhat-Yhat'|): 44.4816\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3581,  E(|Y-Yhat|): 0.6182,  E(|Yhat-Yhat'|): 0.5202\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3086,  E(|Y-Yhat|): 0.6204,  E(|Yhat-Yhat'|): 0.6236\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3084,  E(|Y-Yhat|): 0.6173,  E(|Yhat-Yhat'|): 0.6179\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3091,  E(|Y-Yhat|): 0.6161,  E(|Yhat-Yhat'|): 0.6140\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3371,  E(|Y-Yhat|): 0.6672,  E(|Yhat-Yhat'|): 0.6603\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6874,  E(|Y-Yhat|): 1.1483,  E(|Yhat-Yhat'|): 0.9218\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5841,  E(|Y-Yhat|): 1.1931,  E(|Yhat-Yhat'|): 1.2181\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5931,  E(|Y-Yhat|): 1.2003,  E(|Yhat-Yhat'|): 1.2144\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5916,  E(|Y-Yhat|): 1.1894,  E(|Yhat-Yhat'|): 1.1956\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6664,  E(|Y-Yhat|): 1.3411,  E(|Yhat-Yhat'|): 1.3493\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3430,  E(|Y-Yhat|): 0.6292,  E(|Yhat-Yhat'|): 0.5724\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3128,  E(|Y-Yhat|): 0.6288,  E(|Yhat-Yhat'|): 0.6320\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3125,  E(|Y-Yhat|): 0.6288,  E(|Yhat-Yhat'|): 0.6327\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3119,  E(|Y-Yhat|): 0.6203,  E(|Yhat-Yhat'|): 0.6169\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3341,  E(|Y-Yhat|): 0.6662,  E(|Yhat-Yhat'|): 0.6642\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8921,  E(|Y-Yhat|): 1.5759,  E(|Yhat-Yhat'|): 1.3676\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8502,  E(|Y-Yhat|): 1.7376,  E(|Yhat-Yhat'|): 1.7748\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8489,  E(|Y-Yhat|): 1.7190,  E(|Yhat-Yhat'|): 1.7400\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8444,  E(|Y-Yhat|): 1.7213,  E(|Yhat-Yhat'|): 1.7538\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9644,  E(|Y-Yhat|): 4.0487,  E(|Yhat-Yhat'|): 4.1686\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9254,  E(|Y-Yhat|): 2.2689,  E(|Yhat-Yhat'|): 0.6869\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8095,  E(|Y-Yhat|): 12.6796,  E(|Yhat-Yhat'|): 21.7401\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8426,  E(|Y-Yhat|): 18.3171,  E(|Yhat-Yhat'|): 32.9490\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.2634,  E(|Y-Yhat|): 50.0727,  E(|Yhat-Yhat'|): 95.6186\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0999,  E(|Y-Yhat|): 49.0671,  E(|Yhat-Yhat'|): 93.9343\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3516,  E(|Y-Yhat|): 0.6189,  E(|Yhat-Yhat'|): 0.5345\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3247,  E(|Y-Yhat|): 0.6510,  E(|Yhat-Yhat'|): 0.6526\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3251,  E(|Y-Yhat|): 0.6510,  E(|Yhat-Yhat'|): 0.6518\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3214,  E(|Y-Yhat|): 0.6475,  E(|Yhat-Yhat'|): 0.6522\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3343,  E(|Y-Yhat|): 0.6683,  E(|Yhat-Yhat'|): 0.6682\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7214,  E(|Y-Yhat|): 1.1800,  E(|Yhat-Yhat'|): 0.9173\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6421,  E(|Y-Yhat|): 1.3115,  E(|Yhat-Yhat'|): 1.3388\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6388,  E(|Y-Yhat|): 1.2894,  E(|Yhat-Yhat'|): 1.3011\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6480,  E(|Y-Yhat|): 1.2950,  E(|Yhat-Yhat'|): 1.2940\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6689,  E(|Y-Yhat|): 1.3441,  E(|Yhat-Yhat'|): 1.3504\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3578,  E(|Y-Yhat|): 0.6347,  E(|Yhat-Yhat'|): 0.5538\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3235,  E(|Y-Yhat|): 0.6509,  E(|Yhat-Yhat'|): 0.6549\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3229,  E(|Y-Yhat|): 0.6534,  E(|Yhat-Yhat'|): 0.6609\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3257,  E(|Y-Yhat|): 0.6516,  E(|Yhat-Yhat'|): 0.6517\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3332,  E(|Y-Yhat|): 0.6658,  E(|Yhat-Yhat'|): 0.6652\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9326,  E(|Y-Yhat|): 1.5273,  E(|Yhat-Yhat'|): 1.1892\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8695,  E(|Y-Yhat|): 1.7404,  E(|Yhat-Yhat'|): 1.7418\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8648,  E(|Y-Yhat|): 1.7570,  E(|Yhat-Yhat'|): 1.7843\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8637,  E(|Y-Yhat|): 1.7354,  E(|Yhat-Yhat'|): 1.7434\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9958,  E(|Y-Yhat|): 4.0691,  E(|Yhat-Yhat'|): 4.1466\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0409,  E(|Y-Yhat|): 2.4139,  E(|Yhat-Yhat'|): 0.7458\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9501,  E(|Y-Yhat|): 13.9038,  E(|Yhat-Yhat'|): 23.9074\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.6260,  E(|Y-Yhat|): 19.2848,  E(|Yhat-Yhat'|): 35.3176\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0024,  E(|Y-Yhat|): 12.2742,  E(|Yhat-Yhat'|): 20.5437\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0234,  E(|Y-Yhat|): 12.6567,  E(|Yhat-Yhat'|): 21.2666\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3668,  E(|Y-Yhat|): 0.6124,  E(|Yhat-Yhat'|): 0.4911\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3268,  E(|Y-Yhat|): 0.6585,  E(|Yhat-Yhat'|): 0.6634\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3282,  E(|Y-Yhat|): 0.6604,  E(|Yhat-Yhat'|): 0.6645\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3279,  E(|Y-Yhat|): 0.6606,  E(|Yhat-Yhat'|): 0.6654\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3300,  E(|Y-Yhat|): 0.6675,  E(|Yhat-Yhat'|): 0.6750\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7125,  E(|Y-Yhat|): 1.1910,  E(|Yhat-Yhat'|): 0.9569\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6559,  E(|Y-Yhat|): 1.3187,  E(|Yhat-Yhat'|): 1.3256\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6555,  E(|Y-Yhat|): 1.3235,  E(|Yhat-Yhat'|): 1.3361\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6540,  E(|Y-Yhat|): 1.3217,  E(|Yhat-Yhat'|): 1.3354\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6663,  E(|Y-Yhat|): 1.3422,  E(|Yhat-Yhat'|): 1.3518\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3545,  E(|Y-Yhat|): 0.6322,  E(|Yhat-Yhat'|): 0.5555\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3300,  E(|Y-Yhat|): 0.6593,  E(|Yhat-Yhat'|): 0.6587\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3300,  E(|Y-Yhat|): 0.6575,  E(|Yhat-Yhat'|): 0.6550\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3265,  E(|Y-Yhat|): 0.6610,  E(|Yhat-Yhat'|): 0.6691\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6689,  E(|Yhat-Yhat'|): 0.6712\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9268,  E(|Y-Yhat|): 1.5457,  E(|Yhat-Yhat'|): 1.2378\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8863,  E(|Y-Yhat|): 1.7779,  E(|Yhat-Yhat'|): 1.7833\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8757,  E(|Y-Yhat|): 1.7646,  E(|Yhat-Yhat'|): 1.7777\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8823,  E(|Y-Yhat|): 1.7694,  E(|Yhat-Yhat'|): 1.7742\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0170,  E(|Y-Yhat|): 4.1233,  E(|Yhat-Yhat'|): 4.2125\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0354,  E(|Y-Yhat|): 2.3291,  E(|Yhat-Yhat'|): 0.5873\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9903,  E(|Y-Yhat|): 30.9815,  E(|Yhat-Yhat'|): 57.9825\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 11.5181,  E(|Y-Yhat|): 982.2611,  E(|Yhat-Yhat'|): 1941.4859\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 4.6041,  E(|Y-Yhat|): 542.5531,  E(|Yhat-Yhat'|): 1075.8979\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.6195,  E(|Y-Yhat|): 556.5283,  E(|Yhat-Yhat'|): 1105.8176\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3691,  E(|Y-Yhat|): 0.6232,  E(|Yhat-Yhat'|): 0.5081\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3305,  E(|Y-Yhat|): 0.6611,  E(|Yhat-Yhat'|): 0.6612\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3328,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6674\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3317,  E(|Y-Yhat|): 0.6630,  E(|Yhat-Yhat'|): 0.6627\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3311,  E(|Y-Yhat|): 0.6653,  E(|Yhat-Yhat'|): 0.6684\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7283,  E(|Y-Yhat|): 1.1831,  E(|Yhat-Yhat'|): 0.9095\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6580,  E(|Y-Yhat|): 1.3241,  E(|Yhat-Yhat'|): 1.3321\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6608,  E(|Y-Yhat|): 1.3304,  E(|Yhat-Yhat'|): 1.3392\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6698,  E(|Y-Yhat|): 1.3382,  E(|Yhat-Yhat'|): 1.3368\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6717,  E(|Y-Yhat|): 1.3457,  E(|Yhat-Yhat'|): 1.3480\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3586,  E(|Y-Yhat|): 0.6341,  E(|Yhat-Yhat'|): 0.5509\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3305,  E(|Y-Yhat|): 0.6634,  E(|Yhat-Yhat'|): 0.6659\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3314,  E(|Y-Yhat|): 0.6627,  E(|Yhat-Yhat'|): 0.6627\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3299,  E(|Y-Yhat|): 0.6632,  E(|Yhat-Yhat'|): 0.6667\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3304,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6723\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0029),\n",
       " tensor(0.8703),\n",
       " tensor(0.0030),\n",
       " tensor(0.0029),\n",
       " tensor(0.0039))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=1, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=1, true_function = \"log\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 50)\n",
    "    x2 = torch.linspace(0, 5, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = log_lin(Z)\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa8ff8",
   "metadata": {},
   "source": [
    "# post-ANM, comparing 4 loss functions under different true functions ($X,Y \\in \\mathbb{R}^2, k=2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1ca63",
   "metadata": {},
   "source": [
    "## True function: softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9afde48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8950,  E(|Y-Yhat|): 1.5617,  E(|Yhat-Yhat'|): 1.3335\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7728,  E(|Y-Yhat|): 1.5735,  E(|Yhat-Yhat'|): 1.6015\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7758,  E(|Y-Yhat|): 1.5767,  E(|Yhat-Yhat'|): 1.6019\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7856,  E(|Y-Yhat|): 1.5556,  E(|Yhat-Yhat'|): 1.5400\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9781,  E(|Y-Yhat|): 4.0165,  E(|Yhat-Yhat'|): 4.0768\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8376,  E(|Y-Yhat|): 2.2646,  E(|Yhat-Yhat'|): 0.8539\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6269,  E(|Y-Yhat|): 26.8165,  E(|Yhat-Yhat'|): 50.3792\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5894,  E(|Y-Yhat|): 10.7889,  E(|Yhat-Yhat'|): 18.3989\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.4875,  E(|Y-Yhat|): 5.4591,  E(|Yhat-Yhat'|): 7.9434\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0462,  E(|Y-Yhat|): 6.8632,  E(|Yhat-Yhat'|): 9.6340\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3628,  E(|Y-Yhat|): 0.6082,  E(|Yhat-Yhat'|): 0.4908\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3028,  E(|Y-Yhat|): 0.6077,  E(|Yhat-Yhat'|): 0.6099\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3002,  E(|Y-Yhat|): 0.6091,  E(|Yhat-Yhat'|): 0.6177\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3020,  E(|Y-Yhat|): 0.5998,  E(|Yhat-Yhat'|): 0.5956\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3349,  E(|Y-Yhat|): 0.6668,  E(|Yhat-Yhat'|): 0.6638\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6899,  E(|Y-Yhat|): 1.1444,  E(|Yhat-Yhat'|): 0.9091\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5770,  E(|Y-Yhat|): 1.1781,  E(|Yhat-Yhat'|): 1.2023\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5896,  E(|Y-Yhat|): 1.1774,  E(|Yhat-Yhat'|): 1.1756\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5965,  E(|Y-Yhat|): 1.1861,  E(|Yhat-Yhat'|): 1.1791\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6692,  E(|Y-Yhat|): 1.3496,  E(|Yhat-Yhat'|): 1.3608\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3502,  E(|Y-Yhat|): 0.6328,  E(|Yhat-Yhat'|): 0.5652\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3136,  E(|Y-Yhat|): 0.6315,  E(|Yhat-Yhat'|): 0.6358\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3073,  E(|Y-Yhat|): 0.6195,  E(|Yhat-Yhat'|): 0.6243\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3086,  E(|Y-Yhat|): 0.6187,  E(|Yhat-Yhat'|): 0.6202\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3323,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6706\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9089,  E(|Y-Yhat|): 1.5456,  E(|Yhat-Yhat'|): 1.2734\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8189,  E(|Y-Yhat|): 1.6468,  E(|Yhat-Yhat'|): 1.6559\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8136,  E(|Y-Yhat|): 1.6494,  E(|Yhat-Yhat'|): 1.6717\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8188,  E(|Y-Yhat|): 1.6341,  E(|Yhat-Yhat'|): 1.6306\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0013,  E(|Y-Yhat|): 4.0779,  E(|Yhat-Yhat'|): 4.1532\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9580,  E(|Y-Yhat|): 2.2564,  E(|Yhat-Yhat'|): 0.5967\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.5695,  E(|Y-Yhat|): 13.0111,  E(|Yhat-Yhat'|): 22.8831\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.6596,  E(|Y-Yhat|): 7.7735,  E(|Yhat-Yhat'|): 12.2279\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9725,  E(|Y-Yhat|): 39.6976,  E(|Yhat-Yhat'|): 75.4502\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9620,  E(|Y-Yhat|): 43.1526,  E(|Yhat-Yhat'|): 82.3814\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3515,  E(|Y-Yhat|): 0.6230,  E(|Yhat-Yhat'|): 0.5431\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3171,  E(|Y-Yhat|): 0.6296,  E(|Yhat-Yhat'|): 0.6249\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3131,  E(|Y-Yhat|): 0.6288,  E(|Yhat-Yhat'|): 0.6313\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3139,  E(|Y-Yhat|): 0.6335,  E(|Yhat-Yhat'|): 0.6393\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3302,  E(|Y-Yhat|): 0.6709,  E(|Yhat-Yhat'|): 0.6814\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7186,  E(|Y-Yhat|): 1.1627,  E(|Yhat-Yhat'|): 0.8881\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6080,  E(|Y-Yhat|): 1.2358,  E(|Yhat-Yhat'|): 1.2556\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6184,  E(|Y-Yhat|): 1.2307,  E(|Yhat-Yhat'|): 1.2247\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6096,  E(|Y-Yhat|): 1.2422,  E(|Yhat-Yhat'|): 1.2652\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6687,  E(|Y-Yhat|): 1.3645,  E(|Yhat-Yhat'|): 1.3915\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3533,  E(|Y-Yhat|): 0.6283,  E(|Yhat-Yhat'|): 0.5501\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3155,  E(|Y-Yhat|): 0.6391,  E(|Yhat-Yhat'|): 0.6472\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3203,  E(|Y-Yhat|): 0.6401,  E(|Yhat-Yhat'|): 0.6396\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3168,  E(|Y-Yhat|): 0.6375,  E(|Yhat-Yhat'|): 0.6414\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3299,  E(|Y-Yhat|): 0.6629,  E(|Yhat-Yhat'|): 0.6660\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9208,  E(|Y-Yhat|): 1.5834,  E(|Yhat-Yhat'|): 1.3253\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8761,  E(|Y-Yhat|): 1.7663,  E(|Yhat-Yhat'|): 1.7803\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8646,  E(|Y-Yhat|): 1.7607,  E(|Yhat-Yhat'|): 1.7923\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8773,  E(|Y-Yhat|): 1.7620,  E(|Yhat-Yhat'|): 1.7695\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0015,  E(|Y-Yhat|): 4.0835,  E(|Yhat-Yhat'|): 4.1640\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1352,  E(|Y-Yhat|): 2.4836,  E(|Yhat-Yhat'|): 0.6968\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8682,  E(|Y-Yhat|): 5.2607,  E(|Yhat-Yhat'|): 6.7850\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 3.3991,  E(|Y-Yhat|): 86.8742,  E(|Yhat-Yhat'|): 166.9502\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7008,  E(|Y-Yhat|): 133.7222,  E(|Yhat-Yhat'|): 266.0429\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.1230,  E(|Y-Yhat|): 153.5605,  E(|Yhat-Yhat'|): 300.8749\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3559,  E(|Y-Yhat|): 0.6246,  E(|Yhat-Yhat'|): 0.5375\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3283,  E(|Y-Yhat|): 0.6621,  E(|Yhat-Yhat'|): 0.6675\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3311,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6730\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3375,  E(|Y-Yhat|): 0.6654,  E(|Yhat-Yhat'|): 0.6558\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3309,  E(|Y-Yhat|): 0.6645,  E(|Yhat-Yhat'|): 0.6672\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7209,  E(|Y-Yhat|): 1.1781,  E(|Yhat-Yhat'|): 0.9143\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6663,  E(|Y-Yhat|): 1.3352,  E(|Yhat-Yhat'|): 1.3377\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6649,  E(|Y-Yhat|): 1.3363,  E(|Yhat-Yhat'|): 1.3429\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6574,  E(|Y-Yhat|): 1.3263,  E(|Yhat-Yhat'|): 1.3379\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6703,  E(|Y-Yhat|): 1.3513,  E(|Yhat-Yhat'|): 1.3619\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3616,  E(|Y-Yhat|): 0.6273,  E(|Yhat-Yhat'|): 0.5312\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3284,  E(|Y-Yhat|): 0.6604,  E(|Yhat-Yhat'|): 0.6642\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3316,  E(|Y-Yhat|): 0.6630,  E(|Yhat-Yhat'|): 0.6628\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3331,  E(|Y-Yhat|): 0.6653,  E(|Yhat-Yhat'|): 0.6643\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3296,  E(|Y-Yhat|): 0.6670,  E(|Yhat-Yhat'|): 0.6747\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9443,  E(|Y-Yhat|): 1.5228,  E(|Yhat-Yhat'|): 1.1569\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8631,  E(|Y-Yhat|): 1.7706,  E(|Yhat-Yhat'|): 1.8150\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8788,  E(|Y-Yhat|): 1.7823,  E(|Yhat-Yhat'|): 1.8071\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8729,  E(|Y-Yhat|): 1.7616,  E(|Yhat-Yhat'|): 1.7773\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9703,  E(|Y-Yhat|): 4.0687,  E(|Yhat-Yhat'|): 4.1969\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9848,  E(|Y-Yhat|): 2.3232,  E(|Yhat-Yhat'|): 0.6768\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8898,  E(|Y-Yhat|): 6.7969,  E(|Yhat-Yhat'|): 9.8141\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9031,  E(|Y-Yhat|): 4.0813,  E(|Yhat-Yhat'|): 4.3564\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8527,  E(|Y-Yhat|): 7.5919,  E(|Yhat-Yhat'|): 11.4784\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9130,  E(|Y-Yhat|): 8.0704,  E(|Yhat-Yhat'|): 12.3147\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3602,  E(|Y-Yhat|): 0.6105,  E(|Yhat-Yhat'|): 0.5005\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3325,  E(|Y-Yhat|): 0.6586,  E(|Yhat-Yhat'|): 0.6522\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3330,  E(|Y-Yhat|): 0.6683,  E(|Yhat-Yhat'|): 0.6706\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3348,  E(|Y-Yhat|): 0.6669,  E(|Yhat-Yhat'|): 0.6642\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6663\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7098,  E(|Y-Yhat|): 1.1869,  E(|Yhat-Yhat'|): 0.9542\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6591,  E(|Y-Yhat|): 1.3315,  E(|Yhat-Yhat'|): 1.3448\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6522,  E(|Y-Yhat|): 1.3232,  E(|Yhat-Yhat'|): 1.3420\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6639,  E(|Y-Yhat|): 1.3250,  E(|Yhat-Yhat'|): 1.3221\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6615,  E(|Y-Yhat|): 1.3315,  E(|Yhat-Yhat'|): 1.3400\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3543,  E(|Y-Yhat|): 0.6408,  E(|Yhat-Yhat'|): 0.5731\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3304,  E(|Y-Yhat|): 0.6596,  E(|Yhat-Yhat'|): 0.6582\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3304,  E(|Y-Yhat|): 0.6594,  E(|Yhat-Yhat'|): 0.6581\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3340,  E(|Y-Yhat|): 0.6647,  E(|Yhat-Yhat'|): 0.6615\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3355,  E(|Y-Yhat|): 0.6671,  E(|Yhat-Yhat'|): 0.6631\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8947,  E(|Y-Yhat|): 1.4776,  E(|Yhat-Yhat'|): 1.1658\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5684,  E(|Y-Yhat|): 1.1574,  E(|Yhat-Yhat'|): 1.1781\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5766,  E(|Y-Yhat|): 1.1573,  E(|Yhat-Yhat'|): 1.1614\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5646,  E(|Y-Yhat|): 1.1542,  E(|Yhat-Yhat'|): 1.1792\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0020,  E(|Y-Yhat|): 4.1103,  E(|Yhat-Yhat'|): 4.2167\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6524,  E(|Y-Yhat|): 2.0142,  E(|Yhat-Yhat'|): 0.7237\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8822,  E(|Y-Yhat|): 6.3720,  E(|Yhat-Yhat'|): 10.9797\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7455,  E(|Y-Yhat|): 11.5372,  E(|Yhat-Yhat'|): 21.5833\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8570,  E(|Y-Yhat|): 6.0087,  E(|Yhat-Yhat'|): 10.3034\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8315,  E(|Y-Yhat|): 18.0646,  E(|Yhat-Yhat'|): 32.4661\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3404,  E(|Y-Yhat|): 0.5868,  E(|Yhat-Yhat'|): 0.4929\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2283,  E(|Y-Yhat|): 0.4564,  E(|Yhat-Yhat'|): 0.4562\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2212,  E(|Y-Yhat|): 0.4461,  E(|Yhat-Yhat'|): 0.4498\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2193,  E(|Y-Yhat|): 0.4437,  E(|Yhat-Yhat'|): 0.4489\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3290,  E(|Y-Yhat|): 0.6652,  E(|Yhat-Yhat'|): 0.6723\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6671,  E(|Y-Yhat|): 1.1151,  E(|Yhat-Yhat'|): 0.8959\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4115,  E(|Y-Yhat|): 0.8395,  E(|Yhat-Yhat'|): 0.8559\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4112,  E(|Y-Yhat|): 0.8203,  E(|Yhat-Yhat'|): 0.8181\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4114,  E(|Y-Yhat|): 0.8231,  E(|Yhat-Yhat'|): 0.8234\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6770,  E(|Y-Yhat|): 1.3551,  E(|Yhat-Yhat'|): 1.3563\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3462,  E(|Y-Yhat|): 0.6160,  E(|Yhat-Yhat'|): 0.5395\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2466,  E(|Y-Yhat|): 0.4954,  E(|Yhat-Yhat'|): 0.4976\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2461,  E(|Y-Yhat|): 0.5000,  E(|Yhat-Yhat'|): 0.5077\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2461,  E(|Y-Yhat|): 0.4950,  E(|Yhat-Yhat'|): 0.4979\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3304,  E(|Y-Yhat|): 0.6677,  E(|Yhat-Yhat'|): 0.6747\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9015,  E(|Y-Yhat|): 1.5129,  E(|Yhat-Yhat'|): 1.2229\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8021,  E(|Y-Yhat|): 1.6120,  E(|Yhat-Yhat'|): 1.6199\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7916,  E(|Y-Yhat|): 1.5977,  E(|Yhat-Yhat'|): 1.6124\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7970,  E(|Y-Yhat|): 1.5847,  E(|Yhat-Yhat'|): 1.5753\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0758,  E(|Y-Yhat|): 4.1034,  E(|Yhat-Yhat'|): 4.0554\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9550,  E(|Y-Yhat|): 2.2959,  E(|Yhat-Yhat'|): 0.6818\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6194,  E(|Y-Yhat|): 9.0726,  E(|Yhat-Yhat'|): 14.9065\n",
      "[Epoch 200 (66%), batch 9] energy-loss: -0.8804,  E(|Y-Yhat|): 210.1244,  E(|Yhat-Yhat'|): 422.0097\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2102,  E(|Y-Yhat|): 714.6430,  E(|Yhat-Yhat'|): 1428.8654\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.5450,  E(|Y-Yhat|): 989.4390,  E(|Yhat-Yhat'|): 1975.7880\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3574,  E(|Y-Yhat|): 0.5964,  E(|Yhat-Yhat'|): 0.4780\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3051,  E(|Y-Yhat|): 0.6186,  E(|Yhat-Yhat'|): 0.6270\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3066,  E(|Y-Yhat|): 0.6161,  E(|Yhat-Yhat'|): 0.6190\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3067,  E(|Y-Yhat|): 0.6197,  E(|Yhat-Yhat'|): 0.6260\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3407,  E(|Y-Yhat|): 0.6829,  E(|Yhat-Yhat'|): 0.6844\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7053,  E(|Y-Yhat|): 1.1455,  E(|Yhat-Yhat'|): 0.8804\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5982,  E(|Y-Yhat|): 1.2065,  E(|Yhat-Yhat'|): 1.2167\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6001,  E(|Y-Yhat|): 1.2137,  E(|Yhat-Yhat'|): 1.2272\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6019,  E(|Y-Yhat|): 1.2061,  E(|Yhat-Yhat'|): 1.2083\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6810,  E(|Y-Yhat|): 1.3644,  E(|Yhat-Yhat'|): 1.3669\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3526,  E(|Y-Yhat|): 0.6243,  E(|Yhat-Yhat'|): 0.5434\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3110,  E(|Y-Yhat|): 0.6287,  E(|Yhat-Yhat'|): 0.6354\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3135,  E(|Y-Yhat|): 0.6274,  E(|Yhat-Yhat'|): 0.6276\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3140,  E(|Y-Yhat|): 0.6274,  E(|Yhat-Yhat'|): 0.6267\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3345,  E(|Y-Yhat|): 0.6707,  E(|Yhat-Yhat'|): 0.6723\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9350,  E(|Y-Yhat|): 1.5504,  E(|Yhat-Yhat'|): 1.2308\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8843,  E(|Y-Yhat|): 1.7726,  E(|Yhat-Yhat'|): 1.7766\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8685,  E(|Y-Yhat|): 1.7710,  E(|Yhat-Yhat'|): 1.8051\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8687,  E(|Y-Yhat|): 1.7623,  E(|Yhat-Yhat'|): 1.7873\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0003,  E(|Y-Yhat|): 4.1430,  E(|Yhat-Yhat'|): 4.2855\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0449,  E(|Y-Yhat|): 2.3292,  E(|Yhat-Yhat'|): 0.5686\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9456,  E(|Y-Yhat|): 8.2409,  E(|Yhat-Yhat'|): 12.5905\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0180,  E(|Y-Yhat|): 9.9908,  E(|Yhat-Yhat'|): 15.9456\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9532,  E(|Y-Yhat|): 6.0261,  E(|Yhat-Yhat'|): 8.1458\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0068,  E(|Y-Yhat|): 6.1827,  E(|Yhat-Yhat'|): 8.3519\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3663,  E(|Y-Yhat|): 0.6291,  E(|Yhat-Yhat'|): 0.5257\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3282,  E(|Y-Yhat|): 0.6667,  E(|Yhat-Yhat'|): 0.6772\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3312,  E(|Y-Yhat|): 0.6647,  E(|Yhat-Yhat'|): 0.6669\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3315,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6700\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3325,  E(|Y-Yhat|): 0.6668,  E(|Yhat-Yhat'|): 0.6685\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7058,  E(|Y-Yhat|): 1.1719,  E(|Yhat-Yhat'|): 0.9322\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6576,  E(|Y-Yhat|): 1.3310,  E(|Yhat-Yhat'|): 1.3469\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6614,  E(|Y-Yhat|): 1.3303,  E(|Yhat-Yhat'|): 1.3378\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6643,  E(|Y-Yhat|): 1.3326,  E(|Yhat-Yhat'|): 1.3367\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6664,  E(|Y-Yhat|): 1.3494,  E(|Yhat-Yhat'|): 1.3660\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3504,  E(|Y-Yhat|): 0.6399,  E(|Yhat-Yhat'|): 0.5789\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3281,  E(|Y-Yhat|): 0.6599,  E(|Yhat-Yhat'|): 0.6635\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3318,  E(|Y-Yhat|): 0.6657,  E(|Yhat-Yhat'|): 0.6677\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3297,  E(|Y-Yhat|): 0.6620,  E(|Yhat-Yhat'|): 0.6646\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3361,  E(|Y-Yhat|): 0.6697,  E(|Yhat-Yhat'|): 0.6672\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8144,  E(|Y-Yhat|): 1.4655,  E(|Yhat-Yhat'|): 1.3023\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6545,  E(|Y-Yhat|): 1.3236,  E(|Yhat-Yhat'|): 1.3380\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6370,  E(|Y-Yhat|): 1.2962,  E(|Yhat-Yhat'|): 1.3184\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6462,  E(|Y-Yhat|): 1.3116,  E(|Yhat-Yhat'|): 1.3307\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9745,  E(|Y-Yhat|): 4.0825,  E(|Yhat-Yhat'|): 4.2159\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7225,  E(|Y-Yhat|): 2.0574,  E(|Yhat-Yhat'|): 0.6700\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1548,  E(|Y-Yhat|): 132.1828,  E(|Yhat-Yhat'|): 264.0560\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4661,  E(|Y-Yhat|): 34.7956,  E(|Yhat-Yhat'|): 68.6589\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.9737,  E(|Y-Yhat|): 235.2915,  E(|Yhat-Yhat'|): 464.6356\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 4.5203,  E(|Y-Yhat|): 381.9579,  E(|Yhat-Yhat'|): 754.8752\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3355,  E(|Y-Yhat|): 0.5970,  E(|Yhat-Yhat'|): 0.5229\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2559,  E(|Y-Yhat|): 0.5214,  E(|Yhat-Yhat'|): 0.5310\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2589,  E(|Y-Yhat|): 0.5198,  E(|Yhat-Yhat'|): 0.5218\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2620,  E(|Y-Yhat|): 0.5219,  E(|Yhat-Yhat'|): 0.5198\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6647,  E(|Yhat-Yhat'|): 0.6629\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6968,  E(|Y-Yhat|): 1.1483,  E(|Yhat-Yhat'|): 0.9031\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4762,  E(|Y-Yhat|): 0.9693,  E(|Yhat-Yhat'|): 0.9862\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4888,  E(|Y-Yhat|): 0.9708,  E(|Yhat-Yhat'|): 0.9641\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4826,  E(|Y-Yhat|): 0.9694,  E(|Yhat-Yhat'|): 0.9737\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6678,  E(|Y-Yhat|): 1.3448,  E(|Yhat-Yhat'|): 1.3541\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3448,  E(|Y-Yhat|): 0.6182,  E(|Yhat-Yhat'|): 0.5470\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2784,  E(|Y-Yhat|): 0.5571,  E(|Yhat-Yhat'|): 0.5574\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2784,  E(|Y-Yhat|): 0.5546,  E(|Yhat-Yhat'|): 0.5524\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2758,  E(|Y-Yhat|): 0.5544,  E(|Yhat-Yhat'|): 0.5571\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3312,  E(|Y-Yhat|): 0.6639,  E(|Yhat-Yhat'|): 0.6655\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9418,  E(|Y-Yhat|): 1.5340,  E(|Yhat-Yhat'|): 1.1845\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8783,  E(|Y-Yhat|): 1.7580,  E(|Yhat-Yhat'|): 1.7595\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8672,  E(|Y-Yhat|): 1.7593,  E(|Yhat-Yhat'|): 1.7842\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8627,  E(|Y-Yhat|): 1.7333,  E(|Yhat-Yhat'|): 1.7412\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0390,  E(|Y-Yhat|): 4.1027,  E(|Yhat-Yhat'|): 4.1273\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0635,  E(|Y-Yhat|): 2.4245,  E(|Yhat-Yhat'|): 0.7221\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.0458,  E(|Y-Yhat|): 17.1788,  E(|Yhat-Yhat'|): 30.2661\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4421,  E(|Y-Yhat|): 36.5711,  E(|Yhat-Yhat'|): 70.2579\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.6731,  E(|Y-Yhat|): 50.9617,  E(|Yhat-Yhat'|): 98.5772\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.5874,  E(|Y-Yhat|): 54.8275,  E(|Yhat-Yhat'|): 106.4802\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3693,  E(|Y-Yhat|): 0.6156,  E(|Yhat-Yhat'|): 0.4926\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3273,  E(|Y-Yhat|): 0.6675,  E(|Yhat-Yhat'|): 0.6803\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3314,  E(|Y-Yhat|): 0.6650,  E(|Yhat-Yhat'|): 0.6672\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3317,  E(|Y-Yhat|): 0.6634,  E(|Yhat-Yhat'|): 0.6634\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3294,  E(|Y-Yhat|): 0.6615,  E(|Yhat-Yhat'|): 0.6641\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7125,  E(|Y-Yhat|): 1.1941,  E(|Yhat-Yhat'|): 0.9632\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6540,  E(|Y-Yhat|): 1.3342,  E(|Yhat-Yhat'|): 1.3604\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6578,  E(|Y-Yhat|): 1.3333,  E(|Yhat-Yhat'|): 1.3510\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6627,  E(|Y-Yhat|): 1.3319,  E(|Yhat-Yhat'|): 1.3384\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6705,  E(|Y-Yhat|): 1.3410,  E(|Yhat-Yhat'|): 1.3409\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3549,  E(|Y-Yhat|): 0.6338,  E(|Yhat-Yhat'|): 0.5578\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3312,  E(|Y-Yhat|): 0.6642,  E(|Yhat-Yhat'|): 0.6661\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3319,  E(|Y-Yhat|): 0.6600,  E(|Yhat-Yhat'|): 0.6563\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3298,  E(|Y-Yhat|): 0.6591,  E(|Yhat-Yhat'|): 0.6587\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3285,  E(|Y-Yhat|): 0.6606,  E(|Yhat-Yhat'|): 0.6642\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8834,  E(|Y-Yhat|): 1.4985,  E(|Yhat-Yhat'|): 1.2302\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6287,  E(|Y-Yhat|): 1.2787,  E(|Yhat-Yhat'|): 1.2998\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6245,  E(|Y-Yhat|): 1.2650,  E(|Yhat-Yhat'|): 1.2810\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6207,  E(|Y-Yhat|): 1.2703,  E(|Yhat-Yhat'|): 1.2992\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0337,  E(|Y-Yhat|): 4.2321,  E(|Yhat-Yhat'|): 4.3967\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7889,  E(|Y-Yhat|): 2.0836,  E(|Yhat-Yhat'|): 0.5894\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1561,  E(|Y-Yhat|): 15.7910,  E(|Yhat-Yhat'|): 29.2697\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.2113,  E(|Y-Yhat|): 14.7116,  E(|Yhat-Yhat'|): 27.0006\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9988,  E(|Y-Yhat|): 13.7415,  E(|Yhat-Yhat'|): 25.4855\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.1546,  E(|Y-Yhat|): 42.3811,  E(|Yhat-Yhat'|): 80.4530\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3568,  E(|Y-Yhat|): 0.6057,  E(|Yhat-Yhat'|): 0.4980\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2460,  E(|Y-Yhat|): 0.5014,  E(|Yhat-Yhat'|): 0.5109\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2432,  E(|Y-Yhat|): 0.4911,  E(|Yhat-Yhat'|): 0.4958\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2444,  E(|Y-Yhat|): 0.4943,  E(|Yhat-Yhat'|): 0.4997\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3315,  E(|Y-Yhat|): 0.6641,  E(|Yhat-Yhat'|): 0.6651\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6793,  E(|Y-Yhat|): 1.1367,  E(|Yhat-Yhat'|): 0.9148\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4577,  E(|Y-Yhat|): 0.9054,  E(|Yhat-Yhat'|): 0.8955\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4659,  E(|Y-Yhat|): 0.9307,  E(|Yhat-Yhat'|): 0.9296\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4552,  E(|Y-Yhat|): 0.9157,  E(|Yhat-Yhat'|): 0.9210\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6773,  E(|Y-Yhat|): 1.3653,  E(|Yhat-Yhat'|): 1.3760\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3425,  E(|Y-Yhat|): 0.6209,  E(|Yhat-Yhat'|): 0.5568\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2660,  E(|Y-Yhat|): 0.5322,  E(|Yhat-Yhat'|): 0.5325\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2648,  E(|Y-Yhat|): 0.5334,  E(|Yhat-Yhat'|): 0.5372\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2622,  E(|Y-Yhat|): 0.5293,  E(|Yhat-Yhat'|): 0.5341\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3309,  E(|Y-Yhat|): 0.6636,  E(|Yhat-Yhat'|): 0.6656\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0040),\n",
       " tensor(0.4596),\n",
       " tensor(0.0038),\n",
       " tensor(0.0037),\n",
       " tensor(0.0057))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"softplus\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 50)\n",
    "    x2 = torch.linspace(0, 5, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = F.softplus(Z)   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7d94d",
   "metadata": {},
   "source": [
    "## True function: cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f13cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7107,  E(|Y-Yhat|): 1.2238,  E(|Yhat-Yhat'|): 1.0264\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3219,  E(|Y-Yhat|): 0.6590,  E(|Yhat-Yhat'|): 0.6742\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3154,  E(|Y-Yhat|): 0.6517,  E(|Yhat-Yhat'|): 0.6726\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3224,  E(|Y-Yhat|): 0.6476,  E(|Yhat-Yhat'|): 0.6503\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0395,  E(|Y-Yhat|): 4.3613,  E(|Yhat-Yhat'|): 4.6435\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5244,  E(|Y-Yhat|): 1.9670,  E(|Yhat-Yhat'|): 0.8853\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 5.8201,  E(|Y-Yhat|): 212.1835,  E(|Yhat-Yhat'|): 412.7267\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8651,  E(|Y-Yhat|): 66.3380,  E(|Yhat-Yhat'|): 130.9458\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -0.1054,  E(|Y-Yhat|): 43.8119,  E(|Yhat-Yhat'|): 87.8346\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -1.5962,  E(|Y-Yhat|): 434.4306,  E(|Yhat-Yhat'|): 872.0536\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2677,  E(|Y-Yhat|): 0.4587,  E(|Yhat-Yhat'|): 0.3821\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1040,  E(|Y-Yhat|): 0.2110,  E(|Yhat-Yhat'|): 0.2141\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0999,  E(|Y-Yhat|): 0.2071,  E(|Yhat-Yhat'|): 0.2145\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1026,  E(|Y-Yhat|): 0.2074,  E(|Yhat-Yhat'|): 0.2095\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3353,  E(|Y-Yhat|): 0.6710,  E(|Yhat-Yhat'|): 0.6714\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5449,  E(|Y-Yhat|): 0.8953,  E(|Yhat-Yhat'|): 0.7007\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1846,  E(|Y-Yhat|): 0.3824,  E(|Yhat-Yhat'|): 0.3956\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1809,  E(|Y-Yhat|): 0.3843,  E(|Yhat-Yhat'|): 0.4068\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1880,  E(|Y-Yhat|): 0.3701,  E(|Yhat-Yhat'|): 0.3642\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6790,  E(|Y-Yhat|): 1.3737,  E(|Yhat-Yhat'|): 1.3894\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2696,  E(|Y-Yhat|): 0.4994,  E(|Yhat-Yhat'|): 0.4597\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1408,  E(|Y-Yhat|): 0.2929,  E(|Yhat-Yhat'|): 0.3042\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1401,  E(|Y-Yhat|): 0.2847,  E(|Yhat-Yhat'|): 0.2892\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1375,  E(|Y-Yhat|): 0.2811,  E(|Yhat-Yhat'|): 0.2873\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3357,  E(|Y-Yhat|): 0.6739,  E(|Yhat-Yhat'|): 0.6764\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9340,  E(|Y-Yhat|): 1.5739,  E(|Yhat-Yhat'|): 1.2797\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8917,  E(|Y-Yhat|): 1.7835,  E(|Yhat-Yhat'|): 1.7835\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8848,  E(|Y-Yhat|): 1.7716,  E(|Yhat-Yhat'|): 1.7736\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8884,  E(|Y-Yhat|): 1.7742,  E(|Yhat-Yhat'|): 1.7716\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0024,  E(|Y-Yhat|): 4.0714,  E(|Yhat-Yhat'|): 4.1380\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0653,  E(|Y-Yhat|): 2.3502,  E(|Yhat-Yhat'|): 0.5697\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.2014,  E(|Y-Yhat|): 24.6747,  E(|Yhat-Yhat'|): 44.9466\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0287,  E(|Y-Yhat|): 8.6147,  E(|Yhat-Yhat'|): 13.1721\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 7.8788,  E(|Y-Yhat|): 404.3244,  E(|Yhat-Yhat'|): 792.8912\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.4356,  E(|Y-Yhat|): 383.0364,  E(|Yhat-Yhat'|): 759.2016\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3578,  E(|Y-Yhat|): 0.6317,  E(|Yhat-Yhat'|): 0.5478\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3338,  E(|Y-Yhat|): 0.6704,  E(|Yhat-Yhat'|): 0.6731\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3317,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6718\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3355,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6643\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3356,  E(|Y-Yhat|): 0.6716,  E(|Yhat-Yhat'|): 0.6719\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7263,  E(|Y-Yhat|): 1.1715,  E(|Yhat-Yhat'|): 0.8903\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6674,  E(|Y-Yhat|): 1.3448,  E(|Yhat-Yhat'|): 1.3548\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6575,  E(|Y-Yhat|): 1.3408,  E(|Yhat-Yhat'|): 1.3665\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6640,  E(|Y-Yhat|): 1.3316,  E(|Yhat-Yhat'|): 1.3353\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6747,  E(|Y-Yhat|): 1.3566,  E(|Yhat-Yhat'|): 1.3638\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3584,  E(|Y-Yhat|): 0.6342,  E(|Yhat-Yhat'|): 0.5516\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3289,  E(|Y-Yhat|): 0.6650,  E(|Yhat-Yhat'|): 0.6723\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3313,  E(|Y-Yhat|): 0.6687,  E(|Yhat-Yhat'|): 0.6746\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3324,  E(|Y-Yhat|): 0.6664,  E(|Yhat-Yhat'|): 0.6681\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3334,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6684\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9025,  E(|Y-Yhat|): 1.5669,  E(|Yhat-Yhat'|): 1.3288\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8562,  E(|Y-Yhat|): 1.7408,  E(|Yhat-Yhat'|): 1.7692\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8454,  E(|Y-Yhat|): 1.7021,  E(|Yhat-Yhat'|): 1.7134\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8443,  E(|Y-Yhat|): 1.6886,  E(|Yhat-Yhat'|): 1.6886\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9776,  E(|Y-Yhat|): 4.0585,  E(|Yhat-Yhat'|): 4.1620\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1095,  E(|Y-Yhat|): 2.4573,  E(|Yhat-Yhat'|): 0.6956\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.7515,  E(|Y-Yhat|): 7.4024,  E(|Yhat-Yhat'|): 11.3017\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 3.9179,  E(|Y-Yhat|): 228.3204,  E(|Yhat-Yhat'|): 448.8051\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8584,  E(|Y-Yhat|): 63.5356,  E(|Yhat-Yhat'|): 123.3543\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.9563,  E(|Y-Yhat|): 73.0813,  E(|Yhat-Yhat'|): 140.2498\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3541,  E(|Y-Yhat|): 0.6238,  E(|Yhat-Yhat'|): 0.5393\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3258,  E(|Y-Yhat|): 0.6513,  E(|Yhat-Yhat'|): 0.6511\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3277,  E(|Y-Yhat|): 0.6575,  E(|Yhat-Yhat'|): 0.6597\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3261,  E(|Y-Yhat|): 0.6560,  E(|Yhat-Yhat'|): 0.6597\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6698,  E(|Yhat-Yhat'|): 0.6731\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7143,  E(|Y-Yhat|): 1.1689,  E(|Yhat-Yhat'|): 0.9092\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6449,  E(|Y-Yhat|): 1.2952,  E(|Yhat-Yhat'|): 1.3006\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6425,  E(|Y-Yhat|): 1.2984,  E(|Yhat-Yhat'|): 1.3118\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6443,  E(|Y-Yhat|): 1.2895,  E(|Yhat-Yhat'|): 1.2903\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6829,  E(|Y-Yhat|): 1.3643,  E(|Yhat-Yhat'|): 1.3628\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3597,  E(|Y-Yhat|): 0.6261,  E(|Yhat-Yhat'|): 0.5329\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3257,  E(|Y-Yhat|): 0.6527,  E(|Yhat-Yhat'|): 0.6539\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3263,  E(|Y-Yhat|): 0.6583,  E(|Yhat-Yhat'|): 0.6641\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3245,  E(|Y-Yhat|): 0.6519,  E(|Yhat-Yhat'|): 0.6549\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3320,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6711\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9511,  E(|Y-Yhat|): 1.5323,  E(|Yhat-Yhat'|): 1.1625\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8765,  E(|Y-Yhat|): 1.7865,  E(|Yhat-Yhat'|): 1.8201\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8885,  E(|Y-Yhat|): 1.7920,  E(|Yhat-Yhat'|): 1.8071\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8740,  E(|Y-Yhat|): 1.7664,  E(|Yhat-Yhat'|): 1.7847\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9988,  E(|Y-Yhat|): 4.0391,  E(|Yhat-Yhat'|): 4.0805\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0204,  E(|Y-Yhat|): 2.3583,  E(|Yhat-Yhat'|): 0.6757\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9404,  E(|Y-Yhat|): 4.4486,  E(|Yhat-Yhat'|): 5.0163\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.0003,  E(|Y-Yhat|): 2.6945,  E(|Yhat-Yhat'|): 1.3883\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9731,  E(|Y-Yhat|): 4.3142,  E(|Yhat-Yhat'|): 4.6823\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9375,  E(|Y-Yhat|): 4.3396,  E(|Yhat-Yhat'|): 4.8041\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3621,  E(|Y-Yhat|): 0.6139,  E(|Yhat-Yhat'|): 0.5037\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3334,  E(|Y-Yhat|): 0.6687,  E(|Yhat-Yhat'|): 0.6706\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3336,  E(|Y-Yhat|): 0.6650,  E(|Yhat-Yhat'|): 0.6629\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3308,  E(|Y-Yhat|): 0.6689,  E(|Yhat-Yhat'|): 0.6763\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3345,  E(|Y-Yhat|): 0.6736,  E(|Yhat-Yhat'|): 0.6782\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7162,  E(|Y-Yhat|): 1.1935,  E(|Yhat-Yhat'|): 0.9546\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6763,  E(|Y-Yhat|): 1.3472,  E(|Yhat-Yhat'|): 1.3417\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6764,  E(|Y-Yhat|): 1.3487,  E(|Yhat-Yhat'|): 1.3445\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6675,  E(|Y-Yhat|): 1.3425,  E(|Yhat-Yhat'|): 1.3500\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6674,  E(|Y-Yhat|): 1.3357,  E(|Yhat-Yhat'|): 1.3365\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3549,  E(|Y-Yhat|): 0.6431,  E(|Yhat-Yhat'|): 0.5764\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3316,  E(|Y-Yhat|): 0.6677,  E(|Yhat-Yhat'|): 0.6721\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3323,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6652\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3332,  E(|Y-Yhat|): 0.6655,  E(|Yhat-Yhat'|): 0.6646\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3292,  E(|Y-Yhat|): 0.6615,  E(|Yhat-Yhat'|): 0.6645\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7851,  E(|Y-Yhat|): 1.2751,  E(|Yhat-Yhat'|): 0.9799\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2609,  E(|Y-Yhat|): 0.5400,  E(|Yhat-Yhat'|): 0.5583\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2619,  E(|Y-Yhat|): 0.5313,  E(|Yhat-Yhat'|): 0.5387\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2524,  E(|Y-Yhat|): 0.5241,  E(|Yhat-Yhat'|): 0.5433\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0471,  E(|Y-Yhat|): 4.7426,  E(|Yhat-Yhat'|): 5.3910\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.5714,  E(|Y-Yhat|): 1.9282,  E(|Yhat-Yhat'|): 0.7135\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5911,  E(|Y-Yhat|): 10.0685,  E(|Yhat-Yhat'|): 18.9548\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5594,  E(|Y-Yhat|): 326.8643,  E(|Yhat-Yhat'|): 652.6098\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.4155,  E(|Y-Yhat|): 127.8709,  E(|Yhat-Yhat'|): 250.9109\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 6.5710,  E(|Y-Yhat|): 2223.7888,  E(|Yhat-Yhat'|): 4434.4355\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2588,  E(|Y-Yhat|): 0.4657,  E(|Yhat-Yhat'|): 0.4139\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0763,  E(|Y-Yhat|): 0.1574,  E(|Yhat-Yhat'|): 0.1623\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0711,  E(|Y-Yhat|): 0.1502,  E(|Yhat-Yhat'|): 0.1583\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0753,  E(|Y-Yhat|): 0.1491,  E(|Yhat-Yhat'|): 0.1475\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3444,  E(|Y-Yhat|): 0.6755,  E(|Yhat-Yhat'|): 0.6623\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5645,  E(|Y-Yhat|): 0.9427,  E(|Yhat-Yhat'|): 0.7565\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1364,  E(|Y-Yhat|): 0.2960,  E(|Yhat-Yhat'|): 0.3190\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1357,  E(|Y-Yhat|): 0.2888,  E(|Yhat-Yhat'|): 0.3063\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1294,  E(|Y-Yhat|): 0.2782,  E(|Yhat-Yhat'|): 0.2976\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6937,  E(|Y-Yhat|): 1.4474,  E(|Yhat-Yhat'|): 1.5074\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2921,  E(|Y-Yhat|): 0.5249,  E(|Yhat-Yhat'|): 0.4655\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.1069,  E(|Y-Yhat|): 0.2233,  E(|Yhat-Yhat'|): 0.2327\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1060,  E(|Y-Yhat|): 0.2157,  E(|Yhat-Yhat'|): 0.2196\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1056,  E(|Y-Yhat|): 0.2150,  E(|Yhat-Yhat'|): 0.2189\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3354,  E(|Y-Yhat|): 0.6761,  E(|Yhat-Yhat'|): 0.6814\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9215,  E(|Y-Yhat|): 1.5328,  E(|Yhat-Yhat'|): 1.2224\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8403,  E(|Y-Yhat|): 1.6938,  E(|Yhat-Yhat'|): 1.7068\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8289,  E(|Y-Yhat|): 1.6723,  E(|Yhat-Yhat'|): 1.6868\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8348,  E(|Y-Yhat|): 1.6991,  E(|Yhat-Yhat'|): 1.7285\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0406,  E(|Y-Yhat|): 4.1938,  E(|Yhat-Yhat'|): 4.3066\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0075,  E(|Y-Yhat|): 2.3616,  E(|Yhat-Yhat'|): 0.7084\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9553,  E(|Y-Yhat|): 9.8875,  E(|Yhat-Yhat'|): 15.8644\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.6997,  E(|Y-Yhat|): 11.9587,  E(|Yhat-Yhat'|): 20.5181\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8121,  E(|Y-Yhat|): 15.6940,  E(|Yhat-Yhat'|): 27.7638\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.1829,  E(|Y-Yhat|): 18.3841,  E(|Yhat-Yhat'|): 32.4024\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3619,  E(|Y-Yhat|): 0.6023,  E(|Yhat-Yhat'|): 0.4808\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3252,  E(|Y-Yhat|): 0.6483,  E(|Yhat-Yhat'|): 0.6461\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3210,  E(|Y-Yhat|): 0.6425,  E(|Yhat-Yhat'|): 0.6429\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3209,  E(|Y-Yhat|): 0.6410,  E(|Yhat-Yhat'|): 0.6401\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3373,  E(|Y-Yhat|): 0.6778,  E(|Yhat-Yhat'|): 0.6811\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7138,  E(|Y-Yhat|): 1.1533,  E(|Yhat-Yhat'|): 0.8790\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6426,  E(|Y-Yhat|): 1.2858,  E(|Yhat-Yhat'|): 1.2863\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6313,  E(|Y-Yhat|): 1.2777,  E(|Yhat-Yhat'|): 1.2928\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6228,  E(|Y-Yhat|): 1.2664,  E(|Yhat-Yhat'|): 1.2872\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6823,  E(|Y-Yhat|): 1.3784,  E(|Yhat-Yhat'|): 1.3922\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3569,  E(|Y-Yhat|): 0.6271,  E(|Yhat-Yhat'|): 0.5404\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3221,  E(|Y-Yhat|): 0.6484,  E(|Yhat-Yhat'|): 0.6526\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3244,  E(|Y-Yhat|): 0.6500,  E(|Yhat-Yhat'|): 0.6513\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3204,  E(|Y-Yhat|): 0.6471,  E(|Yhat-Yhat'|): 0.6532\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3361,  E(|Y-Yhat|): 0.6723,  E(|Yhat-Yhat'|): 0.6724\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6865,  E(|Y-Yhat|): 1.0999,  E(|Yhat-Yhat'|): 0.8268\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2005,  E(|Y-Yhat|): 0.4226,  E(|Yhat-Yhat'|): 0.4442\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2028,  E(|Y-Yhat|): 0.4242,  E(|Yhat-Yhat'|): 0.4426\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2065,  E(|Y-Yhat|): 0.4204,  E(|Yhat-Yhat'|): 0.4278\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0345,  E(|Y-Yhat|): 4.8550,  E(|Yhat-Yhat'|): 5.6410\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7768,  E(|Y-Yhat|): 2.1190,  E(|Yhat-Yhat'|): 0.6842\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5028,  E(|Y-Yhat|): 6.7989,  E(|Yhat-Yhat'|): 12.5922\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0218,  E(|Y-Yhat|): 3.3130,  E(|Yhat-Yhat'|): 6.5824\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0183,  E(|Y-Yhat|): 4.2928,  E(|Yhat-Yhat'|): 8.5491\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.5409,  E(|Y-Yhat|): 101.5504,  E(|Yhat-Yhat'|): 198.0190\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2114,  E(|Y-Yhat|): 0.3907,  E(|Yhat-Yhat'|): 0.3586\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0503,  E(|Y-Yhat|): 0.1075,  E(|Yhat-Yhat'|): 0.1144\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0482,  E(|Y-Yhat|): 0.1001,  E(|Yhat-Yhat'|): 0.1037\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0464,  E(|Y-Yhat|): 0.0986,  E(|Yhat-Yhat'|): 0.1042\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3511,  E(|Y-Yhat|): 0.6797,  E(|Yhat-Yhat'|): 0.6571\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.4664,  E(|Y-Yhat|): 0.7884,  E(|Yhat-Yhat'|): 0.6441\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0954,  E(|Y-Yhat|): 0.1949,  E(|Yhat-Yhat'|): 0.1992\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0930,  E(|Y-Yhat|): 0.1920,  E(|Yhat-Yhat'|): 0.1980\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0879,  E(|Y-Yhat|): 0.1888,  E(|Yhat-Yhat'|): 0.2018\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6869,  E(|Y-Yhat|): 1.4257,  E(|Yhat-Yhat'|): 1.4776\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2347,  E(|Y-Yhat|): 0.4447,  E(|Yhat-Yhat'|): 0.4199\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.0778,  E(|Y-Yhat|): 0.1633,  E(|Yhat-Yhat'|): 0.1711\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.0762,  E(|Y-Yhat|): 0.1560,  E(|Yhat-Yhat'|): 0.1597\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.0765,  E(|Y-Yhat|): 0.1592,  E(|Yhat-Yhat'|): 0.1653\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3371,  E(|Y-Yhat|): 0.6785,  E(|Yhat-Yhat'|): 0.6828\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7614,  E(|Y-Yhat|): 1.3538,  E(|Yhat-Yhat'|): 1.1848\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5409,  E(|Y-Yhat|): 1.0895,  E(|Yhat-Yhat'|): 1.0972\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5392,  E(|Y-Yhat|): 1.1043,  E(|Yhat-Yhat'|): 1.1303\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5352,  E(|Y-Yhat|): 1.0869,  E(|Yhat-Yhat'|): 1.1033\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0188,  E(|Y-Yhat|): 4.1674,  E(|Yhat-Yhat'|): 4.2971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.6686,  E(|Y-Yhat|): 2.0082,  E(|Yhat-Yhat'|): 0.6793\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8788,  E(|Y-Yhat|): 4.3277,  E(|Yhat-Yhat'|): 6.8978\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7668,  E(|Y-Yhat|): 5.7643,  E(|Yhat-Yhat'|): 9.9951\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9209,  E(|Y-Yhat|): 40.5002,  E(|Yhat-Yhat'|): 79.1586\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.2416,  E(|Y-Yhat|): 117.3800,  E(|Yhat-Yhat'|): 228.2768\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2900,  E(|Y-Yhat|): 0.5259,  E(|Yhat-Yhat'|): 0.4718\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2147,  E(|Y-Yhat|): 0.4358,  E(|Yhat-Yhat'|): 0.4421\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2150,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.4345\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2107,  E(|Y-Yhat|): 0.4229,  E(|Yhat-Yhat'|): 0.4243\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3352,  E(|Y-Yhat|): 0.6661,  E(|Yhat-Yhat'|): 0.6618\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6175,  E(|Y-Yhat|): 1.0291,  E(|Yhat-Yhat'|): 0.8232\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3834,  E(|Y-Yhat|): 0.7797,  E(|Yhat-Yhat'|): 0.7926\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3843,  E(|Y-Yhat|): 0.7689,  E(|Yhat-Yhat'|): 0.7690\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3828,  E(|Y-Yhat|): 0.7827,  E(|Yhat-Yhat'|): 0.8000\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6681,  E(|Y-Yhat|): 1.3606,  E(|Yhat-Yhat'|): 1.3850\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3109,  E(|Y-Yhat|): 0.5643,  E(|Yhat-Yhat'|): 0.5068\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2412,  E(|Y-Yhat|): 0.4902,  E(|Yhat-Yhat'|): 0.4980\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2387,  E(|Y-Yhat|): 0.4857,  E(|Yhat-Yhat'|): 0.4940\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2378,  E(|Y-Yhat|): 0.4817,  E(|Yhat-Yhat'|): 0.4878\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3315,  E(|Y-Yhat|): 0.6670,  E(|Yhat-Yhat'|): 0.6709\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9443,  E(|Y-Yhat|): 1.5333,  E(|Yhat-Yhat'|): 1.1781\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8695,  E(|Y-Yhat|): 1.7297,  E(|Yhat-Yhat'|): 1.7204\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8669,  E(|Y-Yhat|): 1.7444,  E(|Yhat-Yhat'|): 1.7550\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8480,  E(|Y-Yhat|): 1.7124,  E(|Yhat-Yhat'|): 1.7288\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0094,  E(|Y-Yhat|): 4.1622,  E(|Yhat-Yhat'|): 4.3056\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0280,  E(|Y-Yhat|): 2.3938,  E(|Yhat-Yhat'|): 0.7317\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8580,  E(|Y-Yhat|): 9.8758,  E(|Yhat-Yhat'|): 16.0357\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4305,  E(|Y-Yhat|): 34.1395,  E(|Yhat-Yhat'|): 65.4181\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.7942,  E(|Y-Yhat|): 9.3184,  E(|Yhat-Yhat'|): 15.0485\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8392,  E(|Y-Yhat|): 10.9514,  E(|Yhat-Yhat'|): 18.2245\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3673,  E(|Y-Yhat|): 0.6133,  E(|Yhat-Yhat'|): 0.4920\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3279,  E(|Y-Yhat|): 0.6589,  E(|Yhat-Yhat'|): 0.6620\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3264,  E(|Y-Yhat|): 0.6570,  E(|Yhat-Yhat'|): 0.6611\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3275,  E(|Y-Yhat|): 0.6571,  E(|Yhat-Yhat'|): 0.6591\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3359,  E(|Y-Yhat|): 0.6699,  E(|Yhat-Yhat'|): 0.6680\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7038,  E(|Y-Yhat|): 1.1864,  E(|Yhat-Yhat'|): 0.9651\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6542,  E(|Y-Yhat|): 1.3124,  E(|Yhat-Yhat'|): 1.3164\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6580,  E(|Y-Yhat|): 1.3125,  E(|Yhat-Yhat'|): 1.3090\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6423,  E(|Y-Yhat|): 1.2999,  E(|Yhat-Yhat'|): 1.3152\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6695,  E(|Y-Yhat|): 1.3318,  E(|Yhat-Yhat'|): 1.3246\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3545,  E(|Y-Yhat|): 0.6340,  E(|Yhat-Yhat'|): 0.5590\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3318,  E(|Y-Yhat|): 0.6645,  E(|Yhat-Yhat'|): 0.6654\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3266,  E(|Y-Yhat|): 0.6589,  E(|Yhat-Yhat'|): 0.6646\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3267,  E(|Y-Yhat|): 0.6565,  E(|Yhat-Yhat'|): 0.6595\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3339,  E(|Y-Yhat|): 0.6684,  E(|Yhat-Yhat'|): 0.6690\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8560,  E(|Y-Yhat|): 1.4411,  E(|Yhat-Yhat'|): 1.1702\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5655,  E(|Y-Yhat|): 1.1441,  E(|Yhat-Yhat'|): 1.1573\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5593,  E(|Y-Yhat|): 1.1435,  E(|Yhat-Yhat'|): 1.1683\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5590,  E(|Y-Yhat|): 1.1207,  E(|Yhat-Yhat'|): 1.1233\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0026,  E(|Y-Yhat|): 4.2214,  E(|Yhat-Yhat'|): 4.4378\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7984,  E(|Y-Yhat|): 2.0892,  E(|Yhat-Yhat'|): 0.5815\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.9739,  E(|Y-Yhat|): 5.1341,  E(|Yhat-Yhat'|): 8.3205\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.5351,  E(|Y-Yhat|): 297.3994,  E(|Yhat-Yhat'|): 589.7286\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7667,  E(|Y-Yhat|): 27.4153,  E(|Yhat-Yhat'|): 53.2972\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0616,  E(|Y-Yhat|): 100.7198,  E(|Yhat-Yhat'|): 197.3163\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3372,  E(|Y-Yhat|): 0.5746,  E(|Yhat-Yhat'|): 0.4748\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2202,  E(|Y-Yhat|): 0.4414,  E(|Yhat-Yhat'|): 0.4425\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2181,  E(|Y-Yhat|): 0.4432,  E(|Yhat-Yhat'|): 0.4504\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2193,  E(|Y-Yhat|): 0.4386,  E(|Yhat-Yhat'|): 0.4386\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3367,  E(|Y-Yhat|): 0.6748,  E(|Yhat-Yhat'|): 0.6761\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6495,  E(|Y-Yhat|): 1.0814,  E(|Yhat-Yhat'|): 0.8637\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3982,  E(|Y-Yhat|): 0.8105,  E(|Yhat-Yhat'|): 0.8246\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3999,  E(|Y-Yhat|): 0.8122,  E(|Yhat-Yhat'|): 0.8247\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4025,  E(|Y-Yhat|): 0.8192,  E(|Yhat-Yhat'|): 0.8333\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6846,  E(|Y-Yhat|): 1.3714,  E(|Yhat-Yhat'|): 1.3736\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3289,  E(|Y-Yhat|): 0.5903,  E(|Yhat-Yhat'|): 0.5227\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2440,  E(|Y-Yhat|): 0.4861,  E(|Yhat-Yhat'|): 0.4842\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2413,  E(|Y-Yhat|): 0.4859,  E(|Yhat-Yhat'|): 0.4892\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2399,  E(|Y-Yhat|): 0.4839,  E(|Yhat-Yhat'|): 0.4880\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3339,  E(|Y-Yhat|): 0.6701,  E(|Yhat-Yhat'|): 0.6725\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0173),\n",
       " tensor(2.2250),\n",
       " tensor(0.0196),\n",
       " tensor(0.0147),\n",
       " tensor(0.0160))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"cubic\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = Z ** 3 / 3.0   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e3ea1",
   "metadata": {},
   "source": [
    "## True function: square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "848f7639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8562,  E(|Y-Yhat|): 1.4788,  E(|Yhat-Yhat'|): 1.2453\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6183,  E(|Y-Yhat|): 1.2648,  E(|Yhat-Yhat'|): 1.2930\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6182,  E(|Y-Yhat|): 1.2456,  E(|Yhat-Yhat'|): 1.2547\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6186,  E(|Y-Yhat|): 1.2529,  E(|Yhat-Yhat'|): 1.2687\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9978,  E(|Y-Yhat|): 4.1413,  E(|Yhat-Yhat'|): 4.2869\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8537,  E(|Y-Yhat|): 2.2433,  E(|Yhat-Yhat'|): 0.7792\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3810,  E(|Y-Yhat|): 27.6321,  E(|Yhat-Yhat'|): 52.5021\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5888,  E(|Y-Yhat|): 65.1029,  E(|Yhat-Yhat'|): 127.0282\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -1.3224,  E(|Y-Yhat|): 163.6625,  E(|Yhat-Yhat'|): 329.9698\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.5474,  E(|Y-Yhat|): 410.5961,  E(|Yhat-Yhat'|): 816.0973\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3481,  E(|Y-Yhat|): 0.5701,  E(|Yhat-Yhat'|): 0.4440\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2455,  E(|Y-Yhat|): 0.4972,  E(|Yhat-Yhat'|): 0.5034\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4936,  E(|Yhat-Yhat'|): 0.5026\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4906,  E(|Yhat-Yhat'|): 0.4967\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3342,  E(|Y-Yhat|): 0.6717,  E(|Yhat-Yhat'|): 0.6749\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6711,  E(|Y-Yhat|): 1.0978,  E(|Yhat-Yhat'|): 0.8533\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4531,  E(|Y-Yhat|): 0.9281,  E(|Yhat-Yhat'|): 0.9499\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4513,  E(|Y-Yhat|): 0.9279,  E(|Yhat-Yhat'|): 0.9531\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4607,  E(|Y-Yhat|): 0.9182,  E(|Yhat-Yhat'|): 0.9150\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6575,  E(|Y-Yhat|): 1.3434,  E(|Yhat-Yhat'|): 1.3718\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3344,  E(|Y-Yhat|): 0.6018,  E(|Yhat-Yhat'|): 0.5348\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2644,  E(|Y-Yhat|): 0.5315,  E(|Yhat-Yhat'|): 0.5342\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2689,  E(|Y-Yhat|): 0.5372,  E(|Yhat-Yhat'|): 0.5365\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2677,  E(|Y-Yhat|): 0.5355,  E(|Yhat-Yhat'|): 0.5356\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3338,  E(|Y-Yhat|): 0.6693,  E(|Yhat-Yhat'|): 0.6711\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9338,  E(|Y-Yhat|): 1.5747,  E(|Yhat-Yhat'|): 1.2817\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8842,  E(|Y-Yhat|): 1.7662,  E(|Yhat-Yhat'|): 1.7641\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8770,  E(|Y-Yhat|): 1.7809,  E(|Yhat-Yhat'|): 1.8079\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8812,  E(|Y-Yhat|): 1.7785,  E(|Yhat-Yhat'|): 1.7946\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9895,  E(|Y-Yhat|): 4.1525,  E(|Yhat-Yhat'|): 4.3260\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0685,  E(|Y-Yhat|): 2.3514,  E(|Yhat-Yhat'|): 0.5658\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.0685,  E(|Y-Yhat|): 19.7612,  E(|Yhat-Yhat'|): 35.3855\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9889,  E(|Y-Yhat|): 7.0800,  E(|Yhat-Yhat'|): 10.1822\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.4819,  E(|Y-Yhat|): 51.7538,  E(|Yhat-Yhat'|): 98.5436\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.5508,  E(|Y-Yhat|): 49.9357,  E(|Yhat-Yhat'|): 94.7698\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3579,  E(|Y-Yhat|): 0.6320,  E(|Yhat-Yhat'|): 0.5482\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3312,  E(|Y-Yhat|): 0.6665,  E(|Yhat-Yhat'|): 0.6706\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3325,  E(|Y-Yhat|): 0.6640,  E(|Yhat-Yhat'|): 0.6631\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3337,  E(|Y-Yhat|): 0.6709,  E(|Yhat-Yhat'|): 0.6744\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3349,  E(|Y-Yhat|): 0.6742,  E(|Yhat-Yhat'|): 0.6785\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7266,  E(|Y-Yhat|): 1.1709,  E(|Yhat-Yhat'|): 0.8886\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6700,  E(|Y-Yhat|): 1.3524,  E(|Yhat-Yhat'|): 1.3647\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6729,  E(|Y-Yhat|): 1.3418,  E(|Yhat-Yhat'|): 1.3379\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6680,  E(|Y-Yhat|): 1.3342,  E(|Yhat-Yhat'|): 1.3323\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6686,  E(|Y-Yhat|): 1.3427,  E(|Yhat-Yhat'|): 1.3481\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3584,  E(|Y-Yhat|): 0.6343,  E(|Yhat-Yhat'|): 0.5518\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3295,  E(|Y-Yhat|): 0.6661,  E(|Yhat-Yhat'|): 0.6732\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3304,  E(|Y-Yhat|): 0.6666,  E(|Yhat-Yhat'|): 0.6723\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3320,  E(|Y-Yhat|): 0.6657,  E(|Yhat-Yhat'|): 0.6675\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3309,  E(|Y-Yhat|): 0.6700,  E(|Yhat-Yhat'|): 0.6781\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9183,  E(|Y-Yhat|): 1.5849,  E(|Yhat-Yhat'|): 1.3332\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8699,  E(|Y-Yhat|): 1.7638,  E(|Yhat-Yhat'|): 1.7880\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8714,  E(|Y-Yhat|): 1.7614,  E(|Yhat-Yhat'|): 1.7799\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8632,  E(|Y-Yhat|): 1.7398,  E(|Yhat-Yhat'|): 1.7532\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9796,  E(|Y-Yhat|): 4.0874,  E(|Yhat-Yhat'|): 4.2156\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1503,  E(|Y-Yhat|): 2.4954,  E(|Yhat-Yhat'|): 0.6901\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.6143,  E(|Y-Yhat|): 13.9490,  E(|Yhat-Yhat'|): 24.6692\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 6.9749,  E(|Y-Yhat|): 1084.9654,  E(|Yhat-Yhat'|): 2155.9811\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -29.4764,  E(|Y-Yhat|): 1361.8118,  E(|Yhat-Yhat'|): 2782.5764\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 23.7545,  E(|Y-Yhat|): 1502.4982,  E(|Yhat-Yhat'|): 2957.4873\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3574,  E(|Y-Yhat|): 0.6263,  E(|Yhat-Yhat'|): 0.5377\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3307,  E(|Y-Yhat|): 0.6584,  E(|Yhat-Yhat'|): 0.6553\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3324,  E(|Y-Yhat|): 0.6656,  E(|Yhat-Yhat'|): 0.6665\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3342,  E(|Y-Yhat|): 0.6657,  E(|Yhat-Yhat'|): 0.6631\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3312,  E(|Y-Yhat|): 0.6685,  E(|Yhat-Yhat'|): 0.6746\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7225,  E(|Y-Yhat|): 1.1808,  E(|Yhat-Yhat'|): 0.9166\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6567,  E(|Y-Yhat|): 1.3263,  E(|Yhat-Yhat'|): 1.3392\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6578,  E(|Y-Yhat|): 1.3254,  E(|Yhat-Yhat'|): 1.3352\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6651,  E(|Y-Yhat|): 1.3233,  E(|Yhat-Yhat'|): 1.3163\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6752,  E(|Y-Yhat|): 1.3483,  E(|Yhat-Yhat'|): 1.3462\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3634,  E(|Y-Yhat|): 0.6304,  E(|Yhat-Yhat'|): 0.5340\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3329,  E(|Y-Yhat|): 0.6598,  E(|Yhat-Yhat'|): 0.6538\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3327,  E(|Y-Yhat|): 0.6616,  E(|Yhat-Yhat'|): 0.6579\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3284,  E(|Y-Yhat|): 0.6587,  E(|Yhat-Yhat'|): 0.6607\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6696,  E(|Yhat-Yhat'|): 0.6726\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9513,  E(|Y-Yhat|): 1.5303,  E(|Yhat-Yhat'|): 1.1579\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8823,  E(|Y-Yhat|): 1.7887,  E(|Yhat-Yhat'|): 1.8127\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8917,  E(|Y-Yhat|): 1.8287,  E(|Yhat-Yhat'|): 1.8741\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8813,  E(|Y-Yhat|): 1.7830,  E(|Yhat-Yhat'|): 1.8036\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9842,  E(|Y-Yhat|): 4.0996,  E(|Yhat-Yhat'|): 4.2307\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0206,  E(|Y-Yhat|): 2.3579,  E(|Yhat-Yhat'|): 0.6747\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9508,  E(|Y-Yhat|): 3.4951,  E(|Yhat-Yhat'|): 3.0886\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9892,  E(|Y-Yhat|): 2.7230,  E(|Yhat-Yhat'|): 1.4675\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0090,  E(|Y-Yhat|): 3.2835,  E(|Yhat-Yhat'|): 2.5490\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9373,  E(|Y-Yhat|): 3.2131,  E(|Yhat-Yhat'|): 2.5517\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3621,  E(|Y-Yhat|): 0.6141,  E(|Yhat-Yhat'|): 0.5040\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3318,  E(|Y-Yhat|): 0.6701,  E(|Yhat-Yhat'|): 0.6766\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3364,  E(|Y-Yhat|): 0.6686,  E(|Yhat-Yhat'|): 0.6645\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3320,  E(|Y-Yhat|): 0.6660,  E(|Yhat-Yhat'|): 0.6682\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3399,  E(|Y-Yhat|): 0.6699,  E(|Yhat-Yhat'|): 0.6600\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7161,  E(|Y-Yhat|): 1.1921,  E(|Yhat-Yhat'|): 0.9519\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6722,  E(|Y-Yhat|): 1.3587,  E(|Yhat-Yhat'|): 1.3729\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6777,  E(|Y-Yhat|): 1.3505,  E(|Yhat-Yhat'|): 1.3456\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6816,  E(|Y-Yhat|): 1.3471,  E(|Yhat-Yhat'|): 1.3308\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6662,  E(|Y-Yhat|): 1.3395,  E(|Yhat-Yhat'|): 1.3465\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3550,  E(|Y-Yhat|): 0.6430,  E(|Yhat-Yhat'|): 0.5760\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3326,  E(|Y-Yhat|): 0.6678,  E(|Yhat-Yhat'|): 0.6704\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3332,  E(|Y-Yhat|): 0.6651,  E(|Yhat-Yhat'|): 0.6638\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3310,  E(|Y-Yhat|): 0.6658,  E(|Yhat-Yhat'|): 0.6696\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3304,  E(|Y-Yhat|): 0.6676,  E(|Yhat-Yhat'|): 0.6742\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8927,  E(|Y-Yhat|): 1.4627,  E(|Yhat-Yhat'|): 1.1400\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5841,  E(|Y-Yhat|): 1.1852,  E(|Yhat-Yhat'|): 1.2021\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5859,  E(|Y-Yhat|): 1.1880,  E(|Yhat-Yhat'|): 1.2041\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5860,  E(|Y-Yhat|): 1.1787,  E(|Yhat-Yhat'|): 1.1854\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0244,  E(|Y-Yhat|): 4.1020,  E(|Yhat-Yhat'|): 4.1553\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.7154,  E(|Y-Yhat|): 2.0755,  E(|Yhat-Yhat'|): 0.7203\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.0042,  E(|Y-Yhat|): 7.6331,  E(|Yhat-Yhat'|): 13.2578\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8451,  E(|Y-Yhat|): 22.3561,  E(|Yhat-Yhat'|): 43.0220\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.9192,  E(|Y-Yhat|): 9.4856,  E(|Yhat-Yhat'|): 17.1326\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.1212,  E(|Y-Yhat|): 27.6975,  E(|Yhat-Yhat'|): 51.1526\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3288,  E(|Y-Yhat|): 0.5711,  E(|Yhat-Yhat'|): 0.4846\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2346,  E(|Y-Yhat|): 0.4647,  E(|Yhat-Yhat'|): 0.4603\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2301,  E(|Y-Yhat|): 0.4657,  E(|Yhat-Yhat'|): 0.4712\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2319,  E(|Y-Yhat|): 0.4626,  E(|Yhat-Yhat'|): 0.4615\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6633,  E(|Yhat-Yhat'|): 0.6601\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6545,  E(|Y-Yhat|): 1.0934,  E(|Yhat-Yhat'|): 0.8780\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.4345,  E(|Y-Yhat|): 0.8682,  E(|Yhat-Yhat'|): 0.8674\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.4250,  E(|Y-Yhat|): 0.8570,  E(|Yhat-Yhat'|): 0.8639\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.4258,  E(|Y-Yhat|): 0.8596,  E(|Yhat-Yhat'|): 0.8676\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6768,  E(|Y-Yhat|): 1.3653,  E(|Yhat-Yhat'|): 1.3771\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3402,  E(|Y-Yhat|): 0.6042,  E(|Yhat-Yhat'|): 0.5280\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2576,  E(|Y-Yhat|): 0.5159,  E(|Yhat-Yhat'|): 0.5166\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2530,  E(|Y-Yhat|): 0.5131,  E(|Yhat-Yhat'|): 0.5202\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2537,  E(|Y-Yhat|): 0.5131,  E(|Yhat-Yhat'|): 0.5188\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3322,  E(|Y-Yhat|): 0.6682,  E(|Yhat-Yhat'|): 0.6721\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9338,  E(|Y-Yhat|): 1.5448,  E(|Yhat-Yhat'|): 1.2220\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8515,  E(|Y-Yhat|): 1.7173,  E(|Yhat-Yhat'|): 1.7317\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8416,  E(|Y-Yhat|): 1.7322,  E(|Yhat-Yhat'|): 1.7812\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8554,  E(|Y-Yhat|): 1.7137,  E(|Yhat-Yhat'|): 1.7166\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0361,  E(|Y-Yhat|): 4.0766,  E(|Yhat-Yhat'|): 4.0810\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0469,  E(|Y-Yhat|): 2.3983,  E(|Yhat-Yhat'|): 0.7028\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.0688,  E(|Y-Yhat|): 14.0785,  E(|Yhat-Yhat'|): 24.0195\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.0503,  E(|Y-Yhat|): 39.9752,  E(|Yhat-Yhat'|): 77.8497\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8908,  E(|Y-Yhat|): 8.0277,  E(|Yhat-Yhat'|): 12.2738\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0603,  E(|Y-Yhat|): 8.9155,  E(|Yhat-Yhat'|): 13.7104\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3653,  E(|Y-Yhat|): 0.6055,  E(|Yhat-Yhat'|): 0.4804\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3241,  E(|Y-Yhat|): 0.6524,  E(|Yhat-Yhat'|): 0.6567\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3248,  E(|Y-Yhat|): 0.6497,  E(|Yhat-Yhat'|): 0.6499\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3278,  E(|Y-Yhat|): 0.6525,  E(|Yhat-Yhat'|): 0.6493\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3366,  E(|Y-Yhat|): 0.6771,  E(|Yhat-Yhat'|): 0.6810\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7211,  E(|Y-Yhat|): 1.1616,  E(|Yhat-Yhat'|): 0.8809\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6530,  E(|Y-Yhat|): 1.2966,  E(|Yhat-Yhat'|): 1.2872\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6472,  E(|Y-Yhat|): 1.3046,  E(|Yhat-Yhat'|): 1.3148\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6404,  E(|Y-Yhat|): 1.2923,  E(|Yhat-Yhat'|): 1.3038\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6796,  E(|Y-Yhat|): 1.3679,  E(|Yhat-Yhat'|): 1.3766\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3577,  E(|Y-Yhat|): 0.6289,  E(|Yhat-Yhat'|): 0.5425\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3226,  E(|Y-Yhat|): 0.6538,  E(|Yhat-Yhat'|): 0.6624\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3246,  E(|Y-Yhat|): 0.6519,  E(|Yhat-Yhat'|): 0.6546\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3248,  E(|Y-Yhat|): 0.6542,  E(|Yhat-Yhat'|): 0.6588\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3370,  E(|Y-Yhat|): 0.6724,  E(|Yhat-Yhat'|): 0.6707\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8144,  E(|Y-Yhat|): 1.3624,  E(|Yhat-Yhat'|): 1.0961\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5133,  E(|Y-Yhat|): 1.0412,  E(|Yhat-Yhat'|): 1.0557\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5140,  E(|Y-Yhat|): 1.0480,  E(|Yhat-Yhat'|): 1.0679\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5112,  E(|Y-Yhat|): 1.0420,  E(|Yhat-Yhat'|): 1.0617\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0511,  E(|Y-Yhat|): 4.1655,  E(|Yhat-Yhat'|): 4.2288\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8877,  E(|Y-Yhat|): 2.2077,  E(|Yhat-Yhat'|): 0.6399\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6988,  E(|Y-Yhat|): 26.6246,  E(|Yhat-Yhat'|): 51.8518\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8283,  E(|Y-Yhat|): 43.1218,  E(|Yhat-Yhat'|): 84.5871\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6866,  E(|Y-Yhat|): 31.8616,  E(|Yhat-Yhat'|): 62.3500\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9649,  E(|Y-Yhat|): 102.4827,  E(|Yhat-Yhat'|): 201.0357\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.2942,  E(|Y-Yhat|): 0.5193,  E(|Yhat-Yhat'|): 0.4502\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2009,  E(|Y-Yhat|): 0.4041,  E(|Yhat-Yhat'|): 0.4066\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.1993,  E(|Y-Yhat|): 0.4005,  E(|Yhat-Yhat'|): 0.4024\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.1996,  E(|Y-Yhat|): 0.4000,  E(|Yhat-Yhat'|): 0.4008\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3369,  E(|Y-Yhat|): 0.6749,  E(|Yhat-Yhat'|): 0.6761\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.5645,  E(|Y-Yhat|): 0.9973,  E(|Yhat-Yhat'|): 0.8655\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3565,  E(|Y-Yhat|): 0.7276,  E(|Yhat-Yhat'|): 0.7422\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3630,  E(|Y-Yhat|): 0.7310,  E(|Yhat-Yhat'|): 0.7359\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3572,  E(|Y-Yhat|): 0.7245,  E(|Yhat-Yhat'|): 0.7346\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6631,  E(|Y-Yhat|): 1.3581,  E(|Yhat-Yhat'|): 1.3900\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3007,  E(|Y-Yhat|): 0.5634,  E(|Yhat-Yhat'|): 0.5254\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2273,  E(|Y-Yhat|): 0.4614,  E(|Yhat-Yhat'|): 0.4681\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2304,  E(|Y-Yhat|): 0.4635,  E(|Yhat-Yhat'|): 0.4663\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2282,  E(|Y-Yhat|): 0.4606,  E(|Yhat-Yhat'|): 0.4649\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3375,  E(|Y-Yhat|): 0.6703,  E(|Yhat-Yhat'|): 0.6656\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8598,  E(|Y-Yhat|): 1.5244,  E(|Yhat-Yhat'|): 1.3291\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7643,  E(|Y-Yhat|): 1.5641,  E(|Yhat-Yhat'|): 1.5995\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7545,  E(|Y-Yhat|): 1.5423,  E(|Yhat-Yhat'|): 1.5756\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7549,  E(|Y-Yhat|): 1.5360,  E(|Yhat-Yhat'|): 1.5622\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9874,  E(|Y-Yhat|): 4.1348,  E(|Yhat-Yhat'|): 4.2947\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8448,  E(|Y-Yhat|): 2.1833,  E(|Yhat-Yhat'|): 0.6770\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1615,  E(|Y-Yhat|): 24.3193,  E(|Yhat-Yhat'|): 46.3155\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8633,  E(|Y-Yhat|): 147.3855,  E(|Yhat-Yhat'|): 293.0443\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 5.2253,  E(|Y-Yhat|): 471.4986,  E(|Yhat-Yhat'|): 932.5466\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 3.1342,  E(|Y-Yhat|): 624.5703,  E(|Yhat-Yhat'|): 1242.8722\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3419,  E(|Y-Yhat|): 0.5997,  E(|Yhat-Yhat'|): 0.5156\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2991,  E(|Y-Yhat|): 0.6015,  E(|Yhat-Yhat'|): 0.6048\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2942,  E(|Y-Yhat|): 0.6002,  E(|Yhat-Yhat'|): 0.6121\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3018,  E(|Y-Yhat|): 0.5997,  E(|Yhat-Yhat'|): 0.5957\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6650,  E(|Yhat-Yhat'|): 0.6634\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7039,  E(|Y-Yhat|): 1.1568,  E(|Yhat-Yhat'|): 0.9059\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5640,  E(|Y-Yhat|): 1.1570,  E(|Yhat-Yhat'|): 1.1860\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5773,  E(|Y-Yhat|): 1.1535,  E(|Yhat-Yhat'|): 1.1523\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5840,  E(|Y-Yhat|): 1.1699,  E(|Yhat-Yhat'|): 1.1718\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6701,  E(|Y-Yhat|): 1.3469,  E(|Yhat-Yhat'|): 1.3537\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3492,  E(|Y-Yhat|): 0.6222,  E(|Yhat-Yhat'|): 0.5459\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3038,  E(|Y-Yhat|): 0.6162,  E(|Yhat-Yhat'|): 0.6248\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3046,  E(|Y-Yhat|): 0.6179,  E(|Yhat-Yhat'|): 0.6265\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3060,  E(|Y-Yhat|): 0.6144,  E(|Yhat-Yhat'|): 0.6168\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3304,  E(|Y-Yhat|): 0.6671,  E(|Yhat-Yhat'|): 0.6733\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9474,  E(|Y-Yhat|): 1.5429,  E(|Yhat-Yhat'|): 1.1911\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8779,  E(|Y-Yhat|): 1.7505,  E(|Yhat-Yhat'|): 1.7452\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8768,  E(|Y-Yhat|): 1.7753,  E(|Yhat-Yhat'|): 1.7970\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8673,  E(|Y-Yhat|): 1.7471,  E(|Yhat-Yhat'|): 1.7598\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9751,  E(|Y-Yhat|): 4.1233,  E(|Yhat-Yhat'|): 4.2965\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0729,  E(|Y-Yhat|): 2.4364,  E(|Yhat-Yhat'|): 0.7270\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8527,  E(|Y-Yhat|): 9.6685,  E(|Yhat-Yhat'|): 15.6316\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.8987,  E(|Y-Yhat|): 6.3513,  E(|Yhat-Yhat'|): 8.9054\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.0255,  E(|Y-Yhat|): 8.8258,  E(|Yhat-Yhat'|): 13.6006\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8877,  E(|Y-Yhat|): 8.7242,  E(|Yhat-Yhat'|): 13.6730\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3703,  E(|Y-Yhat|): 0.6166,  E(|Yhat-Yhat'|): 0.4926\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3294,  E(|Y-Yhat|): 0.6642,  E(|Yhat-Yhat'|): 0.6696\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3301,  E(|Y-Yhat|): 0.6641,  E(|Yhat-Yhat'|): 0.6681\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3316,  E(|Y-Yhat|): 0.6694,  E(|Yhat-Yhat'|): 0.6756\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3310,  E(|Y-Yhat|): 0.6714,  E(|Yhat-Yhat'|): 0.6808\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7137,  E(|Y-Yhat|): 1.1957,  E(|Yhat-Yhat'|): 0.9640\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6706,  E(|Y-Yhat|): 1.3408,  E(|Yhat-Yhat'|): 1.3405\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6626,  E(|Y-Yhat|): 1.3390,  E(|Yhat-Yhat'|): 1.3528\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6612,  E(|Y-Yhat|): 1.3343,  E(|Yhat-Yhat'|): 1.3462\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6738,  E(|Y-Yhat|): 1.3498,  E(|Yhat-Yhat'|): 1.3522\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3562,  E(|Y-Yhat|): 0.6360,  E(|Yhat-Yhat'|): 0.5595\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3313,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6672\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3324,  E(|Y-Yhat|): 0.6619,  E(|Yhat-Yhat'|): 0.6589\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3297,  E(|Y-Yhat|): 0.6637,  E(|Yhat-Yhat'|): 0.6680\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3291,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6718\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9168,  E(|Y-Yhat|): 1.5130,  E(|Yhat-Yhat'|): 1.1925\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7622,  E(|Y-Yhat|): 1.5331,  E(|Yhat-Yhat'|): 1.5420\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7567,  E(|Y-Yhat|): 1.5298,  E(|Yhat-Yhat'|): 1.5463\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7580,  E(|Y-Yhat|): 1.5313,  E(|Yhat-Yhat'|): 1.5467\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0263,  E(|Y-Yhat|): 4.1458,  E(|Yhat-Yhat'|): 4.2390\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9146,  E(|Y-Yhat|): 2.2100,  E(|Yhat-Yhat'|): 0.5907\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.4660,  E(|Y-Yhat|): 4.3700,  E(|Yhat-Yhat'|): 5.8081\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.6822,  E(|Y-Yhat|): 17.0622,  E(|Yhat-Yhat'|): 30.7601\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.5473,  E(|Y-Yhat|): 5.4260,  E(|Yhat-Yhat'|): 7.7574\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8490,  E(|Y-Yhat|): 8.4070,  E(|Yhat-Yhat'|): 13.1159\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3585,  E(|Y-Yhat|): 0.6081,  E(|Yhat-Yhat'|): 0.4991\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2983,  E(|Y-Yhat|): 0.6006,  E(|Yhat-Yhat'|): 0.6046\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2922,  E(|Y-Yhat|): 0.5938,  E(|Yhat-Yhat'|): 0.6032\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2963,  E(|Y-Yhat|): 0.5973,  E(|Yhat-Yhat'|): 0.6020\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3343,  E(|Y-Yhat|): 0.6723,  E(|Yhat-Yhat'|): 0.6759\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7079,  E(|Y-Yhat|): 1.1546,  E(|Yhat-Yhat'|): 0.8935\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5727,  E(|Y-Yhat|): 1.1432,  E(|Yhat-Yhat'|): 1.1409\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5766,  E(|Y-Yhat|): 1.1671,  E(|Yhat-Yhat'|): 1.1810\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5727,  E(|Y-Yhat|): 1.1542,  E(|Yhat-Yhat'|): 1.1630\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6726,  E(|Y-Yhat|): 1.3665,  E(|Yhat-Yhat'|): 1.3879\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3463,  E(|Y-Yhat|): 0.6259,  E(|Yhat-Yhat'|): 0.5591\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3031,  E(|Y-Yhat|): 0.6103,  E(|Yhat-Yhat'|): 0.6143\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3065,  E(|Y-Yhat|): 0.6134,  E(|Yhat-Yhat'|): 0.6138\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3005,  E(|Y-Yhat|): 0.6105,  E(|Yhat-Yhat'|): 0.6200\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3351,  E(|Y-Yhat|): 0.6722,  E(|Yhat-Yhat'|): 0.6743\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0051),\n",
       " tensor(0.3044),\n",
       " tensor(0.0053),\n",
       " tensor(0.0045),\n",
       " tensor(0.0060))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"square\", x_lower=-2, x_upper=2, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = (F.relu(Z))**2 / 2.0\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a81f6d",
   "metadata": {},
   "source": [
    "## True function: log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbee07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9025,  E(|Y-Yhat|): 1.5541,  E(|Yhat-Yhat'|): 1.3033\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8446,  E(|Y-Yhat|): 1.6851,  E(|Yhat-Yhat'|): 1.6810\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8336,  E(|Y-Yhat|): 1.6760,  E(|Yhat-Yhat'|): 1.6847\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8472,  E(|Y-Yhat|): 1.6829,  E(|Yhat-Yhat'|): 1.6715\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0231,  E(|Y-Yhat|): 4.1163,  E(|Yhat-Yhat'|): 4.1864\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8870,  E(|Y-Yhat|): 2.3210,  E(|Yhat-Yhat'|): 0.8680\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.7805,  E(|Y-Yhat|): 14.6894,  E(|Yhat-Yhat'|): 25.8178\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.6469,  E(|Y-Yhat|): 14.3872,  E(|Yhat-Yhat'|): 25.4807\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.7733,  E(|Y-Yhat|): 5.4820,  E(|Yhat-Yhat'|): 7.4173\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9915,  E(|Y-Yhat|): 6.1993,  E(|Yhat-Yhat'|): 8.4157\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3647,  E(|Y-Yhat|): 0.6122,  E(|Yhat-Yhat'|): 0.4949\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3217,  E(|Y-Yhat|): 0.6472,  E(|Yhat-Yhat'|): 0.6510\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3187,  E(|Y-Yhat|): 0.6435,  E(|Yhat-Yhat'|): 0.6497\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3183,  E(|Y-Yhat|): 0.6408,  E(|Yhat-Yhat'|): 0.6450\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3315,  E(|Y-Yhat|): 0.6659,  E(|Yhat-Yhat'|): 0.6686\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7186,  E(|Y-Yhat|): 1.1594,  E(|Yhat-Yhat'|): 0.8815\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6268,  E(|Y-Yhat|): 1.2655,  E(|Yhat-Yhat'|): 1.2773\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6334,  E(|Y-Yhat|): 1.2632,  E(|Yhat-Yhat'|): 1.2597\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6358,  E(|Y-Yhat|): 1.2691,  E(|Yhat-Yhat'|): 1.2667\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6695,  E(|Y-Yhat|): 1.3385,  E(|Yhat-Yhat'|): 1.3380\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3513,  E(|Y-Yhat|): 0.6336,  E(|Yhat-Yhat'|): 0.5644\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3236,  E(|Y-Yhat|): 0.6464,  E(|Yhat-Yhat'|): 0.6456\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3224,  E(|Y-Yhat|): 0.6455,  E(|Yhat-Yhat'|): 0.6463\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3189,  E(|Y-Yhat|): 0.6471,  E(|Yhat-Yhat'|): 0.6564\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3310,  E(|Y-Yhat|): 0.6691,  E(|Yhat-Yhat'|): 0.6762\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9288,  E(|Y-Yhat|): 1.5692,  E(|Yhat-Yhat'|): 1.2809\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8711,  E(|Y-Yhat|): 1.7550,  E(|Yhat-Yhat'|): 1.7680\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8736,  E(|Y-Yhat|): 1.7697,  E(|Yhat-Yhat'|): 1.7922\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8835,  E(|Y-Yhat|): 1.7617,  E(|Yhat-Yhat'|): 1.7565\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0061,  E(|Y-Yhat|): 4.1266,  E(|Yhat-Yhat'|): 4.2408\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0433,  E(|Y-Yhat|): 2.3310,  E(|Yhat-Yhat'|): 0.5756\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.5104,  E(|Y-Yhat|): 70.7541,  E(|Yhat-Yhat'|): 136.4873\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9767,  E(|Y-Yhat|): 29.4153,  E(|Yhat-Yhat'|): 54.8771\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 2.1662,  E(|Y-Yhat|): 37.0678,  E(|Yhat-Yhat'|): 69.8032\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9805,  E(|Y-Yhat|): 39.4316,  E(|Yhat-Yhat'|): 74.9023\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3566,  E(|Y-Yhat|): 0.6304,  E(|Yhat-Yhat'|): 0.5476\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3305,  E(|Y-Yhat|): 0.6613,  E(|Yhat-Yhat'|): 0.6615\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3294,  E(|Y-Yhat|): 0.6597,  E(|Yhat-Yhat'|): 0.6605\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3311,  E(|Y-Yhat|): 0.6609,  E(|Yhat-Yhat'|): 0.6596\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3358,  E(|Y-Yhat|): 0.6690,  E(|Yhat-Yhat'|): 0.6663\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7256,  E(|Y-Yhat|): 1.1702,  E(|Yhat-Yhat'|): 0.8892\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6608,  E(|Y-Yhat|): 1.3258,  E(|Yhat-Yhat'|): 1.3300\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6548,  E(|Y-Yhat|): 1.3254,  E(|Yhat-Yhat'|): 1.3411\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6694,  E(|Y-Yhat|): 1.3329,  E(|Yhat-Yhat'|): 1.3271\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6727,  E(|Y-Yhat|): 1.3424,  E(|Yhat-Yhat'|): 1.3394\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3570,  E(|Y-Yhat|): 0.6330,  E(|Yhat-Yhat'|): 0.5519\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3272,  E(|Y-Yhat|): 0.6572,  E(|Yhat-Yhat'|): 0.6602\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3321,  E(|Y-Yhat|): 0.6655,  E(|Yhat-Yhat'|): 0.6669\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3300,  E(|Y-Yhat|): 0.6606,  E(|Yhat-Yhat'|): 0.6612\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3311,  E(|Y-Yhat|): 0.6649,  E(|Yhat-Yhat'|): 0.6677\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9139,  E(|Y-Yhat|): 1.5784,  E(|Yhat-Yhat'|): 1.3288\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8742,  E(|Y-Yhat|): 1.7908,  E(|Yhat-Yhat'|): 1.8333\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8638,  E(|Y-Yhat|): 1.7590,  E(|Yhat-Yhat'|): 1.7906\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8565,  E(|Y-Yhat|): 1.7286,  E(|Yhat-Yhat'|): 1.7443\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9896,  E(|Y-Yhat|): 4.0105,  E(|Yhat-Yhat'|): 4.0418\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.1266,  E(|Y-Yhat|): 2.4767,  E(|Yhat-Yhat'|): 0.7002\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8492,  E(|Y-Yhat|): 5.4335,  E(|Yhat-Yhat'|): 7.1685\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 2.5794,  E(|Y-Yhat|): 69.0851,  E(|Yhat-Yhat'|): 133.0115\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.0857,  E(|Y-Yhat|): 84.6515,  E(|Yhat-Yhat'|): 167.1316\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.4045,  E(|Y-Yhat|): 97.2478,  E(|Yhat-Yhat'|): 189.6867\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3558,  E(|Y-Yhat|): 0.6243,  E(|Yhat-Yhat'|): 0.5369\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3258,  E(|Y-Yhat|): 0.6587,  E(|Yhat-Yhat'|): 0.6657\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3299,  E(|Y-Yhat|): 0.6631,  E(|Yhat-Yhat'|): 0.6664\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3300,  E(|Y-Yhat|): 0.6612,  E(|Yhat-Yhat'|): 0.6625\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3310,  E(|Y-Yhat|): 0.6680,  E(|Yhat-Yhat'|): 0.6740\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7188,  E(|Y-Yhat|): 1.1762,  E(|Yhat-Yhat'|): 0.9149\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6594,  E(|Y-Yhat|): 1.3268,  E(|Yhat-Yhat'|): 1.3348\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6599,  E(|Y-Yhat|): 1.3359,  E(|Yhat-Yhat'|): 1.3519\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6611,  E(|Y-Yhat|): 1.3246,  E(|Yhat-Yhat'|): 1.3269\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6660,  E(|Y-Yhat|): 1.3460,  E(|Yhat-Yhat'|): 1.3599\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3614,  E(|Y-Yhat|): 0.6281,  E(|Yhat-Yhat'|): 0.5333\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3320,  E(|Y-Yhat|): 0.6614,  E(|Yhat-Yhat'|): 0.6587\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3311,  E(|Y-Yhat|): 0.6638,  E(|Yhat-Yhat'|): 0.6653\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3281,  E(|Y-Yhat|): 0.6608,  E(|Yhat-Yhat'|): 0.6656\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3354,  E(|Y-Yhat|): 0.6724,  E(|Yhat-Yhat'|): 0.6740\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9448,  E(|Y-Yhat|): 1.5218,  E(|Yhat-Yhat'|): 1.1539\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8707,  E(|Y-Yhat|): 1.7683,  E(|Yhat-Yhat'|): 1.7953\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8750,  E(|Y-Yhat|): 1.7694,  E(|Yhat-Yhat'|): 1.7887\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8712,  E(|Y-Yhat|): 1.7668,  E(|Yhat-Yhat'|): 1.7912\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9896,  E(|Y-Yhat|): 4.0520,  E(|Yhat-Yhat'|): 4.1247\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9965,  E(|Y-Yhat|): 2.3362,  E(|Yhat-Yhat'|): 0.6794\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.8793,  E(|Y-Yhat|): 6.9000,  E(|Yhat-Yhat'|): 10.0413\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.9145,  E(|Y-Yhat|): 3.4168,  E(|Yhat-Yhat'|): 3.0046\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9249,  E(|Y-Yhat|): 3.6081,  E(|Yhat-Yhat'|): 3.3665\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9217,  E(|Y-Yhat|): 3.6853,  E(|Yhat-Yhat'|): 3.5272\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3601,  E(|Y-Yhat|): 0.6108,  E(|Yhat-Yhat'|): 0.5015\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3301,  E(|Y-Yhat|): 0.6635,  E(|Yhat-Yhat'|): 0.6668\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3323,  E(|Y-Yhat|): 0.6625,  E(|Yhat-Yhat'|): 0.6604\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3326,  E(|Y-Yhat|): 0.6631,  E(|Yhat-Yhat'|): 0.6610\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3367,  E(|Y-Yhat|): 0.6712,  E(|Yhat-Yhat'|): 0.6690\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7106,  E(|Y-Yhat|): 1.1872,  E(|Yhat-Yhat'|): 0.9533\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6570,  E(|Y-Yhat|): 1.3336,  E(|Yhat-Yhat'|): 1.3532\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6592,  E(|Y-Yhat|): 1.3214,  E(|Yhat-Yhat'|): 1.3244\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6603,  E(|Y-Yhat|): 1.3286,  E(|Yhat-Yhat'|): 1.3366\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6667,  E(|Y-Yhat|): 1.3418,  E(|Yhat-Yhat'|): 1.3502\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3546,  E(|Y-Yhat|): 0.6416,  E(|Yhat-Yhat'|): 0.5739\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3287,  E(|Y-Yhat|): 0.6575,  E(|Yhat-Yhat'|): 0.6577\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3306,  E(|Y-Yhat|): 0.6616,  E(|Yhat-Yhat'|): 0.6619\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3275,  E(|Y-Yhat|): 0.6582,  E(|Yhat-Yhat'|): 0.6614\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3326,  E(|Y-Yhat|): 0.6671,  E(|Yhat-Yhat'|): 0.6689\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9199,  E(|Y-Yhat|): 1.5165,  E(|Yhat-Yhat'|): 1.1932\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7309,  E(|Y-Yhat|): 1.4749,  E(|Yhat-Yhat'|): 1.4881\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7350,  E(|Y-Yhat|): 1.4756,  E(|Yhat-Yhat'|): 1.4812\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7256,  E(|Y-Yhat|): 1.4678,  E(|Yhat-Yhat'|): 1.4842\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0378,  E(|Y-Yhat|): 4.0630,  E(|Yhat-Yhat'|): 4.0504\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8022,  E(|Y-Yhat|): 2.1678,  E(|Yhat-Yhat'|): 0.7312\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3870,  E(|Y-Yhat|): 6.5552,  E(|Yhat-Yhat'|): 10.3365\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5197,  E(|Y-Yhat|): 105.4009,  E(|Yhat-Yhat'|): 207.7623\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.2362,  E(|Y-Yhat|): 56.8583,  E(|Yhat-Yhat'|): 111.2443\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.7328,  E(|Y-Yhat|): 92.5785,  E(|Yhat-Yhat'|): 181.6914\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3521,  E(|Y-Yhat|): 0.6019,  E(|Yhat-Yhat'|): 0.4995\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2899,  E(|Y-Yhat|): 0.5841,  E(|Yhat-Yhat'|): 0.5883\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2865,  E(|Y-Yhat|): 0.5782,  E(|Yhat-Yhat'|): 0.5834\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2879,  E(|Y-Yhat|): 0.5779,  E(|Yhat-Yhat'|): 0.5802\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3344,  E(|Y-Yhat|): 0.6711,  E(|Yhat-Yhat'|): 0.6734\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6917,  E(|Y-Yhat|): 1.1467,  E(|Yhat-Yhat'|): 0.9100\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5474,  E(|Y-Yhat|): 1.1132,  E(|Yhat-Yhat'|): 1.1317\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5539,  E(|Y-Yhat|): 1.1042,  E(|Yhat-Yhat'|): 1.1007\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5513,  E(|Y-Yhat|): 1.1075,  E(|Yhat-Yhat'|): 1.1124\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6741,  E(|Y-Yhat|): 1.3540,  E(|Yhat-Yhat'|): 1.3597\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3516,  E(|Y-Yhat|): 0.6246,  E(|Yhat-Yhat'|): 0.5460\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2997,  E(|Y-Yhat|): 0.5936,  E(|Yhat-Yhat'|): 0.5878\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3002,  E(|Y-Yhat|): 0.5977,  E(|Yhat-Yhat'|): 0.5950\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2950,  E(|Y-Yhat|): 0.5972,  E(|Yhat-Yhat'|): 0.6046\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3285,  E(|Y-Yhat|): 0.6696,  E(|Yhat-Yhat'|): 0.6821\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9202,  E(|Y-Yhat|): 1.5322,  E(|Yhat-Yhat'|): 1.2240\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8425,  E(|Y-Yhat|): 1.6796,  E(|Yhat-Yhat'|): 1.6742\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8192,  E(|Y-Yhat|): 1.6930,  E(|Yhat-Yhat'|): 1.7476\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8438,  E(|Y-Yhat|): 1.6863,  E(|Yhat-Yhat'|): 1.6849\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0430,  E(|Y-Yhat|): 4.0656,  E(|Yhat-Yhat'|): 4.0453\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9935,  E(|Y-Yhat|): 2.3421,  E(|Yhat-Yhat'|): 0.6973\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 2.0483,  E(|Y-Yhat|): 16.1565,  E(|Yhat-Yhat'|): 28.2164\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.5275,  E(|Y-Yhat|): 17.9931,  E(|Yhat-Yhat'|): 32.9312\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8679,  E(|Y-Yhat|): 7.2688,  E(|Yhat-Yhat'|): 10.8017\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0622,  E(|Y-Yhat|): 8.1445,  E(|Yhat-Yhat'|): 12.1645\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3629,  E(|Y-Yhat|): 0.6032,  E(|Yhat-Yhat'|): 0.4807\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3222,  E(|Y-Yhat|): 0.6482,  E(|Yhat-Yhat'|): 0.6521\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3202,  E(|Y-Yhat|): 0.6400,  E(|Yhat-Yhat'|): 0.6394\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3217,  E(|Y-Yhat|): 0.6431,  E(|Yhat-Yhat'|): 0.6428\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3342,  E(|Y-Yhat|): 0.6726,  E(|Yhat-Yhat'|): 0.6767\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7122,  E(|Y-Yhat|): 1.1542,  E(|Yhat-Yhat'|): 0.8839\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6353,  E(|Y-Yhat|): 1.2730,  E(|Yhat-Yhat'|): 1.2754\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6360,  E(|Y-Yhat|): 1.2728,  E(|Yhat-Yhat'|): 1.2736\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6251,  E(|Y-Yhat|): 1.2596,  E(|Yhat-Yhat'|): 1.2689\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6798,  E(|Y-Yhat|): 1.3583,  E(|Yhat-Yhat'|): 1.3570\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3568,  E(|Y-Yhat|): 0.6273,  E(|Yhat-Yhat'|): 0.5412\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3200,  E(|Y-Yhat|): 0.6460,  E(|Yhat-Yhat'|): 0.6520\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3273,  E(|Y-Yhat|): 0.6498,  E(|Yhat-Yhat'|): 0.6449\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3215,  E(|Y-Yhat|): 0.6463,  E(|Yhat-Yhat'|): 0.6496\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3332,  E(|Y-Yhat|): 0.6669,  E(|Yhat-Yhat'|): 0.6675\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8899,  E(|Y-Yhat|): 1.4895,  E(|Yhat-Yhat'|): 1.1992\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7167,  E(|Y-Yhat|): 1.4242,  E(|Yhat-Yhat'|): 1.4150\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7078,  E(|Y-Yhat|): 1.4552,  E(|Yhat-Yhat'|): 1.4948\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7041,  E(|Y-Yhat|): 1.4365,  E(|Yhat-Yhat'|): 1.4648\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9993,  E(|Y-Yhat|): 4.1038,  E(|Yhat-Yhat'|): 4.2089\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9122,  E(|Y-Yhat|): 2.2035,  E(|Yhat-Yhat'|): 0.5827\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3226,  E(|Y-Yhat|): 4.5762,  E(|Yhat-Yhat'|): 6.5071\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.4424,  E(|Y-Yhat|): 8.6741,  E(|Yhat-Yhat'|): 14.4633\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.3607,  E(|Y-Yhat|): 7.3232,  E(|Yhat-Yhat'|): 11.9251\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9655,  E(|Y-Yhat|): 11.3035,  E(|Yhat-Yhat'|): 18.6760\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3475,  E(|Y-Yhat|): 0.6050,  E(|Yhat-Yhat'|): 0.5151\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2798,  E(|Y-Yhat|): 0.5688,  E(|Yhat-Yhat'|): 0.5779\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2833,  E(|Y-Yhat|): 0.5662,  E(|Yhat-Yhat'|): 0.5658\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2863,  E(|Y-Yhat|): 0.5710,  E(|Yhat-Yhat'|): 0.5694\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3335,  E(|Y-Yhat|): 0.6677,  E(|Yhat-Yhat'|): 0.6686\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.6650,  E(|Y-Yhat|): 1.1286,  E(|Yhat-Yhat'|): 0.9270\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5340,  E(|Y-Yhat|): 1.0725,  E(|Yhat-Yhat'|): 1.0770\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5394,  E(|Y-Yhat|): 1.0808,  E(|Yhat-Yhat'|): 1.0828\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5324,  E(|Y-Yhat|): 1.0794,  E(|Yhat-Yhat'|): 1.0939\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6726,  E(|Y-Yhat|): 1.3564,  E(|Yhat-Yhat'|): 1.3676\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3413,  E(|Y-Yhat|): 0.6287,  E(|Yhat-Yhat'|): 0.5747\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2927,  E(|Y-Yhat|): 0.5915,  E(|Yhat-Yhat'|): 0.5977\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2940,  E(|Y-Yhat|): 0.5898,  E(|Yhat-Yhat'|): 0.5916\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2941,  E(|Y-Yhat|): 0.5901,  E(|Yhat-Yhat'|): 0.5919\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3331,  E(|Y-Yhat|): 0.6670,  E(|Yhat-Yhat'|): 0.6678\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.8555,  E(|Y-Yhat|): 1.5232,  E(|Yhat-Yhat'|): 1.3353\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.7774,  E(|Y-Yhat|): 1.5563,  E(|Yhat-Yhat'|): 1.5577\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.7628,  E(|Y-Yhat|): 1.5408,  E(|Yhat-Yhat'|): 1.5561\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.7602,  E(|Y-Yhat|): 1.5361,  E(|Yhat-Yhat'|): 1.5519\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.9981,  E(|Y-Yhat|): 4.1116,  E(|Yhat-Yhat'|): 4.2270\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.8096,  E(|Y-Yhat|): 2.1493,  E(|Yhat-Yhat'|): 0.6796\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.3464,  E(|Y-Yhat|): 29.8415,  E(|Yhat-Yhat'|): 56.9902\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.1376,  E(|Y-Yhat|): 53.3573,  E(|Yhat-Yhat'|): 104.4394\n",
      "[Epoch 300 (100%), batch 9] energy-loss: -6.2949,  E(|Y-Yhat|): 1269.2288,  E(|Yhat-Yhat'|): 2551.0474\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: -33.3616,  E(|Y-Yhat|): 1646.8668,  E(|Yhat-Yhat'|): 3360.4568\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3423,  E(|Y-Yhat|): 0.6096,  E(|Yhat-Yhat'|): 0.5345\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.2995,  E(|Y-Yhat|): 0.6004,  E(|Yhat-Yhat'|): 0.6017\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.2975,  E(|Y-Yhat|): 0.6024,  E(|Yhat-Yhat'|): 0.6098\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.2961,  E(|Y-Yhat|): 0.6025,  E(|Yhat-Yhat'|): 0.6127\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3361,  E(|Y-Yhat|): 0.6755,  E(|Yhat-Yhat'|): 0.6789\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7082,  E(|Y-Yhat|): 1.1642,  E(|Yhat-Yhat'|): 0.9120\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.5818,  E(|Y-Yhat|): 1.1848,  E(|Yhat-Yhat'|): 1.2061\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.5830,  E(|Y-Yhat|): 1.1632,  E(|Yhat-Yhat'|): 1.1605\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.5846,  E(|Y-Yhat|): 1.1743,  E(|Yhat-Yhat'|): 1.1794\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6692,  E(|Y-Yhat|): 1.3524,  E(|Yhat-Yhat'|): 1.3664\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3523,  E(|Y-Yhat|): 0.6271,  E(|Yhat-Yhat'|): 0.5496\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3086,  E(|Y-Yhat|): 0.6225,  E(|Yhat-Yhat'|): 0.6278\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3080,  E(|Y-Yhat|): 0.6173,  E(|Yhat-Yhat'|): 0.6186\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3073,  E(|Y-Yhat|): 0.6167,  E(|Yhat-Yhat'|): 0.6188\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3335,  E(|Y-Yhat|): 0.6643,  E(|Yhat-Yhat'|): 0.6617\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9463,  E(|Y-Yhat|): 1.5399,  E(|Yhat-Yhat'|): 1.1871\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8801,  E(|Y-Yhat|): 1.7468,  E(|Yhat-Yhat'|): 1.7335\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8741,  E(|Y-Yhat|): 1.7807,  E(|Yhat-Yhat'|): 1.8132\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8778,  E(|Y-Yhat|): 1.7516,  E(|Yhat-Yhat'|): 1.7477\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0073,  E(|Y-Yhat|): 4.0679,  E(|Yhat-Yhat'|): 4.1212\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 2.0699,  E(|Y-Yhat|): 2.4319,  E(|Yhat-Yhat'|): 0.7241\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.9611,  E(|Y-Yhat|): 6.7673,  E(|Yhat-Yhat'|): 9.6124\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 1.7433,  E(|Y-Yhat|): 17.7356,  E(|Yhat-Yhat'|): 31.9845\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.9708,  E(|Y-Yhat|): 7.7827,  E(|Yhat-Yhat'|): 11.6238\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.8321,  E(|Y-Yhat|): 7.9829,  E(|Yhat-Yhat'|): 12.3018\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3695,  E(|Y-Yhat|): 0.6160,  E(|Yhat-Yhat'|): 0.4930\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3322,  E(|Y-Yhat|): 0.6669,  E(|Yhat-Yhat'|): 0.6693\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3313,  E(|Y-Yhat|): 0.6661,  E(|Yhat-Yhat'|): 0.6695\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3314,  E(|Y-Yhat|): 0.6671,  E(|Yhat-Yhat'|): 0.6713\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3333,  E(|Y-Yhat|): 0.6624,  E(|Yhat-Yhat'|): 0.6581\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7129,  E(|Y-Yhat|): 1.1955,  E(|Yhat-Yhat'|): 0.9652\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6671,  E(|Y-Yhat|): 1.3404,  E(|Yhat-Yhat'|): 1.3465\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6655,  E(|Y-Yhat|): 1.3361,  E(|Yhat-Yhat'|): 1.3411\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6601,  E(|Y-Yhat|): 1.3333,  E(|Yhat-Yhat'|): 1.3464\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6732,  E(|Y-Yhat|): 1.3587,  E(|Yhat-Yhat'|): 1.3709\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3557,  E(|Y-Yhat|): 0.6356,  E(|Yhat-Yhat'|): 0.5597\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3355,  E(|Y-Yhat|): 0.6706,  E(|Yhat-Yhat'|): 0.6700\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3359,  E(|Y-Yhat|): 0.6660,  E(|Yhat-Yhat'|): 0.6602\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3280,  E(|Y-Yhat|): 0.6624,  E(|Yhat-Yhat'|): 0.6688\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3278,  E(|Y-Yhat|): 0.6620,  E(|Yhat-Yhat'|): 0.6685\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.9205,  E(|Y-Yhat|): 1.5408,  E(|Yhat-Yhat'|): 1.2406\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.8482,  E(|Y-Yhat|): 1.7073,  E(|Yhat-Yhat'|): 1.7180\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.8375,  E(|Y-Yhat|): 1.6904,  E(|Yhat-Yhat'|): 1.7057\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.8447,  E(|Y-Yhat|): 1.6876,  E(|Yhat-Yhat'|): 1.6858\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 2.0135,  E(|Y-Yhat|): 4.0635,  E(|Yhat-Yhat'|): 4.0999\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 1.9631,  E(|Y-Yhat|): 2.2560,  E(|Yhat-Yhat'|): 0.5859\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 1.1347,  E(|Y-Yhat|): 341.3447,  E(|Yhat-Yhat'|): 680.4200\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 37.3188,  E(|Y-Yhat|): 1875.7729,  E(|Yhat-Yhat'|): 3676.9082\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 1.8438,  E(|Y-Yhat|): 797.3433,  E(|Yhat-Yhat'|): 1590.9989\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4053,  E(|Y-Yhat|): 916.2636,  E(|Yhat-Yhat'|): 1831.7166\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3675,  E(|Y-Yhat|): 0.6196,  E(|Yhat-Yhat'|): 0.5043\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3215,  E(|Y-Yhat|): 0.6428,  E(|Yhat-Yhat'|): 0.6427\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3188,  E(|Y-Yhat|): 0.6419,  E(|Yhat-Yhat'|): 0.6462\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3193,  E(|Y-Yhat|): 0.6452,  E(|Yhat-Yhat'|): 0.6519\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3361,  E(|Y-Yhat|): 0.6687,  E(|Yhat-Yhat'|): 0.6652\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.7134,  E(|Y-Yhat|): 1.1675,  E(|Yhat-Yhat'|): 0.9083\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.6291,  E(|Y-Yhat|): 1.2692,  E(|Yhat-Yhat'|): 1.2802\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.6394,  E(|Y-Yhat|): 1.2802,  E(|Yhat-Yhat'|): 1.2816\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.6449,  E(|Y-Yhat|): 1.2753,  E(|Yhat-Yhat'|): 1.2607\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.6699,  E(|Y-Yhat|): 1.3387,  E(|Yhat-Yhat'|): 1.3375\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 9] energy-loss: 0.3558,  E(|Y-Yhat|): 0.6316,  E(|Yhat-Yhat'|): 0.5515\n",
      "[Epoch 100 (33%), batch 9] energy-loss: 0.3246,  E(|Y-Yhat|): 0.6493,  E(|Yhat-Yhat'|): 0.6493\n",
      "[Epoch 200 (66%), batch 9] energy-loss: 0.3209,  E(|Y-Yhat|): 0.6487,  E(|Yhat-Yhat'|): 0.6556\n",
      "[Epoch 300 (100%), batch 9] energy-loss: 0.3227,  E(|Y-Yhat|): 0.6477,  E(|Yhat-Yhat'|): 0.6499\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.3339,  E(|Y-Yhat|): 0.6686,  E(|Yhat-Yhat'|): 0.6693\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0031),\n",
       " tensor(1.4946),\n",
       " tensor(0.0029),\n",
       " tensor(0.0028),\n",
       " tensor(0.0038))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_power1 = []\n",
    "MSE_exp = []\n",
    "MSE_log1p = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(10):\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = postanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"log\", x_lower=0, x_upper=5, noise_dist = \"gaussian\", noise_std=1, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(0, 5, 50)\n",
    "    x2 = torch.linspace(0, 5, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = log_lin(Z)\n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_power1 = engression(x, y, lr=0.005, loss_phi=\"power\", beta=1, num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_exp = engression(x, y, lr=0.005, loss_phi=\"exp\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_log1p = engression(x, y, lr=0.005, loss_phi=\"log1p\", num_epochs=300, batch_size=1000, device=device)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_power1 = engressor_power1.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_exp = engressor_exp.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_log1p = engressor_log1p.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_power1.append(torch.mean((y_pred_power1.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_exp.append(torch.mean((y_pred_exp.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_log1p.append(torch.mean((y_pred_log1p.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "\n",
    "torch.mean(torch.stack(MSE_power)), torch.mean(torch.stack(MSE_power1)),torch.mean(torch.stack(MSE_exp)), torch.mean(torch.stack(MSE_log1p)), torch.mean(torch.stack(MSE_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6d027",
   "metadata": {},
   "source": [
    "# Checking the robustness of frac energy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7a033869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"cubic\", x_lower=-2, x_upper=2, noise_dist = \"uniform\", noise_std=5, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = Z ** 3 / 3.0   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device, verbose=False)\n",
    "    engressor_frac = engression(x, y, lr=0.005, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device, verbose=False)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9758cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ca3f7b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "power_list = torch.stack(MSE_power).numpy()\n",
    "frac_list = torch.stack(MSE_frac).numpy()\n",
    "\n",
    "bins = np.linspace(min(power_list.min(), frac_list.min()), max(power_list.max(), frac_list.max()), 101)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(power_list, bins=bins, alpha=0.55, label='power', edgecolor='black')\n",
    "plt.hist(frac_list, bins=bins, alpha=0.55, label='frac', edgecolor='black')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "92e93b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "MSE_power = []\n",
    "MSE_frac = []\n",
    "\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    A0, M0 = generate_mats(dx=2, dy=2, k=2, seed=i, device=device)\n",
    "    x, y = preanm_generator(n=10000, dx=2, dy=2, k=2, true_function = \"cubic\", x_lower=-2, x_upper=2, noise_dist = \"uniform\", noise_std=5, A=A0, M=M0, seed=i, device=device)\n",
    "\n",
    "    x1 = torch.linspace(-2, 2, 50)\n",
    "    x2 = torch.linspace(-2, 2, 50)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([X1.reshape(-1), X2.reshape(-1)], dim=1)\n",
    "    Z = x_eval @ A0.T       \n",
    "    U = Z ** 3 / 3.0   \n",
    "    y_eval = U @ M0.T   \n",
    "\n",
    "    # Fit an engression model\n",
    "    engressor_power = engression(x, y, lr=0.005, num_layer = 4, loss_phi=\"power\", beta=0.5, num_epochs=300, batch_size=1000, device=device, verbose=False)\n",
    "    engressor_frac = engression(x, y, lr=0.005, num_layer = 4, loss_phi=\"frac\", num_epochs=300, batch_size=1000, device=device, verbose=False)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_power = engressor_power.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "    y_pred_frac = engressor_frac.predict(x_eval, target=\"median\", sample_size=1000)\n",
    "\n",
    "    MSE_power.append(torch.mean((y_pred_power.reshape(-1) - y_eval.reshape(-1))**2))\n",
    "    MSE_frac.append(torch.mean((y_pred_frac.reshape(-1) - y_eval.reshape(-1))**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4b84afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "power_list = torch.stack(MSE_power).numpy()\n",
    "frac_list = torch.stack(MSE_frac).numpy()\n",
    "\n",
    "bins = np.linspace(min(power_list.min(), frac_list.min()), max(power_list.max(), frac_list.max()), 101)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(power_list, bins=bins, alpha=0.55, label='power', edgecolor='black')\n",
    "plt.hist(frac_list, bins=bins, alpha=0.55, label='frac', edgecolor='black')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
